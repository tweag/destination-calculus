% -*- latex -*-
\documentclass[acmsmall, screen]{acmart}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\let\Bbbk\relax % needed because of a conflict between amssymb and newtx
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{environ}

% For faulty code
\usepackage[normalem]{ulem}
\makeatletter
\def\uwave{\bgroup \markoverwith{\lower3.5\p@\hbox{\sixly \textcolor{red}{\char58}}}\ULon}
\font\sixly=lasy6 % does not re-load if already loaded, so no memory problem.
\makeatother

% For OTT rendering
\usepackage[supertabular]{ottalt}
\inputott{destination_calculus_ott.tex}
\usepackage{ottstyling}
% Hide "Index for ranges" from the metavars displayed tabular
\patchcmd{\ottmetavars}{$ \ottmv{k} $ & \ottcom{Index for ranges} \\}{}{}{}

% \setlength\textfloatsep{\baselineskip}
% \setlength{\intextsep}{\baselineskip}

\newcommand{\TODO}[1]{~\textnormal{\textcolor{red}{TODO: #1} } }
\newcommand\sepimp{\mathrel{-\mkern-6mu*}}
\newcommand{\parr}{\rotatebox[origin=c]{180}{\&}}
\makeatletter
\newcommand{\smallbullet}{} % for safety
\DeclareRobustCommand\smallbullet{%
\mathord{\mathpalette\smallbullet@{0.5}}%
}
\newcommand{\smallbullet@}[2]{%
\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}%
}
\makeatother

\makeatletter
\newcommand{\oset}[3][0ex]{%
\mathrel{\mathop{#3}\limits^{
  \vbox to#1{\kern-2\ex@
  \hbox{$\scriptstyle#2$}\vss}}}}
\makeatother

\def\mycasem#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{}{#1}}
\def\myfunvm#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{\,\,}{#1}}
\def\myfuntm#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{\,}{#1}}
\def\mydestm#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{}{#1}}
\def\mymul#1{\ifthenelse{\equal{#1}{[[¹]]}}{}{#1}}

\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newcommand\btriangleq{\pmb{\triangleq}}
\newcommand\btriangleqrec{\oset{\mathsf{rec}}{\pmb{\triangleq}}}
\newlength{\interdefskip}
\setlength{\interdefskip}{0.15cm}
\newcommand{\newtype}[3][]{#2~\ifthenelse{\equal{#1}{}}{\btriangleq}{\btriangleqrec}~#3\\[\interdefskip]}
\newcommand{\newoperator}[5][]{\phantom{a}\!\!\!\!\!\!\begin{array}[t]{l}%
#2 ~\pmb{:}~ #3 \\
#4 ~\ifthenelse{\equal{#1}{}}{\btriangleq}{\btriangleqrec}~ #5
\end{array}\\[\interdefskip]}

\newcommand{\newoperatorb}[5][]{\phantom{a}\!\!\!\!\!\!\begin{array}[t]{l}%
#2 ~\pmb{:}~ #3 \\
#4 ~\ifthenelse{\equal{#1}{}}{\btriangleq}{\btriangleqrec}~ #5
\end{array}\\[\interdefskip]}
\newcommand{\figureratio}{0.9}
\newcommand{\codehere}[1]{\begin{center}\begin{minipage}{\figureratio\linewidth}{\small$#1$}\end{minipage}\end{center}}
\NewEnviron{codefig}[2][h]{\begin{figure}[#1]
\codehere{\BODY}#2
\end{figure}}
\NewEnviron{ottfig}[2][h]{\begin{figure}[#1]\visiblespaces
\small\BODY\activespaces #2
\end{figure}}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\acmJournal{PACMPL}
\acmConference[POPL'25]{Principles of Programming Languages}{January 19 -- 25, 2025}{Denver, Colorado}
\title{Destination calculus}
\subtitle{A linear $\lambda-$calculus for pure, functional memory updates}

\author{Arnaud Spiwack}
\orcid{0TBD-0TBD-0TBD-0TBD}
\affiliation{
\institution{Tweag}
\department{OSPO}
\position{Director, Research}
\city{Paris}
\country{France}
}
\email{arnaud.spiwack@tweag.io}

\author{Thomas Bagrel}
\orcid{0009-0008-8700-2741}
\affiliation{
\institution{LORIA/Inria}
\department{MOSEL VERIDIS}
\city{Nancy}
\country{France}
}
\affiliation{
\institution{Tweag}
\department{OSPO}
\city{Paris}
\country{France}
}
\email{thomas.bagrel@loria.fr}
\email{thomas.bagrel@tweag.io}

% On introduit de la mutation controlée dans les FP languages sans endommager la pureté (comme la lazyness peut être vu aussi)

\begin{abstract}
We present the destination calculus, a linear $\lambda-$calculus for
pure, functional memory updates. We introduce the
syntax, type system, and operational semantics of the destination
calculus, and prove type safety formally in the Coq proof assistant.

We show how the principles of the destination calculus can form a theoretical ground
for destination-passing style programming in functional languages. In particular,
we detail how the present work can be applied to Linear Haskell to lift the main 
restriction of DPS programming in Haskell as developed in \cite{bagrel_destination-passing_2024}.
We illustrate this with a range of pseudo-Haskell examples.
\end{abstract}

\maketitle

%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Destination-passing style programming takes its root in the early days of imperative programming. In such language, the programmer is responsible for managing memory allocation and deallocation, and thus is it often unpractical for function calls to allocate memory for their results themselves. Instead, the caller allocates memory for the result of the callee, and passes the address of this output memory cell to the callee as an argument. This is called an \emph{out parameter}, \emph{mutable reference}, or even \emph{destination}.

But destination-passing style is not limited to imperative settings; it can be used in functional programming as well. One example is the linear destination-based API for arrays in Haskell\cite{bernardy_linear_2018}, which enables the user to build an array efficiently in a write-once fashion, without sacrificing the language identity and main guarantees. In this context, a destination points to a yet-unfilled memory slot of the array, and is said to be \emph{consumed} as soon as the associated hole is filled. In this paper, we continue on the same line: we present a linear $\lambda-$calculus embedding the concept of \emph{destinations} as first-class values, in order to provide a write-once memory scheme for pure, functional programming languages.

Why is it important to have destinations as first-class values? Because it allows the user to store them in arbitrary control or data structures, and thus to build complex data structures in arbitrary order/direction. This is a key feature of first-class DPS APIs, compared to ones in which destinations are inseparable from the structure they point to. In the latter case, the user is still forced to build the structure in its canonical order (e.g. from the leaves up to the root of the structure when using data constructors).

\section{Working with destinations}

\activespaces

\codehere{\newoperator
{fillWithUnit}{[[⌊ ① ⌋ ¹ν ¹ν → ①]]}
{fillWithUnit~d}{[[d ⨞ ()]]}}

\codehere{\newoperator
{fillWithInl}{[[⌊ T ⨁ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν]]}
{fillWithInl~d}{[[d ⨞ Inl]]}}

\codehere{\newoperator
{fillWithAPair}{[[⌊ T ⨂ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν]]}
{fillWithAPair~d}{[[d ⨞ (,)]]}}

\codehere{\newoperator
{fillWithInl'}{[[S ⧔ ⌊ T ⨁ U ⌋ ¹ν ¹ν → S ⧔ ⌊ T ⌋ ¹ν]]}
{fillWithInl'~x}{[[x map d ⟼ d ⨞ Inl]]}}

\phantom{a}\TODO{remove the argument in the signature}
\codehere{\newoperator
{[[ˢInl x]]}{[[T ¹ν → T ⨁ U]]}
{[[ˢInl x]]}{[[from⧔' (alloc map d ⟼ d ⨞ Inl ˢ⨞ x)]]}}

\phantom{a}\TODO{remove the arguments in the signature}
\codehere{\newoperator
{[[ˢ(x, y)]]}{[[T ¹ν → U ¹ν → T ⨂ U]]}
{[[ˢ(x, y)]]}{[[from⧔' (alloc map d ⟼ (d ⨞ (,)) case ¹ν  (d1, d2) ⟼ d1 ˢ⨞ x; d2 ˢ⨞ y)]]}}

\codehere{\newoperator
{forget}{[[T]]}
{forget}{[[from⧔' (alloc map d ⟼ ())]]}}

\phantom{a}\TODO{Syntactic sugar: let}
\codehere{\newoperator
{overwrite}{[[T¹ν → T ⨁ U]]}
{overwrite~x}{[[from⧔' (alloc map d ⟼ (ˢλ d' ¹ν ⟼ d ⨞ Inr; d' ˢ⨞ x) (d ⨞ Inl))]]}}

The idea of destination calculus is to provide a building canvas for data structures, represented as a special pair-like structure, called an \emph{ampar}, whose left side is the structure being built, and whose right side carries destinations pointing to holes present in the left side.

A \emph{hole}, denoted $[[+h]]$ in the language, is a memory cell that has not been written to yet. In a practical application, a \emph{hole} would contain garbage data left by the previous use of the memory location, and thus should not be read in any circumstances (or it could lead to a segmentation fault!).

A \emph{destination}, denoted $[[-h]]$, is the address of a hole. It is meant to be one-use only; unlike mutable references which can often be reused.

The simplest form of ampar is a single empty cell on the left with a destination pointing to it on the right: $[[{ h } ⟨ +h ❟ -h ⟩]]$. This ampar is highly analogous to the identity function: the final structure on the left side will correspond to what is fed in the right side.

The main operation to operate on an ampar is $\ottkw{map}$, which binds the right side of the ampar to a variable, while temporarily forgetting about the left side (the incomplete structure that is being mutated behind the scenes):

\codehere{\!\!\!\begin{array}{cl}
      & [[ { h } ⟨ +h ❟ -h ⟩ map d ⟼ d ⨞ Inl ⨞ () ]] \\
[[⟶]] & [[ { h' } ⟨ Inl +h' ❟ -h' ⟩ ]] \\
[[⟶]] & [[ { } ⟨ Inl () ❟ () ⟩ ]]
\end{array}}

The destination-feeding primitive $[[⨞]][[Inl]]$ will in fact follow the destination on the left-hand side and write an $[[Inl]]$ data constructor to the hole pointed by the destination. A new destination $[[-h']]$ is returned to represented the (yet unspecified) payload for the $[[Inl]]$ constructor.

This new destination $[[-h']]$ is then passed to the destination-feeding primitive $[[⨞]][[()]]$, which writes a unit value to the hole pointed by $[[-h']]$. The unit data constructor $[[()]]$ doesn't hold any payload, so there is no new destination to return here (so $[[()]]$ is returned by the primitive instead).

\paragraph{Arbitrary building order}

As mentioned earlier, one key point of destination calculus is being able to build a structure in any desired order. For example, one can build a balanced binary tree of depth two by first building the skeleton, then giving values to the right leaves, then to the left leaves.

Let's build the skeleton first:

\codehere{[[x1]] \btriangleq \!\!\!\begin{array}[t]{l}[[
{ h1 } ⟨ +h1 ❟ -h1 ⟩ map d1 ⟼⮒
‥‥(d1 ⨞ (,)) case ¹ν (d2, d3) ⟼⮒
‥‥‥‥(d2 ⨞ (,)) case ¹ν (d4, d5) ⟼⮒
‥‥‥‥‥‥(d3 ⨞ (,)) case ¹ν (d6, d7) ⟼ ˢ[d4, d5, d6, d7]
]]\end{array}}

\noindent This evaluates to:

\codehere{[[ { h4, h5, h6, h7 } ⟨ ((+h4, +h5), (+h6, +h7)) ❟ ˢ[-h4, -h5, -h6, -h7] ⟩ ]]}

We can see that the skeleton of the tree is here on the left-hand side, but leaves are unspecified yet (so are represented by holes). Let's now fill the right leaves:

\codehere{[[x2]] \btriangleq \!\!\!\begin{array}[t]{l}[[
x1 map d ⟼⮒
‥‥d case ¹ν ˢ[d4, d5, d6, d7] ⟼⮒
‥‥‥‥d5 ⨞ Inr ⨞ () ; d7 ⨞ Inr ⨞ () ; ˢ[d4, d6]
]]\end{array}}

\noindent This reduces to:

\codehere{[[ { h4, h6 } ⟨ ((+h4, Inr ()), (+h6, Inr ())) ❟ ˢ[-h4, -h6] ⟩ ]]}

Now only left leaves are left unspecified, that's why the associated destinations $[[-h4]]$ and $[[-h6]]$ are the only left on the right side. We can finally feed them:

\codehere{[[x3]] \btriangleq \!\!\!\begin{array}[t]{l}[[
x2 map d ⟼⮒
‥‥d case ¹ν ˢ[d4, d6] ⟼⮒
‥‥‥‥d4 ⨞ Inl ⨞ () ; d6 ⨞ Inr ⨞ ()
]]\end{array}}

\noindent $[[x3]]$ reduces to:

\codehere{[[ {} ⟨ ((Inl (), Inr ()), (Inl (), Inr ())) ❟ () ⟩ ]]}

\noindent which is now a complete structure. There is no destination remaining on the right-hand side.

We also see that structure can in fact be built in several stages with ease, with other operations taking place in-between.

\subsection{Progressing towards an efficient queue implementation}\label{ssec:efficient-queue}

If we suppose equirecursive types and a fixed-point operator, then destination-calculus becomes expressive enough to build any usual data structure.

\paragraph{Linked lists}

For starters, we can define lists as the fixpoint of the functor $[[X]] \mapsto [[① ⨁ (T ⨂ X)]]$ where $[[T]]$ is the type of list items. Instead of defining the usual \textsc{Nil} $\ottsctor{[]}$ and \textsc{Cons} $\ottsctor{(::)}$ constructors, we define the more general \textsc{FillNil} $\triangleleft\ottsctor{[]}$ and \textsc{FillCons} $\triangleleft\ottsctor{(::)}$ operators, as presented in Figure~\ref{fig:impl-list}.

\begin{codefig}{\caption{List implementation in equirecursive destination calculus}\label{fig:impl-list}}
\newtype[~\mathttbf{rec}]{[[List T]]}{[[① ⨁ (T ⨂ (List T))]]}
\newoperator
  {\triangleleft\ottsctor{[]}}{[[⌊ List T ⌋ n ¹ν → ①]]}
  {[[d ⨞ [] ]]}{[[d ⨞Inl ⨞()]]}
\newoperator
  {\triangleleft\ottsctor{(::)}}{[[⌊ List T ⌋ n ¹ν → ⌊ T⌋ n ⨂ ⌊ List T ⌋ n]]}
  {[[d ⨞ (::)]]}{[[d ⨞Inr ⨞(,)]]}
\end{codefig}

The \textsc{FillNil} operator consumes a destination of list and fills its associated hole with the value $[[Inl ()]]$ representing the \textsc{Nil} constructor in our encoding. It returns unit $[[()]]$ as there is no new hole/destination created in the process.

The \textsc{FillCons} operator consumes a destination of list as an input, and writes a hollow \textsc{Cons} constructor (represented by $[[Inr (+h1, +h2)]]$ in our encoding) to the hole pointed by the destination. It then returns a pair of destinations $[[-h1]]$ and $[[-h2]]$ to represent the two holes in the hollow \textsc{Cons} constructor ($[[-h1]]$ is a destination for the list item, and $[[-h2]]$ is for the list tail). This behavior is hidden behind the primitives $[[⨞]][[Inr]]\pmb{:}[[⌊① ⨁ (T ⨂ (List T))⌋ n ¹ν → ⌊T ⨂ (List T)⌋ n]]$ (choosing the right branch of a sum type by writing a hollow $[[Inr]]$ constructor) and $[[⨞]][[(,)]]\pmb{:}[[⌊T ⨂ (List T)⌋ n ¹ν → ⌊T⌋ n ⨂ ⌊List T⌋ n]]$ (transforming a destination of pair to a pair of destination by writing a hollow pair constructor).

There is a duality between constructors and destinations-feeding operators (we will dig into it more in Section~\ref{ssec:ampar-motivation}), and that stays true for our newly user-defined operators. If we reverse the arrow direction and remove the destination symbols from \textsc{FillCons} signature, we get $[[T ⨂ (List T) ¹ν → List T]]$ which is the type of the usual \textsc{Cons} constructor! We can in fact recover the \textsc{Cons} constructor:

\codehere{\newoperator
{\ottsctor{(::)}}{[[T ⨂ (List T) ¹ν → List T]]}
{\ottsctor{(::)}~[[x]]~[[xs]]}{[[alloc map d ⟼ (d ⨞ (::)) case ¹ν (dx, dxs) ⟼ dx ˢ⨞ x ; dxs ˢ⨞ xs]]}}

Going from a \textsc{Fill} operator to the associated constructor is completely generic, and with more metaprogramming tools, we could build this transformation into the language.

\paragraph{Difference lists}

While linked lists are optimized for the prepend operation (\textsc{Cons}), they are not efficient for appending or concatenation, as it requires a full copy (or traversal at least) of the first list before the last cons cell can be changed to point to the head of the second list.

Difference lists are a data structure that allows for efficient concatenation. In functional languages, difference lists are often encoded using a function that take a tail, and returns the previously-unfinished list with the tail appended to it. For example, the difference list $[[x1]] \ottsctor{:} [[x2]] \ottsctor{:} \ldots \ottsctor{:} [[xk]] \ottsctor{:} \textcolor{red}{\square}$ is represented by the linear function $\lambda [[xs]] \mapsto [[x1]] \ottsctor{:} [[x2]] \ottsctor{:} \ldots \ottsctor{:} [[xk]] \ottsctor{:} [[xs]]$. This encoding shines when list concatenation calls are nested to the left, as the function encoding delays the actual concatenation so that it happens in a more optimal, right-nested fashion.

In destination calculus, we can go even further, and represent difference lists much like we would do in an imperative programming language (although in a safe setting here), as a pair of an incomplete list who is missing its tail, and a destination pointing to the missing tail's location. This is exactly what an ampar is designed to allow: thanks to an ampar, we can handle incomplete structures safely, with no need to complete them immediately. The incomplete list is represented by the left side of the ampar, and the destination is represented by its right side. Creating an empty difference list is exactly what the $[[alloc]]$ primitive already does when specialized to type $[[List T]]$ : it returns an incomplete list with no items in it, and a destination pointing to that cell so that the list can be built later through destination-feeding primitives, together in an ampar: $[[List T ⧔ ⌊List T⌋ ¹ν]]$. Type definition and operators for difference lists in destination calculus are presented in Figure~\ref{fig:impl-dlist}.

\begin{codefig}{\caption{Difference list implementation in equirecursive destination calculus}\label{fig:impl-dlist}}
\newtype{[[DList T]]}{[[(List T) ⧔ ⌊ List T ⌋ ¹ν]]}
\newoperator
  {\ottkw{append}}{[[DList T ¹ν → T ¹ν → DList T]]}
  {[[ys append y]]}{\!\!\!\begin{array}[t]{l}[[
ys map dys ⟼ (dys ⨞ (::)) case ¹ν⮒
‥‥(dy, dys') ⟼ dy ˢ⨞ y ; dys'
]]\end{array}}
\newoperator
  {\ottkw{concat}}{[[DList T ¹ν → DList T ¹ν → DList T]]}
  {[[ys concat ys']]}{[[ys map d ⟼ d ⨞· ys']]}
\newoperator
  {\ottkw{to}_{\ottstype{List} }}{[[DList T ¹ν → List T]]}
  {[[toList ys]]}{[[from⧔' (ys map d ⟼ d ⨞ [])]]}
\end{codefig}

The $\ottkw{append}$ simply appends an element at the end of the list. It uses \textsc{FillCons} to link a new hollow \textsc{Cons} cell at the end of the list, and then handles the two associated destinations $[[dy]]$ and $[[dt]]$. The former, representing the item slot, is fed with the item to append, while the latter, representing the slot for the tail of the resulting difference list, is returned and so stored back in the right side of the ampar. If that second destination was consumed, and not returned, we would end up with a regular linked list, instead of a difference list.

The $\ottkw{concat}$ operator concatenates two difference lists by writing the head of the second one to the hole left at the end of the first one. This is done using the \textsc{FillComp} primitive $\triangleleft\mybullet \pmb{:} [[⌊ U1 ⌋n ¹ν → U1 ⧔ U2 ¹↑ · n → U2]]$\footnote{In this particular context, $[[U1]] = [[List T]]$ and $[[U2]] = [[⌊ List T ⌋¹ν]]$, so \textsc{FillComp} has signature $\triangleleft\mybullet \pmb{:} [[⌊ List T ⌋¹ν ¹ν → DList T ¹↑ → ⌊ List T ⌋¹ν]]$}. It takes a destination on its left-hand side, and an ampar on its right-hand side. The left side of the ampar (type $[[U1]]$) is fed to the destination (so the incomplete structure is written to a larger incomplete structure from which the destination originated from), and the right side of the ampar (type $[[U2]]$) is returned.

Here the left side of the second ampar is the second incomplete list, which is pasted at the end of the first incomplete list, consuming the destination of the first difference list in the process. Then the right side of the second ampar, that is to say the destination to the yet-unspecified tail of the second difference list, is returned, and stored back in the resulting ampar (thus serves as the new destination to the tail of the the resulting difference list).

Finally, the $\ottkw{to}_{\ottstype{List}}$ operator converts a difference list to a regular list by filling the hole left in the incomplete list using \textsc{FillNil}.

We can note that although this exemple is typical of destination-style programming, it doesn't use the first-class nature of destinations that our calculus allows, and thus can be implemented in other destination-passing style frameworks such as~\cite{bour_tmc_2021} and~\cite{leijen_trmc_2023}. We will see in the next sections what kind of programs can be benefit from first-class destinations.

\paragraph{Efficient queue using previously defined structures}

The usual functional encoding for a queue is two use a pair of lists, one representing the front of the queue, and keeping the element in order, while the second list represent the back of the queue, and is kept in reversed order (e.g the latest inserted element will be at the front of the second list).

With such a queue implementation, dequeueing the front element is efficient (just pattern-match on the first cons cell of the first list, $\mathcal{O}(1)$), and enqueuing a new element is efficient too (just add a new \textsc{Cons} cell at the front of the second list, $\mathcal{O}(1)$ too). However, when the first list is depleted, one has to transfer elements from the second list to the first one, and as such, has to reverse the second list, which is a $\mathcal{O}(n)$ operation (although it is amortized).

With access to efficient difference lists, as shown in the previous paragraph, we can replace the second list by a difference list, to maintain a quick $\ottkw{enqueue}$ operation (still $\mathcal{O}(1)$), but remove the need for a $\ottkw{reverse}$ operation (as $\ottkw{to}_{\ottstype{List} }$ is $\mathcal{O}(1)$ for difference lists). Nothing needs to change for the first list. The corresponding implementation is presented in Figure~\ref{fig:impl-queue}.

\begin{codefig}{\caption{Queue implementation in equirecursive destination calculus}\label{fig:impl-queue}}
\newtype{[[Queue T]]}{[[(List T) ⨂ (DList T)]]}
\newoperator 
  {\ottkw{singleton}}{[[T ¹ν → Queue T]]}
  {[[singleton x]]}{[[ˢ(ˢInr ˢ(x, Inl ()), alloc)]]}
\newoperator
  {\ottkw{enqueue}}{[[Queue T ¹ν → T ¹ν → Queue T]]}
  {[[q enqueue y]]}{[[q case ¹ν (xs, ys) ⟼ ˢ(xs, ys append y)]]}
\newoperator
  {\ottkw{dequeue}}{[[Queue T ¹ν → ① ⨁ (T ⨂ (Queue T))]]}
  {[[dequeue q]]}{\!\!\!\begin{array}[t]{l}[[
q case ¹ν {⮒
‥‥ˢ(ˢInr ˢ(x, xs), ys) ⟼ ˢInr ˢ(x, ˢ(xs, ys)),⮒
‥‥ˢ(Inl (), ys) ⟼ (toList ys) case ¹ν {⮒
‥‥‥‥Inl () ⟼ Inl (),⮒
‥‥‥‥ˢInr ˢ(x, xs) ⟼ ˢInr ˢ(x, ˢ(xs, alloc))⮒
‥‥}⮒
}
]]\end{array}}
\end{codefig}

The $\ottkw{singleton}$ operator creates a pair of a list with a single element, and a fresh difference list (obtained via $[[alloc]]$).

The $\ottkw{enqueue}$ operator appends an element to the difference list, while letting the front list unchanged.

The $\ottkw{dequeue}$ operator is more complex though. It first checks if there is at least one element available in the front list. If there is, it extracts the element $[[x]]$ by removing the first \textsc{Cons} cell of the front list, and returns it alongside the rest of the queue $[[ˢ(xs, ys)]]$. If there isn't, it converts the difference list $[[ys]]$ to a normal list, and pattern-matches on it to look for an available element. If none is found again, it returns $[[Inl ()]]$ to signal that the queue is definitely empty. If an element $[[x]]$ is found, then it returns it alongside the updated queue, made of the tail $[[xs]]$ of the difference list turned into a list, and a fresh difference list given by $[[alloc]]$.

\section{Limitations of the previous approach}

Everything described above is in fact already possible in destination-passing style for Haskell as presented in~\cite{bagrel_destination-passing_2024}. However, there is one fundamental limitation in~\cite{bagrel_destination-passing_2024}: the inability to store destinations in destination-based data structures.

Indeed, that first approach of destination-passing style for Haskell can only be used to build non-linear data structures. More precisely, the \textsc{FillLeaf} operator ($\triangleleft$) can only take arguments with multiplicity $[[ω]]$. This is in fact a much stronger restriction than necessary ; the core idea is \emph{just} to prevent any destination (which is always a linear resource) to appear somewhere in the right-hand side of \textsc{FillLeaf}.

\subsection{Why stored destinations are problematic}\label{ssec:problem-stored-dests}

One core assumption of destination-passing style programming is that once a destination has been linearly consumed, the associated hole has been filled.

However, in a realm where destinations $\ottstype{\lfloor[[T]]\rfloor}$ can be of arbitrary inner type $[[T]]$, they can in particular be used to store a destination itself when $[[T]] = \ottstype{\lfloor[[T']]\rfloor}$!

We have to mark the value being fed in a destination as linearly consumed, so that it cannot be both stored away (to be used later) and pattern-matched on/used in the current context. But that means we have to mark the destination $[[d]] \pmb{:} \ottstype{\lfloor[[T']]\rfloor}$ as linearly consumed too when it is fed to $[[dd]] \pmb{:} \ottstype{\lfloor\lfloor[[T']]\rfloor\rfloor}$ in $[[dd ˢ⨞ d]]$.

As a result, there are in fact two ways to consume a destination: feed it now with a value, or store it away and feed it later. The latter is a much weaker form of consumption, as it doesn't guarantee that the hole associated to the destination has been filled \emph{now}, only that it will be filled later. So our assumption above doesn't hold in general case.

The issue is particularly visible when trying to give semantics to the $\ottkw{alloc'}$ operator with signature $\ottkw{alloc'} \pmb{:} \ottstype{(\lfloor[[T]]\rfloor\,_{\mymul{[[¹]]}}\!\ottstype{\to}\,[[①]])\,_{\mymul{[[¹]]}}\!\ottstype{\to}\,[[T]]}$. It reads: ``given a way of consuming a destination of type $[[T]]$, I'll return an object of type $[[T]]$''. This is an operator we really much want in our system!

The morally correct semantics (in destination calculus pseudo-syntax) would be:

\codehere{\ottkw{alloc'}~(\ottsctor{\lambda}[[d]]\,_{\mymul{[[¹]]}}\!\!\mapsto[[t]]) ~[[⟶]]~ \ottkw{withTmpStore}~\{ [[h]] \coloneq \_ \}~\ottkw{do}~\{ [[ t[d ≔ -h] ]]~\patu~\ottkw{deref}~[[-h]] \}}

It works as expected when the function supplied to $\ottkw{alloc'}$ will indeed use the destination to store a value:

\codehere{\!\!\!\begin{array}{cl}
      & \ottkw{alloc'}~(\ottsctor{\lambda}[[d]]\,_{\mymul{[[¹]]}}\!\!\mapsto[[d ⨞ Inl ⨞ ()]]) \\
[[⟶]] & \ottkw{withTmpStore}~\{ [[h]] \coloneq \_ \}~\ottkw{do}~\{ [[-h ⨞ Inl ⨞ ()]]~\patu~\ottkw{deref}~[[-h]] \} \\
[[⟶]] & \ottkw{withTmpStore}~\{ [[h]] \coloneq [[Inl ()]] \}~\ottkw{do}~\{ \ottkw{deref}~[[-h]] \} \\
[[⟶]] & [[Inl ()]]
\end{array}}

However this falls short when calls to $\ottkw{alloc'}$ are nested in the following way (where $[[dd]]$ \pmb{:} $\ottstype{\lfloor\lfloor[[①]]\rfloor\rfloor}$ and $[[d]]$ \pmb{:} $\ottstype{\lfloor[[①]]\rfloor}$):

\codehere{\!\!\!\begin{array}{cl}
      & \ottkw{alloc'}~(\ottsctor{\lambda}[[dd]]\,_{\mymul{[[¹]]}}\!\!\mapsto\ottkw{alloc'}~(\ottsctor{\lambda}[[d]]\,_{\mymul{[[¹]]}}\!\!\mapsto[[dd ˢ⨞ d]])) \\
[[⟶]] & \ottkw{withTmpStore}~\{ [[hd]] \coloneq \_ \}~\ottkw{do}~\{ \ottkw{alloc'}~(\ottsctor{\lambda}[[d]]\,_{\mymul{[[¹]]}}\!\!\mapsto[[-hd ˢ⨞ d]])~\patu~\ottkw{deref}~[[-hd]] \} \\
[[⟶]] & \ottkw{withTmpStore}~\{ [[hd]] \coloneq \_ \}~\ottkw{do}~\{ \ottkw{withTmpStore}~\{ [[h]] \coloneq \_ \}~\ottkw{do}~\{ [[-hd ˢ⨞ -h]]~\patu~\ottkw{deref}~[[-h]] \}~\patu~\ottkw{deref}~[[-hd]] \} \\
[[⟶]] & \ottkw{withTmpStore}~\{ [[hd]] \coloneq [[-h]] \}~\ottkw{do}~\{ \uwave{ \ottkw{withTmpStore}~\{ [[h]] \coloneq \_ \}~\ottkw{do}~\{ \ottkw{deref}~[[-h]] \} }~\patu~\ottkw{deref}~[[-hd]] \}
\end{array}}

The original term $\ottkw{alloc'}~(\ottsctor{\lambda}[[dd]]\,_{\mymul{[[¹]]}}\!\!\mapsto\ottkw{alloc'}~(\ottsctor{\lambda}[[d]]\,_{\mymul{[[¹]]}}\!\!\mapsto[[dd ˢ⨞ d]]))$ is well typed, as the inner call to $\ottkw{alloc'}$ returns a value of type $[[①]]$ (as $[[d]]$ is of type $\ottstype{\lfloor[[①]]\rfloor}$) and consumes $[[d]]$ linearly. However, we see that because $[[-h]]$ escaped to the parent scope by being stored in a destination of destination coming from the parent scope, the hole $[[h]]$ has not been filled, and thus the inner expression $\ottkw{withTmpStore}~\{ [[h]] \coloneq \_ \}~\ottkw{do}~\{ \ottkw{deref}~[[-h]] \}$ cannot reduce in a meaningful way.

One could argue that the issue comes from the destination-feeding primitive $\triangleleft$ returning unit instead of a special value of a distinct \emph{effect} type. However, the same issue arise if we introduce a distinct type $\ottstype{\lfloor\!\!\rfloor}$ for the effect of feeding a destination ; there is always a way to cheat the system and make a destination escape to a parent scope. This distinct type for effects has in fact existed during the early prototypes of destination calculus, but we removed it as it doesn't solve the scope escape for destination and is indistinguishable in practice from the unit type.

\subsection{Age control to prevent scope escape of destinations}

The solution we chose is to instead track the age of destinations (as De-Brujin-like scope indices), and prevent a destination to escape into the parent scope when stored through age-control restriction on the typing rule of destination-feeding primitives.

Age is represented by a commutative semiring, where $[[ν]]$ indicates that a destination originates from the current scope, and $[[↑]]$ indicates that it originates from the scope just before. We also extend ages to variables (a variable of age $[[a]]$ stands for a value of age $[[a]]$). Finally, age $[[∞]]$ is introduced for variables standing in place of a non-age-controlled value. In particular, destinations can never have age $[[∞]]$ in practice.

Semiring addition $\ottsmode{+}$ is used to find the age of a variable or destination that is used in two different branches of a program. Semiring multiplication $[[·]]$ corresponds to age composition, and is in fact an integer sum on scope indices.
$[[∞]]$ is absorbing for both addition and multiplication.

Truth tables for operators $\ottsmode{+}$ and $\ottsmode{·}$ on ages are presented in Figure~\ref{fig:age-tables}.

\begin{figure}[h]\centering
We pose $[[↑]]^{0} = [[ν]]$ and $[[↑]]^{n} = [[↑]][[·]][[↑]]^{n-1}$

\medskip

\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{|c|c|c|}\hline
$\ottsmode{+}$ & $[[↑]]^{n}$               & $[[∞]]$ \\\hline
$[[↑]]^{m}$    & $\text{if }n = m\text{ then }[[↑]]^{n}\text{ else }[[∞]]$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$                   & $[[∞]]$ \\\hline
\end{tabular}
\end{minipage}\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{|c|c|c|}\hline
$[[·]]$        & $[[↑]]^{n}$               & $[[∞]]$ \\\hline
$[[↑]]^{m}$    & $[[↑]]^{n+m}$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$                   & $[[∞]]$ \\\hline
\end{tabular}
\end{minipage}

\caption{Truth tables for age operators}\label{fig:age-tables}
\end{figure}

Age commutative semiring is then combined with the multiplicity commutative semiring from~\cite{bernardy_linear_2018} to form a canonical product commutative semiring that is used to represent the mode of each typing context binding in our final type system.

The main restriction to prevent parent scope escape is materialized the simplified typing rules of Figure~\ref{fig:age-control-simpl}.

\begin{ottfig}{\caption{Simplified typing rules for age control of destinations}\label{fig:age-control-simpl}}
\ottusedrule{%
\ottdrule{%
}{
\ottshvar{\destminus} \ottshvar{h} :\!_{\!  \ottsmode{\nu}   }\ottstype{\lfloor}\ottstype{T} \ottstype{\rfloor}   \,\vdash\,   \ottshvar{\destminus} \ottshvar{h}   \ottsym{:}   \ottstype{\lfloor}\ottstype{T} \ottstype{\rfloor} }{%
{\ottdrulename{Ty\_val\_Dest$^{\star}$}}{}%
}}
\ottusedrule{%
\ottdrule{%
\ottpremise{\Theta_{{\mathrm{1}}}  \,\vdash\,  \ottnt{t}  \ottsym{:}   \ottstype{\lfloor} \ottstype{T} \ottstype{\rfloor} }%
\ottpremise{\Theta_{{\mathrm{2}}}  \,\vdash\,  \ottnt{t'}  \ottsym{:}  \ottstype{T}}%
}{
\Theta_{{\mathrm{1}}}  +  \ottsmode{\uparrow} \ottsmode{\hspace{-0.1ex}\cdot\hspace{-0.1ex} }  \Theta_{{\mathrm{2}}} \,\vdash\,   \ottnt{t} \triangleleft\!\, \ottnt{t'}   \ottsym{:}   \ottstype{1} }{%
{\ottdrulename{Ty\_term\_FillLeaf$^{\star}$}}{}%
}}
\end{ottfig}

Typing a destination $[[-h]]$ alone requires $[[-h]]$ to have age $[[ν]]$ in the context. And when storing a value through a destination, the ages of the value's dependencies in the context must be one higher than the corresponding ages required to type the value alone (this is the meaning of $\ottsmode{\uparrow} \ottsmode{\hspace{-0.1ex}\cdot\hspace{-0.1ex} }  \Theta_{{\mathrm{2}}}$).

Such a rule system prevents in particular the previous faulty expression $[[-hd ˢ⨞ -h]]$ where $[[-hd]]$ originates from the context parent to the one of $[[-h]]$.

\section{(Updated) breadth-first tree traversal}

The core example that showcases the power of destination-passing style programming with first-class destination is breadth-first tree traversal:

\begin{quote}
Given a tree, create a new one of the same shape, but with the values at the nodes replaced by the numbers $1\ldots|T|$ in breadth-first order.
\end{quote}

Indeed, breadth-first traversal implies that the order in which the structure must be populated (left-to-right, top-to-bottom) is not the same as the structural order of a functional binary tree i.e., building the leaves first and going up to the root.

In~\cite{bagrel_destination-passing_2024}, the author presents a breadth-first traversal implementation that relies on first-class destinations so as to build the final tree in a single pass over the input tree. His implementation, much like ours, uses a queue to store pairs of an input subtree and a destination to the corresponding output subtree. This queue is what materialize the breadth-first processing order: the leading pair $(\langle\text{\itshape input subtree}\rangle, \langle\text{\itshape dest to output subtree}\rangle)$ of the queue is processed, and its children pairs are added back at the end of the queue to be processed later.

However, as evoked earlier in Section~\ref{ssec:problem-stored-dests}, The API presented in~\cite{bagrel_destination-passing_2024} is not able to store linear data, and in particular destinations, in destination-based data structures. It is thus reliant on regular constructor-based Haskell data structures for destination storage.

This is quite impractical as we would like to use the efficient, destination-based queue implementation from Section~\ref{ssec:efficient-queue} to power up the breadth-first tree traversal implementation\footnote{This efficient queue implementation can be, and is in fact, implemented in~\cite{bagrel_destination-passing_2024}: see \url{archive.softwareheritage.org/swh:1:cnt:29e9d1fd48d94fa8503023bee0d607d281f512f8}. But it cannot store linear data}. In our present work fortunately, thanks to the finer age-control mechanism, we can store linear resources in destination-based structures without any issue. Our system is in fact self-contained, as any structure, whatever the use for it be, can be built using a small core of destination-based primitives (and regular data constructors can be retrieved from destination-based primitives, see Section~\ref{ssec:sugar}).

Figure~\ref{fig:impl-boilerplate-bfs} introduces a few extra tools needed for implementation of breadth-first tree traversal, while Figure~\ref{fig:impl-bfs} presents the actual implementation of the traversal. This implementation is as similar as possible to the one from~\cite{bagrel_destination-passing_2024}, as to make it easier to spot the few differences between the two systems.

\begin{codefig}{\caption{Boilerplate for breadth-first tree traversal}\label{fig:impl-boilerplate-bfs}}
\begin{minipage}[t]{0.68\textwidth}\vspace{-0.15cm}$
\newtype{[[Tree T]]}{[[① ⨁ (T ⨂ ((Tree T) ⨂ (Tree T)))]]}
\newoperator
  {\triangleleft\ottsctor{Nil}}{[[⌊ Tree T ⌋ n ¹ν → ①]]}
  {[[d ⨞ Nil]]}{[[d ⨞Inl ⨞()]]}
\newoperator
  {\triangleleft\ottsctor{Node}}{[[⌊ Tree T ⌋ n ¹ν → ⌊T⌋ n ⨂ (⌊ Tree T⌋ n ⨂ ⌊ Tree T⌋ n)]]}
  {[[d ⨞ Node]]}{[[(d ⨞Inr ⨞(,)) case ¹ν (dv, dtlr) ⟼ ˢ(dv, dtlr ⨞(,))]]}$\end{minipage}
\vrule height 0.02\textheight\hspace{0.5cm}
\begin{minipage}[t]{0.31\textwidth}$
\newtype[~\mathttbf{rec}]{[[Nat]]}{[[① ⨁ Nat]]}
\newoperator
  {\ottkw{zero}}{[[Nat]]}
  {[[zero]]}{[[Inl ()]]}
\newoperator
  {\ottkw{succ}}{[[Nat ¹ν → Nat]]}
  {[[succ x]]}{[[ˢInr x]]}$\end{minipage}
\end{codefig}

\begin{codefig}{\caption{Breadth-first tree traversal in destination-passing style}\label{fig:impl-bfs}}
\newoperatorb[~\mathttbf{rec}]
{\ottkw{go}}{[[((! ¹∞ S) ¹ν → T1 ¹ν → (! ¹∞ S) ⨂ T2) ων→ (! ¹∞ S) ¹ν → Queue (Tree T1 ⨂ ⌊ Tree T2 ⌋ ¹ν) ¹ν → (! ¹∞ S)]]}
{[[go f st q]]}{\!\!\!\begin{array}[t]{l}[[
(dequeue q) case ¹ν {⮒
‥‥Inl () ⟼ st,⮒
‥‥ˢInr ˢ(ˢ(tree, dtree), q') ⟼ tree case ¹ν {⮒
‥‥‥‥Inl () ⟼ dtree ⨞ Nil ; go f st q',⮒
‥‥‥‥ˢInr ˢ(x, ˢ(tl, tr)) ⟼ (dtree ⨞ Node) case ¹ν⮒
‥‥‥‥‥‥ˢ(dy, ˢ(dtl, dtr)) ⟼ (༼f st༽ x) case ¹ν⮒
‥‥‥‥‥‥‥‥(st', y) ⟼ dy ˢ⨞ y ; go f st' (⮒
‥‥‥‥‥‥‥‥‥‥q' enqueue ˢ(tl, dtl) enqueue ˢ(tr, dtr)⮒
‥‥‥‥‥‥‥‥)⮒
‥‥}⮒
}
]]\end{array}}
\newoperatorb
{\ottkw{mapAccumBFS}}{[[((! ¹∞ S) ¹ν → T1 ¹ν → (! ¹∞ S) ⨂ T2) ων→ (! ¹∞ S) ¹ν → Tree T1 ¹ν → Tree T2 ⨂ (! ¹∞ S)]]}
{[[mapAccumBFS f st tree]]}{\!\!\!\begin{array}[t]{l}[[
from⧔ (alloc map dtree ⟼ go f st (singleton ˢ(tree, dtree)))
]]\end{array}}
\newoperatorb
{\ottkw{relabelDPS}}{[[Tree ① ¹ν → (Tree Nat) ⨂ (! ¹∞ (! ων Nat))]]}
{[[relabelDPS tree]]}{\!\!\!\begin{array}[t]{l}[[
mapAccumBFS⮒
‥‥(ˢλ ex ¹ν ⟼ ˢλ un ¹ν ⟼ un ; ex case ¹ν⮒
‥‥‥‥ᴇ ¹∞ ex' ⟼ ex' case ¹∞⮒
‥‥‥‥‥‥ᴇ ων st ⟼ ˢ(ˢᴇ ¹∞ (ˢᴇ ων (succ st)), st))⮒
‥‥(ˢᴇ ¹∞ (ˢᴇ ων (succ zero)))⮒
‥‥tree
]]\end{array}}
\end{codefig}

The first important difference is that in the destination calculus implementation, the input tree of type $[[Tree T1]]$ is consumed linearly. The stateful transformer that is applied to each input node to get the output node value is also linear in its two arguments. The state has to wrapped in exponential $\ottstype{!_{[[¹∞]]}}$ so that it can be extracted from the right side of the ampar at the end of the processing (with $\ottkw{from}_{\ottstype{\ltimes}}$). We could imagine a more general version of the traversal, having no constraint on the state type, but necessitating a finalization function $[[S ¹ν → ! ¹∞ S']]$ so that the final state can be returned.

The $\ottkw{go}$ function is in charge of consuming the queue containing the pairs of input subtrees and destinations to the corresponding output subtrees. It dequeues the first pair, and processes it. If the input subtree is $\ottsctor{Nil}$, it feeds $\ottsctor{Nil}$ to the destination for the output tree and continues the processing of next elements with unchanged state. If the input subtree is a node, it writes a hollow $\ottsctor{Node}$ constructor to the hole pointed to by the destination, processes the value of the node with the stateful transformer $[[f]]$, and continues the processing of the updated queue where children subtrees and their accompanying destinations have been enqueued.

$\ottkw{mapAccumBFS}$ is in charge of spawning the initial memory slot for the output tree together with the associated destination, and preparing the initial queue containing a single pair, made of the whole input tree and the aforementioned destination.

$\ottkw{relabelDPS}$ is a special case of $\ottkw{mapAccumBFS}$ that takes the skeleton of a tree (where node values are all unit) and returns a tree of integers, with the same skeleton, but with node values replaced by naturals $1\ldots|T|$ in breadth-first order. The higher-order function passed to $\ottkw{mapAccumBFS}$ is quite verbose: it must consume the previous node value (unit), using $\fatsemi$, then extract the state (representing the next natural number to attribute to a node) from its two nested exponential constructors, and finally return a pair, whose left side is the incremented natural wrapped back into its two exponential layers (new state), and whose right side is the plain natural representing the node value for the output subtree.

Two exponentials are needed here. The first one $\ottstype{!}_{[[¹∞]]}$ is part of $\ottkw{from}_{\ottstype{\ltimes}}$ contract, and ensures that the state cannot capture destinations, so that it can be returned at the end of the processing (as $\ottkw{go}$ runs under a $\ottkw{map}$ over an ampar). The second one $\ottstype{!}_{[[ων]]}$ allows the natural number to be used in a non-linear fashion: it is used once in $[[succ st]]$ to serve as the new state, and another time as the value for the output node. With a more general $\ottkw{from}_{\ottstype{\ltimes}}$ operator, we would be able to use just one exponential layer $\ottstype{!}_{[[ω∞]]}$ over the natural number to achieve the same result.

\section{Language syntax}

\subsection{Introducing the \emph{ampar}}\label{ssec:ampar-motivation}

Minamide's work\cite{minamide_functional_1998} is the earliest record we could find of a functional calculus integrating the idea of incomplete data structures (structures with holes) that exist as first class values and can be interacted with by the user.

In that paper, a structure with a hole is named \emph{hole abstraction}. In the body of a hole abstraction, the bound \emph{hole variable} should be used linearly (exactly once), and must only be used as a parameter of a data constructor. In other terms, the bound \emph{hole variable} cannot be pattern-matched on or used as a parameter of a function call. A hole abstraction is thus a weak form of linear lambda abstraction, which just moves a piece of data into a bigger data structure.

In fact, the type of hole abstraction $\ottstype{([[T1]], [[T2]]) hfun}$ in Minamine's work shares a lot of similarity with the separating implication or \emph{magic wand} $\ottstype{[[T1]] \sepimp [[T2]]}$ from separation logic: given a piece of memory matching description $[[T1]]$, we obtain a (complete) piece of memory matching description $[[T2]]$.

Now, in classical linear logic, we know we can transform linear implication $\ottstype{[[T1]] \multimap [[T2]]}$ into $\ottstype{[[T1]]^{\bot}~\parr~[[T2]]}$. Doing so for the \emph{wand} type $\ottstype{([[T1]], [[T2]]) hfun}$ or $\ottstype{[[T1]] \sepimp [[T2]]}$ gives $\ottstype{\lfloor[[T1]]\rfloor~ \widehat{\parr}~[[T2]]}$, where $\ottstype{\lfloor\smallbullet\rfloor}$ is memory negation, and $\ottstype{\widehat{\parr}}$ is a memory \emph{par} (weaker than the CLL \emph{par} that allows more ``interaction'' of its two sides).

Transforming the hole abstraction from its original implication form to a \emph{par} form let us consider the type $\ottstype{\lfloor[[T1]]\rfloor}$ of \emph{sink} or \emph{destination} of $[[T1]]$ as a first class component of our calculus. We also get to see the hole abstraction aka memory par as a pair-like structure, where the two sides might be coupled together in a way that prevent using both of them simultaneously.

\paragraph{From memory par $\ottstype{\widehat{\parr}}$ to ampar $\ottstype{\ltimes}$}

In CLL, the cut rule states that given $\ottstype{[[T1]]~\parr~[[T2]]}$, we can free up $[[T1]]$ by providing an eliminator of $[[T2]]$, or free up $[[T2]]$ by providing an eliminator of $[[T1]]$. The eliminator of $[[T]]$ can be $\ottstype{[[T]]^\bot}$, or $\ottstype{[[T]]^{\bot^{-1}}} = [[T']]$ if $[[T]]$ is already of the form $\ottstype{[[T']]^\bot}$. In a classical setting, thanks to the involutive nature of negation $\ottstype{\smallbullet^\bot}$, the two potential forms of the eliminator of $[[T]]$ are equal.

In destination calculus though, we don't have an involutive memory negation $\ottstype{\lfloor\smallbullet\rfloor}$. If we are provided with a destination of destination $[[-h']] \pmb{:} \ottstype{\lfloor\lfloor[[T]]\rfloor\rfloor}$, we know that some structure is expecting to store a destination of type $\ottstype{\lfloor[[T]]\rfloor}$. If ever that structure is consumed, then the destination stored inside will have to be fed with a value (remember we are in a linear calculus). So if we allocate a new memory slot of type $[[+h]] \pmb{:} [[T]]$ and its linked destination $[[-h]] \pmb{:} \ottstype{\lfloor[[T]]\rfloor}$, and write $[[-h]]$ to the memory slot pointed to by $[[-h']]$, then we can get back a value of type $[[T]]$ at $[[+h]]$ if ever the structure pointed to by $[[-h']]$ is consumed. Thus, a destination of destination is only equivalent to the promise of an eventual value, not an immediate usable one.

As a result, in destination calculus, we cannot have the same kind of cut rule as in CLL. This is, in fact, the part of destination calculus that was the hardest to design, and the source of a lot of early errors. For a destination of type $\ottstype{\lfloor[[T]]\rfloor}$, both storing it through a destination of destination $\ottstype{\lfloor\lfloor[[T]]\rfloor\rfloor}$ or using it to store a value of type $[[T]]$ constitute a linear use of the destination. But only the latter is a genuine consumption in the sense that it guarantees that the hole associated to the destination has been filled! Storing away the destination of type $\ottstype{\lfloor[[T]]\rfloor}$ originating from $\ottstype{[[T]]~\widehat{\parr}~\lfloor[[T]]\rfloor}$ (through a destination of destination of type $\ottstype{\lfloor\lfloor[[T]]\rfloor\rfloor}$) should not allow to free up the $[[T]]$, as it would in a CLL-like setting.

However, we can recover a memory abstraction that is usable in practice if we know the nature of an memory par side:
\begin{itemize}
\item if the memory par side is a value made only of inert elements and destinations (negative polarity), then we can pattern-match/$\ottkw{map}$ on it, but we cannot store it away to free up the other side;
\item if the memory par side is a value made only of inert elements and holes (positive polarity), then we can store it away in a bigger struct and free up the associated destinations (this is not an issue as the bigger struct will be locked by an memory par too), but we cannot pattern-match/$\ottkw{map}$ on it as it (may) contains holes;
\item if one memory par side is only made of inert elements, we can in fact convert the memory par to a pair, as the memory par doesn't have any form of interaction between its sides.
\end{itemize}

It is important to note that the type of an memory par side is not really enough to determine the nature of the side, as a hole of type $[[T]]$ and and inert value of type $[[T]]$ are indistinguishable at the type level.

So we introduced a more restricted form of memory par, named \emph{ampar} ($\ottstype{\ltimes}$), for \emph{asymmetrical memory par}, in which:
\begin{itemize}
\item the left side is made of inert elements (normal values or destinations from previous scopes) and/or holes if and only if those holes are compensated by destinations on the right side;
\item the right side is made of inert elements and/or destinations.
\end{itemize}

As the right side cannot contain any holes, it is always safe to pattern-match or $\ottkw{map}$ on it. Because the left side cannot contain destinations from the current scope, it is always safe to store it away in a bigger struct and release the right side.

Finally, it is enough to check for the absence of destinations in the right side (which we can do easily just by looking at its type) to convert an \emph{ampar} to a pair, as any remaining hole on the left side would be compensated by a destination on the right side.

\paragraph{Destinations from previous scopes are inert}

In destination calculus, scopes are delimited by the $\ottkw{map}$ operation over ampars. Anytime a $\ottkw{map}$ happens, we enter a new scope, and any preexisting destination or variable see its age increased by one ($\ottsmode{[[↑]]}$). As soon as a destination or variable is no longer of age 0 ($\ottsmode{[[ν]]}$), it cannot be used actively but only passively (e.g. it cannot be applied if it is a function, or used to store a value if it is a destination, but it can be stored away in a dest, or pattern-matched on).

This is a core feature of the language that ensures part of its safety.

\subsection{Names and variables}

The destination calculus uses two classes of names: regular variable names $[[x]], [[y]]$, and hole names, $[[h]], [[h1]], [[h2]]$ which represents the identifier or address of a memory cell that hasn't been written to yet, as illustrated in Figure~\ref{fig:grammar-names}.

Hole names are represented by natural numbers under the hood, so they can act both as relative offsets or absolute positions in memory. Typically, when a structure is effectively allocated, its hole (and destination) names are shifted by the maximum hole name encountered so far in the program ; this corresponds to finding the next unused memory cell in which to write new data.

We sometimes need to keep track of hole names bound by a particular runtime value or evaluation context, hence we also define sets of hole names $[[H]], [[H1]], [[H2]]\ldots$.

Shifting all hole names in a set by a given offset $[[h']]$ is denoted $[[H ⩲ h']]$. We also define a conditional shift operation $\ottshvar{[}[[H ⩲ h']]\ottshvar{]}$ which shifts each hole name appearing in the operand to the left of the brackets by $[[h']]$ if this hole name is also member of $[[H]]$. This conditional shift can be used on a single hole name, a value, or a typing context.

\begin{ottfig}{\caption{Grammar for variable, hole and destination names}\label{fig:grammar-names}}
\ottmetavars
\ottgrammartabular{
\otthvar\ottinterrule
\otthvars\ottafterlastrule
}
\end{ottfig}

\subsection{Term and value core syntax}\label{ssec:core-syntax}

Destination calculus is based on linear simply-typed $\lambda$-calculus, with built-in support for sums, pairs, and exponentials. The syntax of terms, which is presented in Figure~\ref{fig:grammar-terms} is quite unusual, as we need to introduce all the tooling required to manipulate destinations, which constitute the primitive way of building a data structures for the user.

\begin{ottfig}{\caption{Grammar for terms}\label{fig:grammar-terms}}
\ottgrammartabular{
\ottterm\ottinterrule
\ottval\ottafterlastrule
}
\end{ottfig}

In fact, the grammatical class of values $[[v]]$, presented as a subset of terms $[[t]]$, could almost be removed completely from the user syntax, and just used as a denotation for runtime data structures. We only need to keep the \emph{ampar} value $[[{ h } ⟨ +h ❟ -h ⟩]]$ as part of the user syntax as a way to spawn a fresh memory cell to be later filled using destination-feeding primitives (see $[[alloc]]$ in Section~\ref{ssec:sugar}).

Pattern-matching on every type of structure (except unit) is parametrized by a mode $[[m]]$ to which the scrutinee is consumed. The variables which bind the subcomponents of the scrutinee then inherit this mode. In particular, this choice crystalize the equivalence $[[! ωa (T1 ⨂ T2)]] \simeq [[(! ωa T1) ⨂ (! ωa T2)]]$, which is not part of intuitionistic linear logic, but valid in Linear Haskell\cite{bernardy_linear_2018}.

$\ottkw{map}$ is the main primitive to operate on an \emph{ampar}, which represents an incomplete data structure whose building is in progress. $\ottkw{map}$ binds the right-hand side of the \emph{ampar} --- the one containing destinations of that \emph{ampar} --- to a variable, allowing those destinations to be operated on by destination-filling primitives. The left-hand side of the \emph{ampar} is inaccessible as it is being mutated behind the scenes by the destination-feeding primitives.

$\ottkw{to}_{\ottstype{\ltimes}}$ embeds an already completed structure in an \emph{ampar} whose left side is the structure, and right side is unit. We have an operator \textsc{FillComp} ($\triangleleft\mybullet$) allowing to compose two \emph{ampar}s by writing the root of the second one to a destination of the first one, so by throwing $\ottkw{to}_{\ottstype{\ltimes}}$ to the mix, we can compose an \emph{ampar} with a normal (completed) structure (see the sugar operator \textsc{FillLeaf} ($\triangleleft$) in Section~\ref{ssec:sugar}).

$\ottkw{from}_{\ottstype{\ltimes}}$ is used to convert an \emph{ampar} to a pair, when the right side of the \emph{ampar} is an exponential of the form $[[ᴇ ¹∞ v]]$. Indeed, when the right side has such form, it cannot contains destinations (as destinations always have a finite age), thus it cannot contain holes in its left side either (as holes on the left side are always compensated 1:1 by a destination on the right side). As a result, it is valid to convert an \emph{ampar} to a pair in these circumstances. $\ottkw{from}_{\ottstype{\ltimes}}$ is in particular used to extract a structure from its \emph{ampar} building shell when it is complete (see the sugar operator $\ottkw{from'}_{\ottstype{\ltimes}}$ in Section~\ref{ssec:sugar}).

The remaining term operators $[[⨞]][[()]], [[⨞]][[Inl]], [[⨞]][[Inr]], [[⨞]]\,\expcons{[[m]]}, [[⨞]][[(,)]], [[⨞]](\lamnt{[[x]]}{[[m]]}{[[u]]})$ are all destination-feeding primitives. They write a layer of value/constructor to the hole pointed by the destination operand, and return the potential new destinations that are created in the process (or unit if there is none).

\subsection{Syntactic sugar for constructors and commonly used operations}\label{ssec:sugar}

As we said in section~\ref{ssec:core-syntax}, the grammatical class of values is mostly used for runtime only; in particular, data constructors in the value class can only take other values as arguments, not terms (this help us ensure that no free variable can appear in a value). Thus we introduce syntactic for data constructors taking arbitrary terms as parameters (as we often find in functional programming languages) using destination-feeding primitives in Figure~\ref{fig:sugar}.

\begin{ottfig}{\caption{Syntactic sugar forms for terms}\label{fig:sugar}}
\ottgrammartabular{
\ottsterm\ottafterlastrule
}
\end{ottfig}

$\ottkw{from}_{\ottstype{\ltimes}'}$ is a simpler variant of $\ottkw{from}_{\ottstype{\ltimes}}$ that allows to extract the right side of an ampar when the right side has been fully consumed. We implement it in terms of $\ottkw{from}_{\ottstype{\ltimes}}$ to keep the core calculus tidier (and limit the number of typing rules, evaluation contexts, etc), but it can be implemented much more efficiently in a real-world implementation.

All the desugarings are presented in Figure~\ref{fig:desugaring}.

\begin{codefig}{\caption{Desugaring of syntactic sugar forms for terms}\label{fig:desugaring}}
\begin{minipage}[t]{0.62\textwidth}$
[[alloc]] \btriangleq \!\!\!\begin{array}[t]{l}[[
{ 1 } ⟨ +1 ❟ -1 ⟩
]]\end{array}\\[\interdefskip]
[[from⧔' t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
(from⧔ (t map un ⟼ un ; ᴇ ¹∞ () )) case ¹ν⮒
‥‥( st , ex ) ⟼ ex case ¹ν⮒
‥‥‥‥ᴇ ¹∞ un ⟼ un ; st
]]\end{array}\\[\interdefskip]
[[ˢλ x m ⟼ u]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ ( λ x m ⟼ u )⮒
)
]]\end{array}\\[\interdefskip]
[[ˢ( t1 , t2 )]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥(d ⨞ (,)) case ¹ν⮒
‥‥‥‥‥‥( d1 , d2 ) ⟼ d1 ˢ⨞ t1 ; d2 ˢ⨞ t2⮒
)
]]\end{array}$\end{minipage}
\vrule height 0.01\textheight\hspace{0.5cm}
\begin{minipage}[t]{0.38\textwidth}$
[[t ˢ⨞ t']] \btriangleq \!\!\!\begin{array}[t]{l}[[
t ⨞· (to⧔ t')
]]\end{array}\\[\interdefskip]
[[ˢInl t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ Inl ˢ⨞ t⮒
)
]]\end{array}\\[\interdefskip]
[[ˢInr t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ Inr ˢ⨞ t⮒
)
]]\end{array}\\[\interdefskip]
[[ˢᴇ m t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ ᴇ m ˢ⨞ t⮒
)
]]\end{array}$\end{minipage}
\end{codefig}

\section{Type system}

\subsection{Syntax for types, modes, and typing contexts}

\ottgrammartabular{
\otttype\ottinterrule
\ottmode\ottinterrule
\ottmul\ottinterrule
\ottage\ottinterrule
\ottctx\ottafterlastrule
}

\subsection{Typing of terms and values}

\ottdefnTyXXval{}
\ottdefnTyXXterm{}

\subsection{Derived typing rules for syntactic sugar forms}

\ottdefnTyXXsterm{}

\section{Evaluation contexts and semantics}

\subsection{Evaluation contexts forms}

\ottgrammartabular{
\ottectx\ottinterrule
\ottectxs\ottafterlastrule
}

\subsection{Typing of evaluation contexts and commands}

\ottdefnTyXXectxs{}
\ottdefnTy{}

\subsection{Small-step semantics}

\ottdefnSem{}

\section{Proof of type safety using Coq proof assistant}

\begin{itemize}
\item Not particularly elegant. Max number of goals observed 232
(solved by a single call to the \verb|congruence| tactic). When you
have a computer, brute force is a viable strategy. (in particular,
no semiring formalisation, it was quicker to do directly)
\item Rules generated by ott, same as in the article (up to some
notational difference). Contexts are not generated purely by syntax,
and are interpreted in a semantic domain (finite functions).
\item Reasoning on closed terms avoids almost all complications on
binder manipulation. Makes proofs tractable.
\item Finite functions: making a custom library was less headache than
using existing libraries (including \verb|MMap|). Existing libraries
don't provide some of the tools that we needed, but the most important
factor ended up being the need for a modicum of dependency between
key and value. There wasn't really that out there. Backed by actual
functions for simplicity; cost: equality is complicated.
\item Most of the proofs done by author with very little prior
experience to Coq.
\item Did proofs in Coq because context manipulations are tricky.
\item Context sum made total by adding an extra invalid \emph{mode}
(rather than an extra context). It seems to be much simpler this
way.
\item It might be a good idea to provide statistics on the number of
lemmas and size of Coq codebase.
\item (possibly) renaming as permutation, inspired by nominal sets,
make more lemmas don't require a condition (but some lemmas that
wouldn't in a straight renaming do in exchange).
\item (possibly) methodology: assume a lot of lemmas, prove main
theorem, prove assumptions, some wrong, fix. A number of wrong lemma
initially assumed, but replacing them by correct variant was always
easy to fix in proofs.
\item Axioms that we use and why (in particular setoid equality not
very natural with ott-generated typing rules).
\item Talk about the use and benefits of Copilot.
\end{itemize}

\section{Implementation of destination calculus using in-place memory mutations}

What needs to be changed (e.g. linear alloc)

\section{Related work}

\section{Conclusion and future work}

\clearpage{}
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}{}

\end{document}

% Local Variables:
% eval: (auto-fill-mode 0)
% eval: (visual-line-mode 1)
% End: