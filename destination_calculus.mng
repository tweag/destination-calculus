% -*- latex -*-
\documentclass[acmsmall, screen, review]{acmart}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\let\Bbbk\relax % needed because of a conflict between amssymb and newtx
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{environ}
\usepackage[capitalize, noabbrev]{cleveref}
\usepackage{xifthen}
\usepackage{expl3}
\usepackage{xparse}
\citestyle{acmauthoryear}

% For faulty code
\usepackage[normalem]{ulem}
\makeatletter
\def\uwave{\bgroup \markoverwith{\lower3.5\p@\hbox{\sixly \textcolor{red}{\char58}}}\ULon}
\font\sixly=lasy6 % does not re-load if already loaded, so no memory problem.
\makeatother

% For OTT rendering
\usepackage[supertabular]{ottalt}
\inputott{destination_calculus_ott.tex}
\usepackage{ottstyling}
% Hide "Index for ranges" from the metavars displayed tabular
\patchcmd{\ottmetavars}{$ \ottmv{k} $ & \ottcom{Index for ranges} \\}{}{}{}
\patchcmd{\ottdruleTyXXectxsXXOpenAmpar}{%
\ottpremise{\texttt{LinOnly }\Delta_{{\mathrm{3}}}\texttt{ }}%
\ottpremise{\texttt{FinAgeOnly }\Delta_{{\mathrm{3}}}\texttt{ }}%
}{\ottpremise{\texttt{LinOnly }\Delta_{{\mathrm{3}}}\texttt{ }\quad\texttt{FinAgeOnly }\Delta_{{\mathrm{3}}}}}{}{}
\patchcmd{\ottdruleTyXXvalXXAmpar}{%
\ottpremise{\texttt{LinOnly }\Delta_{{\mathrm{3}}}\texttt{ }}%
\ottpremise{\texttt{FinAgeOnly }\Delta_{{\mathrm{3}}}\texttt{ }}%
}{\ottpremise{\texttt{LinOnly }\Delta_{{\mathrm{3}}}\texttt{ }%
\quad\quad%
\texttt{FinAgeOnly }\Delta_{{\mathrm{3}}}}}{}{}
\patchcmd{\ottdruleTyXXtermXXPatS}{%
\ottpremise{ \Theta_{{\mathrm{2}}} ,~  \ottmv{x_{{\mathrm{1}}}} :\!_{\! \ottsmode{m} } \ottstype{T_{{\mathrm{1}}}}    \,\pmb{\vdash}\,  \ottnt{u_{{\mathrm{1}}}}  \pmb{:}  \ottstype{U}}%
\ottpremise{ \Theta_{{\mathrm{2}}} ,~  \ottmv{x_{{\mathrm{2}}}} :\!_{\! \ottsmode{m} } \ottstype{T_{{\mathrm{2}}}}    \,\pmb{\vdash}\,  \ottnt{u_{{\mathrm{2}}}}  \pmb{:}  \ottstype{U}}%
}{\ottpremise{ \Theta_{{\mathrm{2}}} ,~  \ottmv{x_{{\mathrm{1}}}} :\!_{\! \ottsmode{m} } \ottstype{T_{{\mathrm{1}}}}    \,\pmb{\vdash}\,  \ottnt{u_{{\mathrm{1}}}}  \pmb{:}  \ottstype{U}%
\quad\quad%
\Theta_{{\mathrm{2}}} ,~  \ottmv{x_{{\mathrm{2}}}} :\!_{\! \ottsmode{m} } \ottstype{T_{{\mathrm{2}}}}    \,\pmb{\vdash}\,  \ottnt{u_{{\mathrm{2}}}}  \pmb{:}  \ottstype{U}}}{}{}

\usepackage{tikzit}
\definecolor{sczcolor}{RGB}{0,0,0}
\definecolor{scicolor}{RGB}{60, 103, 163}
\definecolor{sciicolor}{RGB}{199, 22, 6}
\definecolor{sciiicolor}{RGB}{97, 5, 94}
\newcommand{\scz}[1]{\textcolor{sczcolor}{#1}}
\newcommand{\sci}[1]{\textcolor{scicolor}{#1}}
\newcommand{\scii}[1]{\textcolor{sciicolor}{#1}}
\newcommand{\sciii}[1]{\textcolor{sciiicolor}{#1}}
\input{style.tikzstyles}
% \setlength\textfloatsep{\baselineskip}
% \setlength{\intextsep}{\baselineskip}

%%%%%%%%%%%%%%%%% Editing marks %%%%%%%%%%%%%%%%%

  % TOGGLE ME to turn off all the commentary:
  \InputIfFileExists{no-editing-marks}{
    \def\noeditingmarks{}
  }

  \usepackage{xargs}
  \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
  % ^^ Need for pgfsyspdfmark apparently?
  \ifx\noeditingmarks\undefined
      % Adapting to acmart's small margins
      \setlength{\marginparsep}{0.3em}
      \setlength{\marginparwidth}{1.4cm}

      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{{\color{blue}{#1}}}
      \newcommand{\note}[1]{{\color{blue}{\begin{itemize} \item {#1} \end{itemize}}}}
      \newenvironment{alt}{\color{red}}{}

      \newcommandx{\unsure}[2][1=]{\todo[linecolor=orange,backgroundcolor=orange!25,bordercolor=orange,#1]{#2}}
      \newcommandx{\info}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{#2}}
      \newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
      \newcommandx{\inconsistent}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
      \newcommandx{\critical}[2][1=]{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}
      \newcommand{\improvement}[1]{\todo[linecolor=pink,backgroundcolor=pink!25,bordercolor=pink]{#1}}
      \newcommandx{\resolved}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}} % use this to mark a resolved question
  \else
  %    \newcommand{\Red}[1]{#1}
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{#1}
      \newcommand{\note}[1]{}
      \newenvironment{alt}{}{}
  %    \renewcommand\todo[2]{}
      \newcommand{\unsure}[2][1=]{}
      \newcommand{\info}[2][1=]{}
      \newcommand{\change}[2]{}
      \newcommand{\inconsistent}[2]{}
      \newcommand{\critical}[2]{}
      \newcommand{\improvement}[1]{}
      \newcommand{\resolved}[2]{}

  \fi

%%%%%%%%%%%%%%%%% /Editing marks %%%%%%%%%%%%%%%%%

\newcommand{\TODO}[1]{\textnormal{\textcolor{red}{TODO: #1} } }
\newcommand\sepimp{\mathrel{-\mkern-6mu*}}
\newcommand{\textopname}[1]{``#1''}
\newcommand{\parr}{\rotatebox[origin=c]{180}{\&}}
\makeatletter
\newcommand{\smallbullet}{} % for safety
\DeclareRobustCommand\smallbullet{%
\mathord{\mathpalette\smallbullet@{0.5}}%
}
\newcommand{\smallbullet@}[2]{%
\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}%
}
\makeatother

\makeatletter
\newcommand{\oset}[3][0ex]{%
\mathrel{\mathop{#3}\limits^{
  \vbox to#1{\kern-2\ex@
  \hbox{$\scriptstyle#2$}\vss}}}}
\makeatother

\def\mycasem#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{}{#1}}
\def\myfunvm#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{\,\,}{#1}}
\def\myfuntm#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{\,}{#1}}
\def\mydestm#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{}{#1}}
\def\mymul#1{\ifthenelse{\equal{#1}{[[¹]]}}{}{#1}}

\newcommand{\destcalculus}{\ensuremath{\lambda_d}}
\newcommand{\destcalculusinplace}{\ensuremath{\lambda_{d\,\ottkw{ip}}}}
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newcommand\btriangleq{\pmb{\triangleq}}
\newcommand\btriangleqrec{\oset{\mathsf{rec}}{\pmb{\triangleq}}}
\newlength{\interdefskip}
\setlength{\interdefskip}{0.1cm}
\newcommand{\newtype}[3][]{#2~\ifthenelse{\equal{#1}{}}{\btriangleq}{\btriangleqrec}~#3\\[\interdefskip]}
\newcommand{\newoperator}[5][]{\phantom{a}\!\!\!\!\!\!\begin{array}[t]{l}%
#2 ~\pmb{:}~ #3 \\
#4 ~\ifthenelse{\equal{#1}{}}{\btriangleq}{\btriangleqrec}~ #5
\end{array}\\[\interdefskip]}

\newcommand{\newoperatorb}[5][]{\phantom{a}\!\!\!\!\!\!\begin{array}[t]{l}%
#2 ~\pmb{:}~ #3 \\
#4 ~\ifthenelse{\equal{#1}{}}{\btriangleq}{\btriangleqrec}~\\\myspace{1}#5
\end{array}\\[\interdefskip]}
\newcommand{\figureratio}{0.9}
\newcommand{\codehere}[2][t]{\begin{center}\begin{minipage}[#1]{\figureratio\linewidth}{\small$#2$}\end{minipage}\end{center}}
\NewEnviron{codefig}[2][t]{\begin{figure}[#1]
\codehere{\BODY}#2
\end{figure}}
\NewEnviron{ottfig}[2][t]{\begin{figure}[#1]\visiblespaces
\small\BODY\activespaces #2
\end{figure}}
\newcommand{\sidebysidecodehere}[4]{\begin{center}\begin{minipage}[#1]{\figureratio\linewidth}
\noindent\begin{minipage}[#1]{#2\linewidth-0.03\linewidth}{\small$#3$}\end{minipage}
\hfill
\vrule width 0.5pt % Vertical rule of 1pt width
\hfill
\begin{minipage}[#1]{\linewidth-#2\linewidth-0.03\linewidth}{\small$#4$}\end{minipage}
\end{minipage}\end{center}
}

\newcommand{\sidebysidecodefig}[6][t]{
\begin{figure}[#1]
\sidebysidecodehere{#3}{#4}{#5}{#6}
#2
\end{figure}
}

\newenvironment{stretchedarray}[2][1]
  {\bgroup\renewcommand*{\arraystretch}{#1}\begin{array}{#2}}
  {\end{array}\egroup}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal's boilerplate
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}


%%
%% These commands are for a JOURNAL article.
\acmJournal{JACM}
\acmVolume{XX}
\acmNumber{XX}
\acmArticle{XXX}
\acmMonth{1}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\acmJournal{PACMPL}
\title{Destination calculus}
\subtitle{A linear $\lambda-$calculus for pure, functional memory updates}

\author{Arnaud Spiwack}
\orcid{0000-0002-5985-2086}
\affiliation{
\institution{Tweag}
\department{OSPO}
\position{Director, Research}
\city{Paris}
\country{France}
}
\email{arnaud.spiwack@tweag.io}

\author{Thomas Bagrel}
\orcid{0009-0008-8700-2741}
\affiliation{
\institution{LORIA/Inria}
\department{MOSEL VERIDIS}
\city{Nancy}
\country{France}
}
\affiliation{
\institution{Tweag}
\department{OSPO}
\city{Paris}
\country{France}
}
\email{thomas.bagrel@loria.fr}
\email{thomas.bagrel@tweag.io}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003752.10010124.10010125.10010130</concept_id>
<concept_desc>Theory of computation~Type structures</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011039</concept_id>
<concept_desc>Software and its engineering~Formal language definitions</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008.10011009.10011012</concept_id>
<concept_desc>Software and its engineering~Functional languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008.10011024.10011028</concept_id>
<concept_desc>Software and its engineering~Data types and structures</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Type structures}
\ccsdesc[500]{Software and its engineering~Formal language definitions}
\ccsdesc[500]{Software and its engineering~Functional languages}
\ccsdesc[500]{Software and its engineering~Data types and structures}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{destination, functional programming, linear types, pure language}


\begin{abstract}
  Destination-passing---aka out-parameters---is taking a parameter to fill rather than returning a result from a function. Due to its apparent imperative nature, destination-passing has struggled to find its way to pure functional programming. In this paper, we present a pure core calculus with destinations. Our calculus subsumes all the existing systems, and can be used to reason about their correctness or extension. In addition our calculus can express programs that were previously not known to be expressible in a pure language. This is guaranteed by a modal type system where modes are used to represent both linear types and a system of ages to manage scopes. Type safety of our core calculus was largely proved formally with the Coq proof assistant.
\end{abstract}

\maketitle

%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{sec:intro}

In destination-passing style a function doesn't return a value: it takes as an argument a location where the value ought to be returned. In our notation, a function of type $[[T ¹ν → U]]$ would, in destination-passing style, have type $[[T ¹ν → ⌊ U ⌋ ¹ν ¹ν → ①]]$ instead. This style is common in systems programming, where destinations $[[⌊ U ⌋ ¹ν]]$ are more commonly known as “out parameters”. In C, $[[⌊ U ⌋ ¹ν]]$ would typically be a pointer of type $\ottstype{\mathtt{U\,*}}$.

The reason why system programs rely on destinations so much is that using destinations can save calls to the memory allocator. If a function returns a $[[U]]$, it has to allocate the space for a $[[U]]$. But with destinations, the caller is responsible for finding space for a $[[U]]$. The caller may simply ask for the space to the memory allocator, in which case we've saved nothing; but it can also reuse the space of an existing $[[U]]$ which it doesn't need anymore, or it could use a space in an array, or it could allocate the space in a region of memory that the memory allocator doesn't have access to, like a memory-mapped file.

This does all sound quite imperative, but we argue that the same consideration are relevant for functional programming, albeit to a lesser extent. In fact~\cite{shaikhha_destination-passing_2017} has demonstrated that using destination-passing in the intermediate language of a functional array-programming language allowed for some significant optimizations. Where destination truly shine in functional programming, however, is that they increase the expressiveness of the language; destinations allow for meaningfully new programs to be written. This point was first explored in~\cite{bagrel_destination-passing_2024}.

The trouble, of course, is that destination are imperative; we wouldn't want have to sacrifice the immutability of our linked data structure (we'll usually just say “structure”) for the sake of the more situational destinations. The goal is to extend functional programming just enough to be able to build immutable structures by destination passing without endangering purity and memory safety. This is precisely what~\cite{bagrel_destination-passing_2024} does, using a linear type system to restrict mutation. Destination become write-once references into an immutable structure with holes. In that we follow their leads, but we refine the type system further to allow for even more programs, as we discuss in \cref{sec:scope-escape-dests}.

There are two key elements to the expressiveness of destination-passing:
\begin{itemize}
\item structures can be built in any order. Not only from the leaves to the root, like in ordinary functional programming, but also from the root to the leaves, or any combination thereof. This can be done in ordinary functional program using function composition in a form of continuation-passing, destinations act as an optimization. This line of work was pioneered by~\cite{minamide_functional_1998}. While this only increases expressiveness in combination with the next point, the optimization is significant enough that destination-passing has been implemented in the Ocaml optimizer to support tail modulo constructor~\cite{bour_tmc_2021};
\item destinations are first-class values, which can be passed and stored like ordinary values. This is the innovation of~\cite{bagrel_destination-passing_2024} upon which we build. The consequence is that not only the order in which a structure is built is arbitrary, this order can be determined dynamically during the runtime of the program.
\end{itemize}

To support this programming style, we introduce \destcalculus{}. We intend \destcalculus{} to serve as a core calculus to reason about safe destinations. Indeed \destcalculus{} subsumes all the systems that we've discussed in this section: they can all be encoded in \destcalculus{} via simple macro expansion. As such we expect that potential extensions to these systems can be justified by giving their semantics as an expansion in \destcalculus{}.

Our contributions are as follows:
\begin{itemize}
\item \destcalculus{}, a linear and modal simply typed $\lambda$-calculus with destinations (\cref{sec:syntax-type-system,sec:ectxs-sem}). \destcalculus{} is expressive enough so that previous calculi for destinations can be encoded in \destcalculus{} (see~\cref{sec:related-work});
\item a demonstration that \destcalculus{} is more expressive than previous calculi with destinations (\cref{sec:scope-escape-dests,sec:bft}), namely that destination can be stored in structures with holes. We show how we can improve, in particular, on the breadth-first traversal example of~\cite{bagrel_destination-passing_2024};
\item an implementation strategy for \destcalculus{} which uses mutation without compromising the purity of \destcalculus{} (\cref{sec:implementation});
\item formally-verified proofs, with the Coq proof assistant, of the main safety lemmas (\cref{sec:formal-proof}).
\end{itemize}

\section{Working with destinations}\label{sec:working-with-dests}

Let's introduce \destcalculus, our simply typed $\lambda$-calculus with destination. The syntax is standard, except that we use linear logic's $[[T⨁U]]$ and $[[T⨂U]]$ for sums and products, since \destcalculus is linearly typed, even though it isn't a focus in this section.

\subsection{Building up a vocabulary}\label{ssec:build-up-vocab}

\activespaces

In its simplest form, destination passing, much like continuation passing, is using a location, received as an argument, to return a value. Instead of a function with signature $[[T ¹ν → U]]$, in \destcalculus{} you would have $[[T ¹ν → ⌊ U ⌋ ¹ν ¹ν → ①]]$, where $[[⌊ U ⌋ ¹ν]]$ is read “destination of type $[[U]]$”. For instance, here is a destination-passing version of the identity function:

\codehere{\newoperator
{\ottkw{dId}}{[[T ¹ν → ⌊ T ⌋ ¹ν ¹ν → ①]]}
{\ottkw{dId}~[[x]]~[[d]]}{[[d ˢ⨞ x]]}}

We think of a destination as a reference to an uninitialized memory location, and $[[d ˢ⨞ x]]$ (read “fill $[[d]]$ with $[[x]]$”) as writing $[[x]]$ to the memory location.

The form $[[d ˢ⨞ x]]$ is the simplest way to use a destination. But we don't have to fill a destination with a complete value in a single step. Destinations can be filled piecemeal.

\codehere{\newoperator
{\ottkw{fillWithInl}}{[[⌊ T ⨁ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν]]}
{\ottkw{fillWithInl}~[[d]]}{[[d ⨞ Inl]]}}

In this example, we're building a value of sum type $[[T ⨁ U]]$ by setting the outermost constructor to left variant $[[Inl]]$. We think of $[[d ⨞ Inl]]$ (read “fill $[[d]]$ with $[[Inl]]$”) as allocating memory to store a block of the form $[[Inl]]~\holesq$, write the address of that block to the location that $[[d]]$ points to, and return a destination pointing to the uninitialized argument of $[[Inl]]$.

Notice that we are constructing the structure from the outermost constructor inward: we've built a value of the form $[[Inl]]~\holesq$, but we have yet to describe what goes in the hole $\holesq$. We call such incomplete values \emph{hollow constructors}. This is opposite to how functional programming usually works, where values are built from the innermost constructors outward: first we make a value $[[v]]$ and only then can we use $[[Inl]]$ to make an $[[Inl v]]$. This will turn out to be a key ingredient in the expressiveness of destination passing.

Yet, everything we've shown so far could have been done with continuations. So it's worth asking: how are destination different from continuations? Part of the answer lies in our intention to represent destinations as pointers to uninitialized memory (see~\cref{sec:implementation}). But where destinations really differ from continuations is when one has several destinations at hand. Then they have to fill \emph{all} the destinations; whereas when one has multiple continuations, they can only return to one of them. Multiple destination arises when filling a destination of product type (tuple):

\codehere{\newoperator
{\ottkw{fillWithAPair}}{[[⌊ T ⨂ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν]]}
{\ottkw{fillWithAPair}~[[d]]}{[[d ⨞ (,)]]}}

To fill a destination for a pair (product type $[[T ⨂ U]]$), we must fill both the first field and the second field. In plain English, it sounds obvious, but the key remark is that $\ottkw{fillWithAPair}$ doesn't exist on continuations.

\paragraph{Structures with holes}
Let's now turn to how we can use the result made by filling destinations. Observe, as a preliminary remark, that while a destination is used to build a structure, the type of the structure being built might be different from the type of the destination. For instance, $\ottkw{fillWithInl}$ above, returns a destination $[[⌊ T ⌋ ¹ν]]$ while it is used to build a structure of type $[[T ⨁ U]]$. To represents this, \destcalculus{} uses a type $[[S ⧔ ⌊ T ⌋ ¹ν]]$ for a structure of type $[[S]]$ missing a value of type $[[T]]$ to be complete; we then say it has a \emph{hole} of type $[[T]]$. There can be several holes in $[[S]]$, resulting in several destinations on the right hand side: for example, $[[S ⧔ (⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν)]]$ carries a tuple of destinations.

The general form $[[S ⧔ T ]]$ is read “$[[S]]$ ampar $[[T]]$”. The name “ampar” stands for “asymmetric memory par”; we will explain why it is asymmetric in~\cref{ssec:ampar-motivation}. For now, it's sufficient to observe that $[[S ⧔ ⌊ T ⌋ ¹ν]]$ is akin to a “par” type $\ottstype{S \parr T^\perp}$ in linear logic; you can think of $[[S ⧔ ⌊ T ⌋ ¹ν]]$ as a (linear) function from $[[T]]$ to $[[S]]$. That structures with holes could be seen a linear functions was first observed in~\cite{minamide_functional_1998}, we elaborate on the value of having a par type rather than a function type in~\cref{sec:bft}. A similar connective is called $\ottstype{Incomplete}$ in~\cite{bagrel_destination-passing_2024}.

Destinations always exist within the context of a structure with holes: a destination is a pointer to a hole in the structure. Crucially, destinations are otherwise ordinary values. To access the destinations, \destcalculus{} provides a $\ottkw{map}$ construction, which lets us apply a function to the right-hand side of an ampar:

\sidebysidecodehere{t}{0.50}{
  \newoperator
  {\ottkw{fillWithInl'}}{[[S ⧔ ⌊ T ⨁ U ⌋ ¹ν ¹ν → S ⧔ ⌊ T ⌋ ¹ν]]}
  {\ottkw{fillWithInl'}~[[x]]}{[[x map d ⟼ d ⨞ Inl]]}
}{
  \newoperator
  {\ottkw{fillWithAPair'}}{[[S ⧔ ⌊ T ⨂ U ⌋ ¹ν ¹ν → S ⧔ (⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν)]]}
  {\ottkw{fillWithAPair'}~[[x]]}{[[x map d ⟼ d ⨞ (,)]]}
}

To tie this up, we need a way to introduce and to eliminate structures with holes. Structures with holes are introduced with $[[alloc]]$ which creates a value of type $[[T ⧔ ⌊ T ⌋ ¹ν]]$. $[[alloc]]$ is a bit like the identity function: it is a hole (of type $[[T]]$) that needs a value of type $[[T]]$ to be a complete value of type $[[T]]$. Structures with holes are eliminated with\footnote{As the name suggest, there is a more general elimination $\ottkw{from}_{\ottkw{\ltimes} }$. It will be discussed in~\cref{sec:syntax-type-system}.} $\ottkw{from}_{\ottkw{\ltimes} }' : [[S⧔① ¹ν → S]]$: if all the destinations have been consumed and only unit remains on the right side, then a structure with holes is really just a normal, complete structure.

Equipped with these, we can, for instance, derive traditional constructors from piecemeal filling. In fact, \destcalculus{} doesn't have primitive constructor forms, constructors in \destcalculus{} are syntactic sugar. We show here the definition of $[[Inl]]$ and $[[(,)]]$, but the other constructors are derived similarly.

\codehere{\newoperator
{[[Inl]]}{[[T ¹ν → T ⨁ U]]}
{[[ˢInl x]]}{[[from⧔' (alloc map d ⟼ d ⨞ Inl ˢ⨞ x)]]}}

\codehere{\newoperator
{[[(,)]]}{[[T ¹ν → U ¹ν → T ⨂ U]]}
{[[ˢ(x, y)]]}{[[from⧔' (alloc map d ⟼ (d ⨞ (,)) case ¹ν  (d1, d2) ⟼ d1 ˢ⨞ x; d2 ˢ⨞ y)]]}}


\paragraph{Memory safety and purity}
At this point, the reader may be forgiven for feeling distressed at all the talk of mutations and uninitialized memory. How is it consistent with our claim to be building a pure and memory-safe language? The answer is that it wouldn't be if we'd allow unrestricted use of destination. Instead \destcalculus{} uses a linear type system to ensure that:

\begin{itemize}
\item destination are written at least once, preventing examples like:

  \codehere{\newoperator
  {\ottkw{forget}}{[[T]]}
  {\ottkw{forget}}{[[from⧔' (alloc map d ⟼ ())]]}}

  where reading the result of $\ottkw{forget}$ would result in reading a hole that we never filled, in other words, reading uninitialized memory;
\item destination are written at most once, preventing examples like:

  \codehere{\newoperator
  {\ottkw{ambiguous1}}{[[Bool]]}
  {\ottkw{ambiguous1}}{[[from⧔' (alloc map d ⟼ d ˢ⨞ true; d ˢ⨞ false)]]}
  \newoperator
  {\ottkw{ambiguous2}}{[[Bool]]}
  {\ottkw{ambiguous2}}{[[from⧔' (alloc map d ⟼ let x ≔ (d ˢ⨞ false) in d ˢ⨞ true; x)]]}}

  where $\ottkw{ambiguous1}$ returns $[[false]]$ and $\ottkw{ambiguous2}$ returns $[[true]]$ due to evaluation order, even though let-expansion should be valid in a pure language.
\end{itemize}

\subsection{Functional queues, with destinations}\label{ssec:efficient-queue}

Now that we have an intuition of how destinations work, let's see how they can be used to build usual data structures. For that section, we suppose that \destcalculus{} is equipped with equirecursive types and a fixed-point operator, that isn't part of our formally proven fragment.

\paragraph{Linked lists}

We define lists as the fixpoint of the functor $[[X]] \mapsto [[① ⨁ (T ⨂ X)]]$. For convenience, we also define synthetic filling operators $\triangleleft\ottsctor{[]}$ and $\triangleleft\ottsctor{(::)}$:

\sidebysidecodehere{b}{0.50}{
\newtype[~\mathttbf{rec}]{[[List T]]}{[[① ⨁ (T ⨂ (List T))]]}
\newoperator
  {\triangleleft\ottsctor{[]}}{[[⌊ List T ⌋ ¹ν ¹ν → ①]]}
  {[[d ⨞ [] ]]}{[[d ⨞Inl ⨞()]]}
}{
\newoperator
  {\triangleleft\ottsctor{(::)}}{[[⌊ List T ⌋ ¹ν ¹ν → ⌊ T⌋ ¹ν ⨂ ⌊ List T ⌋ ¹ν]]}
  {[[d ⨞ (::)]]}{[[d ⨞Inr ⨞(,)]]}
}

Just like we did in~\cref{ssec:build-up-vocab} we can recover traditional constructors from filling operators:

\codehere{\newoperator
{\ottsctor{(::)}}{[[T ⨂ (List T) ¹ν → List T]]}
{\ottsctor{(::)}~[[x]]~[[xs]]}{[[from⧔' (alloc map d ⟼ (d ⨞ (::)) case ¹ν (dx, dxs) ⟼ dx ˢ⨞ x ; dxs ˢ⨞ xs)]]}}

\paragraph{Difference lists}
\newcommand{\lstcat}{\mathop{+\!+}}
Just like in any language, iterated concatenation of lists
$(([[xs1]] \lstcat [[xs2]])\lstcat \ldots)\lstcat [[xs]]_n$
is quadratic in \destcalculus{}. The usual solution to this is difference lists. The name difference lists covers many related implementation, but in pure functional languages, a difference list is usually represented as a function\improvement{I'm drastically cutting on the tutorial, but this needs a citation}. A singleton difference list is $\lambda [[ys]]. [[x]]\ottsctor{::}[[ys]]$, and concatenation of difference lists is function composition. Difference lists are turned into a list by applying it to the empty list. The consequence is that no matter how many composition we have, each cons cell $\ottsctor{::}$ will be allocated a single time, making the iterated concatenation linear indeed.

However, each concatenation allocates a closure. If we're building a difference list from singletons and composition, there's roughly one composition per $\ottsctor{::}$, so iterated composition effectively performs two traversals of the list. We can do better!

In \destcalculus{} we can represent a difference list as a list with a hole. A singleton difference list is $[[x]] \ottsctor{::} \holesq$, concatenation is filling the hole with another difference list. The details are on the left of~\cref{fig:impl-dlist-queue}. This encoding makes no superfluous traversal; in fact, concatenation is an $O(1)$ in-place update.

\sidebysidecodefig{\caption{Difference list and queue implementation in equirecursive \destcalculus{}}\label{fig:impl-dlist-queue}}{t}{0.51}{
\newtype{[[DList T]]}{[[(List T) ⧔ ⌊ List T ⌋ ¹ν]]}
\newoperatorb
  {\ottkw{append}}{[[DList T ¹ν → T ¹ν → DList T]]}
  {[[ys append y]]}{\!\!\!\begin{array}[t]{l}[[
ys map dys ⟼ (dys ⨞ (::)) case ¹ν⮒
‥‥(dy, dys') ⟼ dy ˢ⨞ y ; dys'
]]\end{array}}
\newoperator
  {\ottkw{concat}}{[[DList T ¹ν → DList T ¹ν → DList T]]}
  {[[ys concat ys']]}{[[ys map d ⟼ d ⨞· ys']]}
\newoperator
  {\ottkw{to}_{\ottkw{List} }}{[[DList T ¹ν → List T]]}
  {[[toList ys]]}{[[from⧔' (ys map d ⟼ d ⨞ [])]]}
}{
\newtype{[[Queue T]]}{[[(List T) ⨂ (DList T)]]}
\newoperator
  {\ottkw{singleton}}{[[T ¹ν → Queue T]]}
  {[[singleton x]]}{[[ˢ(ˢInr ˢ(x, Inl ()), alloc)]]}
\newoperatorb
  {\ottkw{enqueue}}{[[Queue T ¹ν → T ¹ν → Queue T]]}
  {[[q enqueue y]]}{[[q case ¹ν (xs, ys) ⟼ ˢ(xs, ys append y)]]}
\newoperatorb
  {\ottkw{dequeue}}{[[Queue T ¹ν → ① ⨁ (T ⨂ (Queue T))]]}
  {[[dequeue q]]}{\!\!\!\begin{array}[t]{l}[[
q case ¹ν {⮒
‥‥ˢ(ˢInr ˢ(x, xs), ys) ⟼ ˢInr ˢ(x, ˢ(xs, ys)),⮒
‥‥ˢ(Inl (), ys) ⟼ (toList ys) case ¹ν {⮒
‥‥‥‥Inl () ⟼ Inl (),⮒
‥‥‥‥ˢInr ˢ(x, xs) ⟼ ˢInr ˢ(x, ˢ(xs, alloc))⮒
‥‥}⮒
}]]\end{array}}
}

\paragraph{Efficient queue using previously defined structures}
A simple way to implement a queue in a purely functional language is as a pair of lists $[[ˢ(front, back)]]$\improvement{cite Okasaki's efficient queues and compare}. Elements are popped from $[[front]]$ and are enqueued in $[[back]]$. When we need to pop an element and $[[front]]$ is empty, then we set the queue to $[[ˢ(reverse back, ˢ[])]]$, and pop from the new front.

For such a simple implementation, this is surprisingly efficient: the cost of the reverse operation is $O(1)$ amortized. Except that the cost is only amortized if the queue is used linearly. And even then, there's still one superfluous traversal of the $[[back]]$ list, compared to an imperative implementation.

But, taking a step back, this $[[back]]$ list which has to be reversed before it is accessed is really merely a representation of lists that can be extended from the back. And we already know an efficient implementation of lists that can be extended from the back but only accessed linearly: difference lists.

So we can give an improved version of the simple functional queue using destination. The implementation is on the right-hand side of~\cref{fig:impl-dlist-queue}\improvement{the implementation could be improved a little by using the cons and nil synthetic constructors}. Note that contrary to an imperative programming language, we can't implement the queue as a single difference list: our type system prevents us from reading the front elements of difference lists. Just like for the simple functional queue, we need a pair of a list that we can read from, and one that we can extend. Nevertheless this implementation of queues is both pure, as guaranteed by the \destcalculus{} type system, and nearly as efficient as what an imperative programming language would afford.

\section{Scope escape of destinations}\label{sec:scope-escape-dests}

Everything described in~\cref{sec:working-with-dests} is in fact already possible in DPS for Haskell as presented in~\cite{bagrel_destination-passing_2024}. However, in the aforementioned paper, destinations cannot be stored in destination-based data structures. This restriction is a rather blunt approach to prevent scope escape of destinations that itself is a great threat to memory safety of the system. Let's see why.

Initially, we made one core assumption about destination-passing style safety: once a destination has been linearly used, the associated hole has been written to. However, if destinations $\ottstype{\lfloor[[T]]\rfloor}$ can have arbitrary inner type $[[T]]$, they can be used to store a destination itself when $[[T]] = \ottstype{\lfloor[[T']]\rfloor}$!

The value fed in a destination is linearly used, which means it cannot be both stored away to be used later, and used in the current context (as that would result in two uses). But that also means that the destination $[[d]] \pmb{:} \ottstype{\lfloor[[T']]\rfloor}$ is linearly used too when it is fed to $[[dd]] \pmb{:} \ottstype{\lfloor\lfloor[[T']]\rfloor\rfloor}$ in $[[dd ˢ⨞ d]]$.

As a result, there are in fact two ways to use a destination linearly: fill it now with a value, or store it away and fill it later. The latter is a much weaker form of linear use, as it doesn't guarantee that the hole associated to the destination has been written to \emph{now}, only that it will be written to later. So our initial assumption above doesn't hold in general case.

The issue is particularly visible when trying to give semantics to the $\ottkw{alloc'}$ operator:\info{the mode on the right-most arrow should be $[[¹∞]]$, but we mask it for now}

\codehere{\newoperator
  {\ottkw{alloc'}}{[[(⌊ T ⌋ ¹ν ¹ν → ①) ¹ν → T]]}
  {[[alloc' f]]}{[[from⧔' (alloc map d ⟼ f d)]]}
}

With linear store semantics, this is how $\ottkw{alloc'}$ would behave:

\codehere{[[Z | alloc' (ˢλ d ¹ν ⟼ t) ⟶ Z ⨆ { h ≔ ⬜ } | (t[d ≔ -h] ; deref -h) ]]}

It works as expected when the function supplied to $\ottkw{alloc'}$ will indeed use the destination to store a value:

\codehere{\!\!\!\begin{array}{crcl}
       & $\{ \}$ &|&\!\! [[alloc' (ˢλ d ¹ν ⟼ d ⨞ Inl ⨞ () ) ]] \\
[[⟶]] & [[ { h ≔ ⬜ } ]] &|&\!\! [[-h ⨞ Inl ⨞ () ; deref -h ]] \\
[[⟶]] & [[ { h ≔ Inl () } ]] &|&\!\! [[deref -h ]] \\
[[⟶]] & $\{ \}$ &|&\!\! [[Inl () ]]
\end{array}}

However this falls short when calls to $\ottkw{alloc'}$ are nested in the following way (where $[[dd]]$ \pmb{:} $\ottstype{\lfloor\lfloor[[①]]\rfloor\rfloor}$ and $[[d]]$ \pmb{:} $\ottstype{\lfloor[[①]]\rfloor}$):

\codehere{\!\!\!\begin{array}{crcl}
       & $\{ \}$ &|&\!\! [[alloc' (ˢλ dd ¹ν ⟼ alloc' (ˢλ d ¹ν ⟼ dd ˢ⨞ d)) ]] \\
[[⟶]] & [[ { hd ≔ ⬜ } ]] &|&\!\! [[alloc' (ˢλ d ¹ν ⟼ -hd ˢ⨞ d) ; deref -hd]] \\
[[⟶]] & [[ { hd ≔ ⬜ , h ≔ ⬜ } ]] &|&\!\! [[ -hd ˢ⨞ -h ; deref -h ; deref -hd]] \\
[[⟶]] & [[ { hd ≔ -h , ul< h ≔ ⬜ > } ]] &|&\!\! [[ ul< deref -h > ; deref -hd]]
\end{array}}

The original term $[[alloc' (ˢλ dd ¹ν ⟼ alloc' (ˢλ d ¹ν ⟼ dd ˢ⨞ d)) ]]$ is well typed, as the inner call to $\ottkw{alloc'}$ returns a value of type $[[①]]$ (as $[[d]]$ is of type $\ottstype{\lfloor[[①]]\rfloor}$) and uses $[[d]]$ linearly. However, the variable $[[d]]$ that stands for destination $[[-h]]$ isn't filled with a value but instead escapes its scope by being fed to a destination of destination $[[dd]]$ coming from the outer scope (this still counts as a linear usage). Hence the associated hole $[[h]]$ doesn't receive a value and the reduction get stuck when trying to dereference $[[-h]]$.

\improvement{I remember that we did wonder about that at some point, so it's worth preempting the question. Though I'd rather we found a slightly different example that doesn't exploit the return type of fill than a paragraph recounting our life story.

Thomas: the second example is too long/complex to show the whole semantics, so I kept the first one in a detailed fashion, and just mentionned the second one.
}
One could argue that the issue comes from the primitive $\blacktriangleleft$ returning a value of type $[[①]]$ instead of a dedicated effect type. However, the same issue arise in following program, which is well-typed even when $\blacktriangleleft$ and the function accepted by $\ottkw{alloc'}$ have an arbitrary return type $[[E]]$, so we decided not to introduce yet another type in the system:

\codehere{[[
alloc' (ˢλ dd ¹ν ⟼ (dd ⨞ (,)) case ¹ν (dd1, d2) ⟼⮒
‥‥‥‥‥‥‥‥‥‥༼(alloc' (ˢλ d ¹ν ⟼ dd1 ˢ⨞ d)) case ¹ν { true ⟼ d2 ˢ⨞ true , false ⟼ d2 ˢ⨞ false }༽)
]]}

\noindent where {$
[[dd]] \pmb{:} \ottstype{\lfloor \lfloor[[Bool]]\rfloor \otimes [[Bool]] \rfloor},~
[[dd1]] \pmb{:} \ottstype{\lfloor \lfloor[[Bool]]\rfloor \rfloor},~
[[d2]] \pmb{:} \ottstype{\lfloor [[Bool]] \rfloor},~
[[d]] \pmb{:} \ottstype{\lfloor [[Bool]] \rfloor}
$}.

% We initially thought that the issue of scope escape using destinations of destinations was just a result of a flawed type design in our initial prototype, and that it could be fixed easily. It seems, in fact, to be an irrefutable proof that the assumption "consuming a destination means that the equivalent hole has been written to" is not true in all generality. Thus, we have to add stronger restrictions to our system to ensure that this assumption holds in the desired context. For that, we introduce age-control of bindings in~\cref{ssec:age-control}.

In the next section, we motivate why being able to store destinations in destinations of destinations is a desirable property of our system that we don't want to give up. In~\cref{sec:syntax-type-system}, we present a finer type system that prevents scope escape while still allowing to store destinations in destination-based data structures.

\section{Breadth-first tree traversal}\label{sec:bft}

The core example that showcases the power of destination-passing style programming with first-class destination --- that we borrow from \cite{bagrel_destination-passing_2024} --- is breadth-first tree traversal:

\begin{quote}
Given a tree, create a new one of the same shape, but with the values at the nodes replaced by the numbers $1\ldots|T|$ in breadth-first order.
\end{quote}

Indeed, breadth-first traversal implies that the order in which the structure must be populated (left-to-right, top-to-bottom) is not the same as the structural order of a functional binary tree, that is, building the leaves first and going up to the root.

In the aforementioned paper, the author presents a breadth-first traversal implementation that relies on first-class destinations so as to build the final tree in a single pass over the input tree. Their implementation, exactly like ours, uses a queue to store pairs of an input subtree and a destination to the corresponding output subtree. This queue is what materialize the breadth-first processing order: the leading pair $(\textit{input subtree}, \textit{dest to output subtree})$ of the queue is processed, and pairs of the same shape for children nodes are appended at the end of the queue.

\improvement{Rework the next couple of paragraph to flow a little bit better.}
However, as evoked earlier, the API presented in~\cite{bagrel_destination-passing_2024} is not able to store linear data, and in particular destinations, in destination-based data structures. So they cannot use the efficient, destination-based queue implementation from~\cref{ssec:efficient-queue} to power up the breadth-first tree traversal implementation\footnote{This efficient queue implementation can be, and is in fact, implemented in~\cite{bagrel_destination-passing_2024}: see \url{archive.softwareheritage.org/swh:1:cnt:29e9d1fd48d94fa8503023bee0d607d281f512f8}. But it cannot store linear data}. With \destcalculus{}, this is now possible. In fact, our system is self-contained, in the sense that every possible structure can be built using destination-based primitives (and regular data constructors can be retrieved from destination-based primitives, as detailed in~\cref{fig:sterm}).

\cref{fig:impl-bfs} presents the \destcalculus{} implementation of the breadth-first tree traversal. We assume that we have a binary tree type alias $[[Tree T]]$ and natural number type alias $[[Nat]]$ encoded using standard sum and product types. $[[Tree T]]$ is equipped with operators $\triangleleft\ottsctor{Nil}$ and $\triangleleft\ottsctor{Node}$, that are implemented in terms of our core destination-filling primitives.

% \sidebysidecodefig{\caption{Boilerplate for breadth-first tree traversal}\label{fig:impl-boilerplate-bfs}}{t}{0.7}{
% \newtype{[[Tree T]]}{[[① ⨁ (T ⨂ ((Tree T) ⨂ (Tree T)))]]}
% \newoperator
%   {\triangleleft\ottsctor{Nil}}{[[⌊ Tree T ⌋ n ¹ν → ①]]}
%   {[[d ⨞ Nil]]}{[[d ⨞Inl ⨞()]]}
% \newoperator
%   {\triangleleft\ottsctor{Node}}{[[⌊ Tree T ⌋ n ¹ν → ⌊T⌋ n ⨂ (⌊ Tree T⌋ n ⨂ ⌊ Tree T⌋ n)]]}
%   {[[d ⨞ Node]]}{[[(d ⨞Inr ⨞(,)) case ¹ν (dv, dtlr) ⟼ ˢ(dv, dtlr ⨞(,))]]}
% }{
% \newtype[~\mathttbf{rec}]{[[Nat]]}{[[① ⨁ Nat]]}
% \newoperator
%   {\ottkw{zero}}{[[Nat]]}
%   {[[zero]]}{[[Inl ()]]}
% \newoperator
%   {\ottkw{succ}}{[[Nat ¹ν → Nat]]}
%   {[[succ x]]}{[[ˢInr x]]}
% }

\begin{codefig}{\caption{Breadth-first tree traversal in destination-passing style}\label{fig:impl-bfs}}
\newoperator[~\mathttbf{rec}]
{\ottkw{go}}{[[(S ¹∞ → T1 ¹ν → (! ¹∞ S) ⨂ T2) ω∞→ S ¹∞  → Queue (Tree T1 ⨂ ⌊ Tree T2 ⌋ ¹ν) ¹ν → (! ¹∞ S)]]}
{[[go f st q]]}{\!\!\!\begin{array}[t]{l}[[
(dequeue q) case ¹ν {⮒
‥‥Inl () ⟼ ˢᴇ ¹∞ st,⮒
‥‥ˢInr ˢ(ˢ(tree, dtree), q') ⟼ tree case ¹ν {⮒
‥‥‥‥Inl () ⟼ dtree ⨞ Nil ; go f st q',⮒
‥‥‥‥ˢInr ˢ(x, ˢ(tl, tr)) ⟼ ༼(dtree ⨞ Node) case ¹ν⮒
‥‥‥‥‥‥ˢ(dy, ˢ(dtl, dtr)) ⟼ ༼(༼f st༽ x) case ¹ν⮒
‥‥‥‥‥‥‥‥ˢ(ˢᴇ ¹∞ st', y) ⟼⮒
‥‥‥‥‥‥‥‥‥‥dy ˢ⨞ y ;⮒
‥‥‥‥‥‥‥‥‥‥go f st' (q' enqueue ˢ(tl, dtl) enqueue ˢ(tr, dtr))༽༽⮒
‥‥}⮒
}
]]\end{array}}
\newoperator
{\ottkw{mapAccumBFS}}{[[(S ¹∞ → T1 ¹ν → (! ¹∞ S) ⨂ T2) ω∞→ S ¹∞ → Tree T1 ¹∞ → Tree T2 ⨂ (! ¹∞ S)]]}
{[[mapAccumBFS f st tree]]}{\!\!\!\begin{array}[t]{l}[[
from⧔ (alloc map dtree ⟼ go f st (singleton ˢ(tree, dtree)))
]]\end{array}}
\newoperator
{\ottkw{relabelDPS}}{[[Tree ① ¹∞ → (Tree Nat) ⨂ (! ¹∞ (! ων Nat))]]}
{[[relabelDPS tree]]}{\!\!\!\begin{array}[t]{l}[[
mapAccumBFS⮒
‥‥(ˢλ ex ¹∞ ⟼ ˢλ un ¹ν ⟼ un ; ༼ex case ¹∞⮒
‥‥‥‥ᴇ ων st ⟼ ˢ(ˢᴇ ¹∞ (ˢᴇ ων (succ st)), st)༽)⮒
‥‥(ˢᴇ ων (succ zero))⮒
‥‥tree
]]\end{array}}
\end{codefig}

The stateful transformer $[[f]]$ that is applied to each input node has type $[[S ¹∞ → T1 ¹ν → (! ¹∞ S) ⨂ T2]]$. It takes the current state and node value and returns the next state and value for output node. The state has to be wrapped in an exponential $\ottstype{!_{[[¹∞]]}}$ in the return type to witness that it cannot capture destinations. That way, the state can be extracted using $\ottkw{from}_{\ottkw{\ltimes} }$ at the end of the processing.
% We could imagine a more general version of the traversal, having no constraint on the state type, but necessitating a finalization function $[[S ¹ν → ! ¹∞ S']]$ so that the final state can be returned.

The $\ottkw{go}$ function is in charge of consuming the queue containing the pairs of input subtrees and destinations to the corresponding output subtrees. It dequeues the first pair, and processes it. If the input subtree is $\ottsctor{Nil}$, it fills $\ottsctor{Nil}$ into the destination for the output tree and continues the processing of next elements with unchanged state. If the input subtree is a node, it writes a hollow $\ottsctor{Node}$ constructor to the hole pointed to by the destination $[[dtree]]$, processes the value $[[x]]$ of the input node with the stateful transformer $[[f]]$, and continues the processing of the updated queue where children subtrees and their associated destinations have been enqueued.

$\ottkw{mapAccumBFS}$ spawns the initial memory slot for the output tree, and prepares the initial queue containing a single pair, made of the whole input tree and a destination to the aforementioned memory slot.

$\ottkw{relabelDPS}$ is a special case of $\ottkw{mapAccumBFS}$ that takes the skeleton of a tree (where node values are all unit) and returns a tree of integers, with the same skeleton, but with node values replaced by naturals $1\ldots|T|$ in breadth-first order. The higher-order function passed to $\ottkw{mapAccumBFS}$ is quite verbose: it must consume the value of the input node (unit) using $\fatsemi$, then extract the state (representing the next natural number to attribute to a node) from its exponential wrapper, and finally return a pair, whose left side is the incremented natural wrapped back into its two exponential layers (new label for next node), and whose right side is the original natural acting as a label for the current node The extra exponential $\ottstype{!}_{[[ων]]}$ around $[[Nat]]$ let us use the natural number twice.

You might wonder what all the fuchsia subscripts $[[¹∞]], [[ων]]\ldots$ mean. It's now time to cover the type and mode system of \destcalculus{}.

\section{Language syntax and type system}\label{sec:syntax-type-system}

\newcommand{\grammsep}{\hspace*{2ex}|\hspace*{2ex}}
\newcommand{\grammdef}{:\hspace*{-0.3ex}:\hspace*{-0.3ex}=}

\begin{codefig}{\caption{Grammar of \destcalculus{}}\label{fig:grammar}}{\setlength{\arraycolsep}{1ex}
\!\!\!\begin{array}{rrl}
[[t]], [[u]] &::=& [[v]] \grammsep [[x]] \grammsep [[t' t]] \grammsep [[t ; t']] \\
             &|\,& [[ t case m { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[t case m ( x1 , x2 ) ⟼ u]] \grammsep [[t case m ᴇ n x ⟼ u]] \\
             &|\,& [[t map x ⟼ t']] \grammsep [[ to⧔ t ]] \grammsep [[ from⧔ t ]] \\
             &|\,& [[ t ⨞ () ]] \grammsep [[ t ⨞ Inl ]] \grammsep [[t ⨞ Inr]] \grammsep [[t ⨞ (,)]] \grammsep [[t ⨞ ᴇ m]] \grammsep [[t ⨞ ( λ x m ⟼ u )]] \grammsep [[t ⨞· t']] \\
&&\\
       [[v]] &::=& [[+ h]] \hspace*{\widthof{$[[H ⟨ v2 ❟ v1 ⟩]]$}-\widthof{$[[+ h]]$}}\quad\quad\textit{(hole)} \\
             &|\,& [[- h]] \hspace*{\widthof{$[[H ⟨ v2 ❟ v1 ⟩]]$}-\widthof{$[[- h]]$}}\quad\quad\textit{(destination)} \\
             &|\,& [[H ⟨ v2 ❟ v1 ⟩]] \quad\quad\textit{(ampar value form)} \\
             &|\,& [[()]] \grammsep [[ᵛλ x m ⟼ u]] \grammsep [[Inl v]] \grammsep [[Inr v]] \grammsep [[ᴇ m v]] \grammsep [[( v1 , v2 )]] \\
&&\\
[[T]], [[U]], [[S]] &::=& [[⌊ T ⌋ m]] \quad\quad\textit{(destination)} \\
                    &|\,& [[U ⧔ T]] \hspace*{\widthof{$[[⌊ T ⌋ m]]$}-\widthof{$[[U ⧔ T]]$}}\quad\quad\textit{(ampar)} \\
                    &|\,& [[①]] \grammsep [[T1 ⨁ T2]] \grammsep [[T1 ⨂ T2]] \grammsep [[! m T]] \grammsep [[T m → U]] \\
&&\\
        [[m]], [[n]] &::=& [[p a]] \hspace*{\widthof{$[[⌊ T ⌋ m]]$}-\widthof{$[[p a]]$}}\quad\quad\textit{(pair of multiplicity and age)} \\
               [[p]] &::=& [[¹]] \grammsep [[ω]] \\
               [[a]] &::=& [[ν]] \grammsep [[↑]] \grammsep [[∞]] \\
&&\\
[[O]], [[G]], [[P]], [[D]] &::=& [[{ }]] \grammsep [[{ x : m T }]] \grammsep [[{ + h : T n }]] \grammsep [[{ - h : m ⌊ T ⌋ n }]] \\
                           &|\,& [[O1 , O2]] \grammsep [[O1 + O2]] \grammsep [[m · O]] \grammsep [[-⁻¹ D]]
\end{array}
}\end{codefig}

\sidebysidecodefig{\caption{Syntactic sugar forms for terms}\label{fig:sterm}}{t}{0.425}{
[[alloc]] \btriangleq \!\!\!\begin{array}[t]{l}[[
{ 1 } ⟨ +1 ❟ -1 ⟩
]]\end{array}\\[\interdefskip]
[[t ˢ⨞ t']] \btriangleq \!\!\!\begin{array}[t]{l}[[
t ⨞· (to⧔ t')
]]\end{array}\\[\interdefskip]
[[ˢInl t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc map d ⟼⮒
‥‥d ⨞ Inl ˢ⨞ t⮒
)
]]\end{array}\\[\interdefskip]
[[ˢInr t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc map d ⟼⮒
‥‥d ⨞ Inr ˢ⨞ t⮒
)
]]\end{array}\\[\interdefskip]
[[ˢᴇ m t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc map d ⟼⮒
‥‥d ⨞ ᴇ m ˢ⨞ t⮒
)
]]\end{array}
}{
[[from⧔' t]] \btriangleq \\
\myspace{1}\!\!\!\begin{array}[t]{l}[[
(from⧔ (t map un ⟼ un ; ᴇ ¹∞ () )) case ¹ν⮒
‥‥( st , ex ) ⟼ ༼ex case ¹ν⮒
‥‥‥‥ᴇ ¹∞ un ⟼ un ; st༽
]]\end{array}\\[\interdefskip]
[[ˢλ x m ⟼ u]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc map d ⟼⮒
‥‥d ⨞ ( λ x m ⟼ u )⮒
)
]]\end{array}\\[\interdefskip]
[[ˢ( t1 , t2 )]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (alloc map d ⟼⮒
‥‥(d ⨞ (,)) case ¹ν⮒
‥‥‥‥( d1 , d2 ) ⟼ d1 ˢ⨞ t1 ; d2 ˢ⨞ t2⮒
)
]]\end{array}
}

\destcalculus{} is based on simply typed lambda calculus, with a first-order type system, featuring modal function types and modal boxing, in addition to unit ($[[①]]$), product ($\ottstype{\otimes}$) and sum ($\ottstype{\oplus}$) types. It is also equipped with the destination type $[[⌊ T ⌋ m]]$ and ampar type $[[S ⧔ T]]$ that have been previewed in~\cref{sec:working-with-dests} to represent DPS structure building. The core grammar of the language is presented in~\cref{fig:grammar}. We also provide commonly used syntactic sugar forms for terms in~\cref{fig:sterm}.

Modes in \destcalculus{} have two axes --- multiplicity (i.e. linear/non-linear), and age control --- and they take place on variable bindings in typing contexts $[[O]]$, and on function arrows, but are not part of the type itself.

We omit the mode annotation $[[m]]$ on function arrows and destinations when the mode in question is the multiplicative neutral element $[[¹ν]]$ of the mode semiring (in particular, a function arrow without annotation is linear by default). A function arrow with multiplicity $[[¹]]$ is equivalent to the linear arrow $\ottstype{\multimap}$ from~\cite{girard_linear_1995}.

Let's now introduce the age mode axis, which is a novel feature of this calculus.

\subsection{Age-control for bindings to prevent scope escape of destinations}\label{ssec:age-control}

The solution we chose to alleviate scope escape of destinations (detailed in~\cref{sec:scope-escape-dests}) is to track the age of destinations (as De-Brujin-like scope indices), and to set age-control restriction on the typing rule of destination-filling primitives.

Age is represented by a commutative semiring, where $[[ν]]$ indicates that a destination originates from the current scope, and $[[↑]]$ indicates that it originates from the scope just before. We also extend ages to variables (a variable of age $[[a]]$ stands for a value of age $[[a]]$). Finally, age $[[∞]]$ is introduced for variables standing in place of a non-age-controlled value. In particular, destinations can never have age $[[∞]]$; a main role of age $[[∞]]$ is thus to act as a proof that no destination can be part of the value.

Semiring addition $\ottsmode{+}$ is used to find the age of a variable or destination that is used in two subterms of a program. Semiring multiplication $[[·]]$ corresponds to age composition, and is in fact an integer sum on scope indices.
$[[∞]]$ is absorbing for both addition and multiplication.

Tables for the operations $\ottsmode{+}$ and $\ottsmode{[[·]]}$ are presented in~\cref{fig:mul-age-tables}.

\begin{figure}[t]
\begin{tabular}{|c|c|c|}\hline
$\ottsmode{+}$ & $[[↑]]^{n}$               & $[[∞]]$ \\\hline
$[[↑]]^{m}$    & $\text{if }n = m\text{ then }[[↑]]^{n}\text{ else }[[∞]]$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$                   & $[[∞]]$ \\\hline
\end{tabular}
\hfill
\begin{tabular}{|c|c|c|}\hline
$[[·]]$        & $[[↑]]^{n}$               & $[[∞]]$ \\\hline
$[[↑]]^{m}$    & $[[↑]]^{n+m}$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$                   & $[[∞]]$ \\\hline
\end{tabular}
\hfill
\vrule width 0.5pt % Vertical rule of 1pt width
\hfill
\begin{tabular}{|c|c|c|}\hline
$\ottsmode{+}$ & $[[¹]]$ & $[[ω]]$ \\\hline
$[[¹]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
$[[ω]]$        & $[[ω]]$ & $[[ω]]$ \\\hline
\end{tabular}
\hfill
\begin{tabular}{|c|c|c|}\hline
$[[·]]$        & $[[¹]]$ & $[[ω]]$ \\\hline
$[[¹]]$        & $[[¹]]$ & $[[∞]]$ \\\hline
$[[ω]]$        & $[[∞]]$ & $[[∞]]$ \\\hline
\end{tabular}

\bigskip

We pose $[[↑]]^{0} = [[ν]]$ and $[[↑]]^{n} = [[↑]][[·]][[↑]]^{n-1}$

\caption{Operation tables for age and multiplicity semirings}\label{fig:mul-age-tables}
\end{figure}

Age commutative semiring is then combined with the multiplicity commutative semiring from~\cite{bernardy_linear_2018} to form a canonical product commutative semiring that forms the mode of each typing context binding in our final type system.

\subsection{Design motivation behind the ampar and destination types}\label{ssec:ampar-motivation}

Minamide's work\cite{minamide_functional_1998} is the earliest record we could find of a functional calculus integrating the idea of incomplete data structures (structures with holes) that exist as first class values and can be interacted with by the user.

In that paper, a structure with a hole is named \emph{hole abstraction}. In the body of a hole abstraction, the bound \emph{hole variable} should be used linearly (exactly once), and must only be used as a parameter of a data constructor (it cannot be pattern-matched on). A hole abstraction of type $\ottstype{([[T]], [[S]]) hfun}$ is thus a weak form of linear lambda abstraction $\ottstype{[[T]] \multimap [[S]]}$, which just moves a piece of data into a bigger data structure.

% In fact, the type of hole abstraction $\ottstype{([[T1]], [[T2]]) hfun}$ in Minamine's work shares a lot of similarity with the separating implication or \emph{magic wand} $\ottstype{[[T1]] \sepimp [[T2]]}$ from separation logic: given a piece of memory matching description $[[T1]]$, we obtain a (complete) piece of memory matching description $[[T2]]$.

Now, in classical linear logic, we know we can transform linear implication $\ottstype{[[T]] \multimap [[S]]}$ into $\ottstype{[[S]]~\parr~[[T]]^{\perp}}$. Doing so for the type $\ottstype{([[T]], [[S]]) hfun}$ gives $\ottstype{[[S]] ~\widehat{\parr}~ \lfloor[[T]]\rfloor}$, where $\ottstype{\lfloor\smallbullet\rfloor}$ is memory negation, and $\ottstype{\widehat{\parr}}$ is a memory \emph{par} (it allows less interaction than the CLL \emph{par}, because $\ottstype{hfun}$ is weaker than $\ottstype{\multimap}$).

Transforming the hole abstraction from its original implication form to a \emph{par} form let us consider the \emph{destination} type $\ottstype{\lfloor[[T]]\rfloor}$ as a first class component of our calculus. We also get to see the hole abstraction aka. memory par as a pair-like structure, where the two sides might be coupled together in a way that prevent using both of them simultaneously.

\paragraph{From memory par $\ottstype{\widehat{\parr}}$ to ampar $\ottstype{\ltimes}$}

\TODO{Should I mention that dests is a non-involutive negation?}

In CLL, thanks to the cut rule, any of the sides $[[S]]$ or $[[T]]$ of a par $\ottstype{[[S]]~\parr~[[T]]}$ can be eliminated, by interaction with the opposite type $\ottstype{\smallbullet^{\perp}}$, to free up the other side. But in \destcalculus{}, we have two types of interaction to consider: interaction between $[[T]]$ and $\ottstype{\lfloor[[T]]\rfloor}$, and interaction between $[[T]]$ and $[[T]]\ottstype{\to\smallbullet}$. The structure that may contain holes, $[[S]]$, can safely interact with $\ottstype{\lfloor[[S]]\rfloor}$ (merge it into a bigger structure with holes), but not with $[[T]]\ottstype{\to\smallbullet}$, as it would let the user read an incomplete structure! On the other hand, a complete value of type $[[T]] = \ottstype{(\ldots\lfloor[[T']]\rfloor\ldots)}$ containing destinations (but no holes) can safely interact with a function $[[T ¹ν → ①]]$ (in particular, the function can pattern-match on the value of type $[[T]]$ to access the destinations), but it is not always safe to fill it into a $\ottstype{\lfloor[[T]]\rfloor}$ as that might allow scope escape of destination $\ottstype{\lfloor[[T']]\rfloor}$ as we've just seen in~\cref{sec:scope-escape-dests}.

To recover sensible rules for the connective, we decided to make it asymmetric, hence ampar ($[[S ⧔ T]]$) for \emph{asymmetrical memory par}:
\begin{itemize}
\item the left side $[[S]]$ can contain holes, and can be only be eliminated by interaction with $\ottstype{\lfloor[[S]]\rfloor}$ using \textopname{fillComp} ($\triangleleft\mybullet$) to free up the right side $[[T]]$;
\item the right side $[[T]]$ cannot contain holes (it might contain destinations), and can be eliminated by interaction with $[[T ¹ν → ①]]$ to free up the left side $[[S]]$. At term level, this is done using $\ottkw{from}_{\ottkw{\ltimes}}'$ and $\ottkw{map}$.
\end{itemize}

\subsection{Typing of terms and values}\label{ssec:ty-term-val}

The typing rules for \destcalculus{} are highly inspired from\cite{bernardy_modality_2020} and Linear Haskell~\cite{bernardy_linear_2018}, and are detailed in~\cref{fig:ty-term-sterm}. In particular, we use the same additive/multiplicative approach on contexts for linearity and age enforcement

\begin{ottfig}{\caption{Typing rules for terms and syntactic sugar}\label{fig:ty-term-sterm}}
\ottdefnTyXXterm{}
\bgroup\renewcommand\ottaltinferrule[4]{
  \inferrule*[fraction={===},narrower=0.3,lab=#1,#2]
    {#3}
    {#4}
}
\ottdefnTyXXsterm{}
\egroup
\end{ottfig}

Destinations and holes are two faces of the same coin, as seen in~\cref{ssec:build-up-vocab}, and must always be in 1:1 correspondance. Thus, the new idea of our type system is to feature \emph{hole bindings} $[[{ + h : T n }]]$ and \emph{destination bindings} $[[{ - h : m ⌊ T ⌋ n }]]$ in addition to the variable bindings $[[{ x : m T}]]$ that usually populates typing contexts.

Such bindings mention two distinct classes of names: regular variable names $[[x]], [[y]]$, and \emph{hole names} $[[h]], [[h1]], [[h2]]$ which are identifiers for a memory cell that hasn't been written to yet. Hole names are represented by natural numbers under the hood, so they are equipped with addition $[[h+h']]$ and can act both as relative offsets or absolute positions in memory. Typically, when a structure is effectively allocated, its hole (and destination) names are shifted by the maximum hole name encountered so far in the program (denoted $[[max(hnames(C))]]$) ; this corresponds to finding the next unused memory cell in which to write new data.

The mode $[[n]]$ of a hole binding $[[{ + h : T n }]]$ (also present in the corresponding destination type $[[⌊ T ⌋ n]]$) indicates the mode a value must have to be written to it (that is to say, the mode of bindings that the value depends on to type correctly).\footnote{To this day, the only way for a value to have a constraining mode is to capture a destination (otherwise the value has mode $[[ω∞]]$, meaning it can be used in any possible way), as destinations are the only intrinsically linear values in the calculus, but we will see in~\cref{sec:implementation} that other forms of intrinsic linearity can be added to the langage for practical reasons.} We see the mode of a hole coming into play when a hole is located behind an exponential constructor: we should only write a non-linear value to the hole $[[+h]]$ in $[[ᴇ ων +h]]$. In particular, we should not store a destination into this hole, otherwise it could later be extracted and used in a non-linear fashion.

A destination binding $[[{ - h : m ⌊ T ⌋ n }]]$ mentions two modes, $[[m]]$ and $[[n]]$; but only the former (left one, $[[m]]$) is the actual mode of the binding (in particular, it informs on the age of the destination itself). The latter, $[[n]]$, is part of the destination's type $[[⌊ T ⌋ n]]$ and corresponds to the mode a value has to have to be written to the corresponding hole. In a well-typed, closed program, the mode $[[m]]$ of a destination binding can never be of multiplicity $[[ω]]$ or age $[[∞]]$ in the typing tree; it is always linear and of finite age.

We also extend mode product to a point-wise action on typing contexts:

\[\left\{\begin{array}{rcl}
  [[n'·({ x : m T })]] &=& [[{ x : n' · m T }]] \\
  [[n'·({ + h : T n })]] &=& [[{ + h : T n' · n }]] \\
  [[n'·({ - h : m ⌊ T ⌋ n })]] &=& [[{ - h : n' · m ⌊ T ⌋ n }]]
\end{array}\right.\]

\cref{fig:ty-term-sterm} presents the typing rules for terms, and rules for syntactic sugar forms that have been derived from term rules and proven formally too. \cref{fig:ty-val} presents the typing rules for values of the language.
In every figure,
\begin{itemize}
  \item $[[O]]$ denotes an arbitrary typing context, with no particular constraint;
  \item $[[G]]$ denotes a typing context made only of hole and destination bindings;
  \item $[[P]]$ denotes a typing context made only of destination and variable bindings;
  \item $[[D]]$ denotes a typing context made only of destination bindings.
\end{itemize}

\subsubsection{Typing of terms $[[⊢]]$}

A term $[[t]]$ always types in a context $[[P]]$ made only of destination and variable bindings. That being said, typing rules for terms and their syntactic sugar in~\cref{fig:ty-term-sterm} never explicitly mention a destination binding; only variable bindings. Only at runtime some variables will be substituted by destinations having a matching type and mode (that is why terms cannot type in a context made of variable bindings alone). As a result, the type system the user has to deal with is only slightly more complex than a linear type system \emph{à la}~\cite{bernardy_linear_2018}, because of the addition of the age control axis. So at the moment, we can forget about destination and hole bindings specificities.

Let's focus on a few particularities of the type system for terms.

The predicate $\mathtt{DisposableOnly}~[[P]]$ in rules \textsc{Ty-term-Val} and \textsc{Ty-term-Var} says that $[[P]]$ can only contain variable bindings with multiplicity $[[ω]]$, for which weakening is allowed in linear logic. It is enough to allow weakening at the leaves of the typing tree, that is to say the two aforementioned rules (\textsc{Ty-term-Val} is indeed a leaf for judgments $[[⊢]]$, that holds a subtree of judgments $[[⫦]]$).\critical{revisit if we allow weakening for dests, as we could replace that by $[[ων·P]]$ as in~\cite{bernardy_linear_2018}}

\newcommand{\pleq}{\stackrel{\pmb{\mathrm{p}}}{\mathtt{<:}}}
\newcommand{\aleq}{\stackrel{\pmb{\mathrm{a}}}{\mathtt{<:}}}

Rule \textsc{Ty-term-Var}, in addition to weakening, allows for dereliction of the mode for the variable used, with subtyping constraint $[[¹ν <: m]]$ defined as~~$[[p a]] \mathtt{<:} [[p' a']] \Longleftrightarrow [[p]] \pleq [[p']] \land~[[a]] \aleq [[a']]$ where:

{\setlength{\arraycolsep}{\widthof{\,}}
\begin{minipage}{0.2\linewidth}
$\left\{\begin{array}{rcl}
  [[¹]] &\pleq& [[¹]] \\
  [[p]] &\pleq& [[ω]]
\end{array}\right.$
\end{minipage}\begin{minipage}{0.8\linewidth}
$\left\{\begin{array}{rcl}
  [[↑]]^{m} &\aleq& [[↑]]^{n} \Longleftrightarrow m = n \quad \text{(no finite age dereliction ; recall that $[[↑]]^{0} = [[ν]]$)} \\
  [[a]] &\aleq& [[∞]]
\end{array}\right.$
\end{minipage}
}

Rule \textsc{Ty-term-PatU} is elimination (or pattern-matching) for unit, and is also used to chain destination-filling operations.

Pattern-matching with rules \textsc{Ty-term-App}, \textsc{Ty-term-PatS}, \textsc{Ty-term-PatP} and \textsc{Ty-term-PatE} is parametrized by a mode $[[m]]$ by which the typing context $[[P1]]$ of the scrutinee is multiplied. The variables which bind the subcomponents of the scrutinee then inherit this mode. In particular, this choice crystalize the equivalence $[[! ωa (T1 ⨂ T2)]] \simeq [[(! ωa T1) ⨂ (! ωa T2)]]$, which is not part of intuitionistic linear logic, but valid in Linear Haskell~\cite{bernardy_linear_2018}. We omit the mode annotation on $\ottkw{case}$ statements and lambda abstractions when the mode in question is the multiplicative neutral element $[[¹ν]]$ of the mode semiring.

The Rule \textsc{Ty-term-Map} is where most of the safety of the system lies, and it is there where scope control takes place. It opens an ampar $[[t]]$, and binds its right side (containing destinations for holes on the other side, among other things) to variable $[[x]]$ and then execute body $[[t']]$. This lets the user access destinations of an ampar while temporarily forgetting about the structure with holes (being mutated behind the scenes by the destination-filling primitives). Type safety for $\ottkw{map}$ is based on the idea that a new scope is created for $[[x]]$ and $[[t']]$, so anything already present in the ambient scope (represented by $[[P2]]$ in the conclusion) appears older when we see it from $[[t']]$ point of view. Indeed, when entering a new scope, the age of every remaining binding from the previous scopes is incremented by $[[↑]]$. That way we can distinguish $[[x]]$ from anything else that was already bound using the age of bindings alone. That's why $[[t']]$ types in $[[¹↑·P2,{ x : ¹ν T }]]$ while the global term $[[t map x ⟼ t']]$ types in $[[P1,P2]]$ (notice the absence of shift on $[[P2]]$). A schematic explanation of the scope rules is given in~\cref{fig:scope-rules}.

\begin{figure}[t]
  \scalebox{1.0}{\tikzfig{mapscopes}}
  \caption{Scope rules for $\ottkw{map}$ in \destcalculus{}}
  \label{fig:scope-rules}
\end{figure}

We see in the schema that the left of an ampar (the structure being built) ``takes place'' in the ambient scope. The right side however, where destinations are, has its own new, inner scope that is opened when $\ottkw{map}$ped over. When filling a destination (e.g. $[[x1 ˢ⨞ x0]]$ in the figure), the right operand must be from a scope $[[↑]]$ older than the destination on the left of the operator, as this value will end up on the left of the ampar (which is thus in a scope $[[↑]]$ older than the destination originating from the right side).

The rule \textsc{Ty-term-FillComp}, or its simpler variant, \textsc{Ty-sterm-FillLeaf} from~\cref{fig:ty-term-sterm} confirm this intuition. The left operand of these operators must be a destination that types in the ambient context (both $[[P1]]$ unchanged in the premise and conclusion of the rules). The right operand, however, is a value that types in a context $[[P2]]$ in the premise, but requires $[[¹↑·P2]]$ in the conclusion. This is the opposite of the shift that $\ottkw{map}$ does: while $\ottkw{map}$ opens a child scope for its body, \textopname{fillComp} ($\triangleleft\mybullet$)/\textopname{fillLeaf}($\blacktriangleleft$) opens a portal to the parent scope for their right operand, as seen in the schema. The same phenomenon happens for the resources captured by the body of a lambda abstraction in \textsc{Ty-term-FillF}.

In \textsc{Ty-term-ToA}, the operator $\ottkw{to}_{\ottkw{\ltimes}}$ embeds an already completed structure in an ampar whose left side is the structure, and right side is unit.

When using $\ottkw{from}_{\ottkw{\ltimes} }'$ (rule \textsc{Ty-sterm-FromA'}), the left of an ampar is extracted to the ambient scope (as seen at the bottom of~\cref{fig:scope-rules} with $[[x22]]$): this is the fundamental reason why the left of an ampar has to ``take place'' in the ambient scope. We know the structure is complete and can be extracted because the right side is of type unit ($[[①]]$), and thus no destination on the right side means no hole can remain on the left. $\ottkw{from}_{\ottkw{\ltimes}}'$ is implemented in terms of $\ottkw{from}_{\ottkw{\ltimes}}$ in~\cref{fig:sterm} to keep the core calculus tidier (and limit the number of typing rules, evaluation contexts, etc), but it can be implemented much more efficiently in a real-world implementation.

When an ampar is complete and disposed of with the more general $\ottkw{from}_{\ottkw{\ltimes} }$ in rule \textsc{Ty-term-FromA} however, we extract both sides of the ampar to the ambient scope, even though the right side is normally in a different scope. This is only safe to do because the right side is required to have type $[[! ¹∞ T]]$, which means it is scope-insensitive (it cannot contain any scope-controlled resource). This also ensures that the right side cannot contain destinations, meaning that the structure on the left is complete and ready to be read.

The remaining operators $[[⨞]][[()]], [[⨞]][[Inl]], [[⨞]][[Inr]], [[⨞]]\,\expcons{[[m]]}, [[⨞]][[(,)]]$ from rules \textsc{Ty-term-Fill$*$} are the destination-filling primitives. They write a hollow constructor to the hole pointed by the destination operand, and return the potential new destinations that are created in the process (or unit if there is none).

\subsubsection{Typing of values $[[⫦]]$}

\begin{ottfig}{\caption{Typing rules for values}\label{fig:ty-val}}\ottdefnTyXXval{}\end{ottfig}

Values $[[v]]$, presented as a subset of terms $[[t]]$, could be removed completely from the user syntax (given we promote $[[alloc]]$ to a first-class keyword), and just used as a denotation for runtime data structures.

The typing of runtime values, given in~\cref{fig:ty-val} is where hole and destination bindings appears. Values can have holes and destinations inside, but a value used as a term must not have any hole\footnote{see \textsc{Ty-term-Val}: the value must type in a context $[[D]]$ which means ``destination only'' by convention}. Also, variables cannot mention free variables; that makes it easier to prove substitution properties (as we will see in~\cref{ssec:sem}, we perform substitutions not only in terms, but also in evaluation contexts sometimes).

Hole bindings and destination bindings of the same hole name $[[h]]$ are meant to annihilate each other in the typing context given they have a matching base type and mode. That way, the typing context of a term can stay constant during reduction:

\begin{itemize}
  \item when destination-filling primitives are evaluated to build up data structures, they linearly consume a destination and write to a hole at the same time which makes both disappear, thus the typing context stays balanced;
  \item when a new hole is created, a matching destination is returned too, so the typing context stays balanced too.
\end{itemize}

However, the annihilation between a destination and a hole binding having the same name in the typing tree is only allowed to happen around an ampar, as it is the ampar connective that bind the name across the two sides (the names bound are actually stored in a set $[[H]]$ on the ampar value $[[H ⟨ v2 ❟ v1 ⟩]]$). In fact, an ampar can be seen as a sort of lambda-abstraction, whose body (containing holes instead of variables) and input sink (destinations) are split across two sides, and magically interconnected through the ampar connective.

In most rules, we use a sum $[[G1 + G2]]$ for typing contexts (or the disjoint variant $[[G1,G2]]$). This sum doesn't not allow for annihilation of bindings with the same name; the operation is partial, and in particular it isn't defined if a same hole name is present in the operands in the two different forms (hole binding and destination binding). In particular, a pair $[[(+h, -h)]]$ is not well-typed. A single typing context $[[G]]$ is not allowed either to contain both a hole binding and a destination binding for the same hole name.

\paragraph{Typing of ampars}

As stated above, the core idea of \textsc{Ty-val-Ampar} is to act as a binding connective for hole and destinations.

We define a new operator $\ottshname{\destminus^{\scriptscriptstyle\text{-}1} }$ to represent the matching hole bindings for a set of destination bindings. It is a partial, point-wise operation on typing bindings of a context where:

\[\begin{array}{rcl}
  [[-⁻¹ ({ - h : ¹ν ⌊ T ⌋ n })]] &=& [[{ + h : T n }]] \\
  % \ottshname{\destminus^{\scriptscriptstyle\text{-}1} }(\ottnt{n} :\!_{\![[m]]}[[T]]) &=& \text{error}
\end{array}\]

Only an input context $[[D]]$ made only of destination bindings, with mode $[[¹ν]]$, results in a valid output context (which is then only composed of hole bindings).

Equipped with this operator, we introduce the annihilation of bindings using $[[D3]]$ to represent destinations on the right side $[[v1]]$, and $[[-⁻¹D3]]$ to represent the matching hole bindings on the left side $[[v2]]$ (the structure under construction). Those bindings are only present in the premises of the rule but get removed in the conclusion. Those hole names are only local, bound to that ampar and don't affect the outside world nor can be referenced from the outside.

Both sides of the ampar may also contain stored destinations from other scopes, represented by $[[¹↑·D1]]$ and $[[D2]]$ in the respective typing contexts of $[[v1]]$ and $[[v2]]$.\unsure{Not sure whether we should explain why $[[D1]]$ is offset by $[[¹↑]]$ in the premise but not in conclusion. It's hard to come up with an intuitive narrative.} All holes introduced by this ampar have to be annihilated by matching destinations; following our naming convention, no hole binding can appear in $[[D1,D2]]$ in the conclusion.

The properties $\mathtt{LinOnly}~[[D3]]$ and $\mathtt{FinAgeOnly}~[[D3]]$ are true given that $[[-⁻¹ D3]]$ is a valid typing context, so are not really a new restriction on $[[D3]]$. They are mostly used to ease the mechanical proof of type safety for the system.

\paragraph{Other notable typing rules for values}

Rules \textsc{Ty-val-Hole} and \textsc{Ty-val-Dest} indicates that a hole or destination must have mode $[[¹ν]]$ in the typing context to be used (except when a destination is stored away, as we have seen)\critical{revisit this if we allow weakening for dests}.

Rules for unit, left and right variants, and product are straightforward.

Rule \textsc{Ty-val-Exp} is rather classic too: we multiply the dependencies $[[G]]$ of the value by the mode $[[n]]$ of the exponential. The intuition is that if $[[v]]$ uses a resource $[[v']]$ twice, then $\expcons{\ottsmode{2}}[[v]]$, that corresponds to two uses of $[[v]]$ (in a system with such a mode), will use $[[v']]$ four times.

Rule \textsc{Ty-val-Fun} indicates that (value level) lambda abstractions cannot have holes inside. In other terms, a function value cannot be built piecemeal like other data structures, its whole body must be a complete term right from the beginning. It cannot contain free variables either, as the body of the function must type in context $[[D,{x:m T}]]$ where $[[D]]$ is made only of destination bindings. One might wonder, how can we represent a curryfied function $[[ˢλ x ¹ν ⟼ ˢλ y ¹ν ⟼ x concat y]]$ as the value level, as the inner abstraction captures the free variable $[[x]]$ ? The answer is that such a function, at value level, is encoded as $[[ᵛλ x ¹ν ⟼ from⧔' (alloc map d ⟼ d ⨞ ( λ y ¹ν ⟼ x concat y))]]$, where the inner closure is not yet in value form, but pending to be built into a value. As the form $[[d ⨞ ( λ y ¹ν ⟼ t)]]$ is part of term syntax, and not value syntax, we allow free variable captures in it.

\section{Evaluation contexts and semantics}\label{sec:ectxs-sem}

The semantics of \destcalculus{} are given using small-step reductions on a pair $[[ C[t] ]]$ of an evaluation context $[[C]]$ (represented by a stack) determining a focusing path, and a term $[[t]]$ under focus. Such a pair $[[ C[t] ]]$ is called a \emph{command}, and represents a running program. We think that our semantics makes it easier to reason about types and linearity of a running program with holes than a store-based approach such as the one previewed in~\cref{sec:scope-escape-dests}.

\subsection{Evaluation contexts forms}\label{ssec:ectxs}

\begin{codefig}{\caption{Grammar for evaluation contexts}\label{fig:grammar-ectxs}}{\setlength{\arraycolsep}{1ex}
\!\!\!\begin{array}{rrl}
[[c]] &::=& [[t' ⬜]] \grammsep [[⬜ v]] \grammsep [[⬜ ; u]] \\
      &|\,& [[ ⬜ case m { Inl x1 ⟼ u1 , Inr x2 ⟼ u2 } ]] \grammsep [[⬜ case m ( x1 , x2 ) ⟼ u]] \grammsep [[⬜ case m ᴇ n x ⟼ u]] \\
      &|\,& [[⬜ map x ⟼ t']] \grammsep [[ to⧔ ⬜ ]] \grammsep [[ from⧔ ⬜ ]] \\
      &|\,& [[ ⬜ ⨞ () ]] \grammsep [[ ⬜ ⨞ Inl ]] \grammsep [[⬜ ⨞ Inr]] \grammsep [[⬜ ⨞ (,)]] \grammsep [[⬜ ⨞ ᴇ m]] \grammsep [[⬜ ⨞ ( λ x m ⟼ u )]] \grammsep [[⬜ ⨞· t']] \grammsep [[v ⨞· ⬜]] \\
      &|\,& [[ H ᵒᵖ⟨ v2 ❟ ⬜ ⟩ ]] \quad\quad\textit{(open ampar focus component)} \\
&&\\
[[C]] &::=& [[ ⬜ ]] \grammsep [[C ∘ c]] \grammsep [[C [ h ≔ H v ] ]]
\end{array}
}\end{codefig}

The grammar of evaluation contexts is given in~\cref{fig:grammar-ectxs}. An evaluation context $[[C]]$ is the composition of an arbitrary number of focusing components $[[c1]], [[c2]]\ldots$. We chose to represent this composition explicitly using a stack, instead of a meta-operation that would only let us access its final result. As a result, focusing and defocusing operations are made explicit in the semantics, resulting in a more verbose but simpler proof. It is also easier to imagine how to build a stack-based interpreter for such a language.

Focusing components are all directly derived from the term syntax, except for the ``open ampar'' focus component $[[H ᵒᵖ⟨ v2 ❟ ⬜ ⟩]]$. This focus component indicates that an ampar is currently being $\ottkw{map}$ped on, with its left-hand side $[[v2]]$ (the structure being built) being attached to the ``open ampar'' focus component, while its right-hand side (containing destinations) is either in subsequent focus components, or in the term under focus.

We introduce a special substitution $[[ C[h ≔ H v] ]]$ that is used to update structures under construction that are attached to open ampar focus components in the stack. Such a substitution is triggered when a destination $[[-h]]$ is filled in the term under focus, and results in the value $[[v]]$ (that may contain holes itself, e.g. if it is a hollow constructor $[[( +h1, +h2 )]]$) being written to the hole $[[+h]]$ (that must appear somewhere on an open ampar payload). The set $[[H]]$ tracks the potential hole names introduced by value $[[v]]$.

\subsection{Typing of evaluation contexts and commands}\label{ssec:ty-ectxs-cmd}

Evaluation contexts are typed in a context $[[D]]$ that can only contains destination bindings. $[[D]]$ represents the typing context available for the term that will be put in the box $\square$ of the evaluation context. In other terms, while the typing context of a term is a list of requirements so that it can be typed, the typing context of an evaluation context is the set of bindings that it makes available to the term under focus. As a result, while the typing of a term $[[P ⊢ t : T]]$ is additive (the typing requirements for a function application is the sum of the requirements for the function itself and for its argument), the typing of an evaluation context $[[D ⊣ C : T ↣ U0]]$ is subtractive : adding the focus component $[[t' ⬜]]$ to the stack $[[C]]$ will remove whatever is needed to type $[[t']]$ from the typing context provided by $[[C]]$. The whole typing rules for evaluation contexts $[[C]]$ as well as commands $[[ C[t] ]]$ are presented in~\cref{fig:ty-ectxs-cmd}.

\begin{ottfig}[p]{\caption{Typing rules for evaluation contexts and commands}\label{fig:ty-ectxs-cmd}}
\ottdefnTyXXectxs{}
\ottdefnTy{}
\end{ottfig}

An evaluation context has a pseudo-type $[[T]]\ottstype{\rightarrowtail}[[U0]]$, where $[[T]]$ denotes the type of the focus (i.e. the type of the term that can be put in the box of the evaluation context) while $[[U0]]$ denotes the type of the resulting command (when the box of the evaluation context is filled with a term).

Composing an evaluation context of pseudo-type $[[T]]\ottstype{\rightarrowtail}[[U0]]$ with a new focus component never affects the type $[[U0]]$ of the future command ; only the type $[[T]]$ of what can be put in the box is altered.

All typing rules for evaluation contexts can be derived from the ones for the corresponding term (except for the rule \textsc{Ty-ectxs-OpenAmpar} that is the truly new form). Let's take the rule \textsc{Ty-ectxs-PatP} as an example:

\begin{itemize}
  \item the typing context $[[m·D1 + D2]]$ in the premise for $[[C]]$ corresponds to $[[m·P1 + P2]]$ in the conclusion of \textsc{Ty-term-PatP} in~\cref{fig:ty-term-sterm};
  \item the typing context $[[D2,{ x1 : m T1 },{ x2 : m T2 }]]$ in the premise for term $[[u]]$ corresponds to the typing context $[[P2,{ x1 : m T1 },{ x2 : m T2 }]]$ for the same term in \textsc{Ty-term-PatP};
  \item the typing context $[[D1]]$ in the conclusion for $[[C ∘ (⬜ case m (x1 , x2) ⟼ u) ]]$ corresponds to the typing context $[[P1]]$ in the premise for $[[t]]$ in \textsc{Ty-term-PatP} (the term $[[t]]$ is located where the box $\square$ is in \textsc{Ty-ectxs-OpenAmpar}).
\end{itemize}

In a way, the typing rule for an evaluation context is a ``rotation'' of the typing rule for the associated term, where the typing contexts of one premise and the conclusion are swapped, an the typing context of the other potential premise is kept unchanged (with the added difference that free variables cannot appear in typing contexts of evaluation contexts, so any $[[P]]$ becomes a $[[D]]$).

As we see at the bottom of the figure, a command $[[ C [ t ] ]]$ (i.e. a pair of an evaluation context and a term) is well typed when the evaluation context $[[C]]$ provides a typing context $[[D]]$ that is exactly one in which $[[t]]$ is well typed. We can always embed a well-typed, closed term $[[{} ⊢ t : T]]$ as a well-typed command using the identity evaluation context: $[[t]] \simeq [[ ⬜[t] ]]$ and we thus have $[[⊢ ⬜[t] : T]]$ where $[[D]] = [[{ }]]$ (the empty context).

\subsection{Small-step semantics}\label{ssec:sem}

We equip \destcalculus{} with small-step semantics. There are three types of semantic rules:
\begin{itemize}
  \item focus rules, where we remove a layer from term $[[t]]$ (which cannot be a value) and push a corresponding focus component on the stack $[[C]]$;
  \item unfocus rules, where $[[t]]$ is a value and thus we pop a focus component from the stack $[[C]]$ and transform it back to a term, so that a redex appears (or so that another focus/unfocus rule can be triggered);
  \item reduction rules, where the actual computation logic takes place.
\end{itemize}

% \ExplSyntaxOn
% \NewDocumentCommand{\transformsemname}{m}
% {
%   \tl_set:Nn \l_tmpa_tl { #1 } % Store the input string in a temporary variable
% %  \regex_replace_all:nnN { \\[a-zA-Z]- } { \1 } \l_tmpa_tl
%   \regex_replace_once:nnN { (.+) - Red } { Red - \1 } \l_tmpa_tl % Perform the regex replacement
%   \regex_replace_once:nnN { (.+) - Focus } { Focus - \1 } \l_tmpa_tl % Perform the regex replacement
%   \regex_replace_once:nnN { (.+) - Unfocus } { Unfocus - \1 } \l_tmpa_tl % Perform the regex replacement
%   \regex_replace_once:nnN { (.+) - Unfocus } { Unfocus - \1 } \l_tmpa_tl % Perform the regex replacement
%   \l_tmpa_tl % Output the transformed string
% }
% \ExplSyntaxOff

\newlength{\tempwidth}
\newcommand{\ifnonempty}[2]{%
  \settowidth{\tempwidth}{#1}%
  \ifthenelse{\lengthtest{\tempwidth < 1ex}}{}{#2}
}

\bgroup
\renewcommand\arraystretch{1.4}
\renewcommand\ottaltinferrule[4]{
  % #4 is conclusion
  % #3 is premise
  % #1 is rule name
  %  & \text{#1}
  \ensuremath{#4} \ifnonempty{\ensuremath{#3}}{\quad\textit{when}\quad\ensuremath{#3}} \\
}
Here is the whole set of rules for \textsc{PatP}, in the previously announced order:
{\small\[\begin{array}{ll}\drule{Focus-PatP}
\drule{Unfocus-PatP}
\drule{Red-PatP}
\end{array}\]}
\egroup

Rules are triggered in a purely deterministic fashion; once a subterm is a value, it cannot be focused again. As focusing and defocusing rules are entirely mechanical (they are just a matter of pushing and popping a focus component on the stack), we only present the set of reduction rules for the system in~\cref{fig:sem}, but the whole system is included in the annex (see~\cref{fig:sem-full1,fig:sem-full2}).

\begin{ottfig}{\caption{Small-step semantics}\label{fig:sem}}
\bgroup
\renewcommand\arraystretch{1.4}
\renewcommand\ottaltinferrule[4]{
  % #4 is conclusion
  % #3 is premise
  % #1 is rule name
  %  & \text{#1}
  \ensuremath{#4} & \text{\textsc{#1}} \\
}
\makeatletter
\renewenvironment{drulepar}[3][\relax]
  {\ifx#1\relax\else\def\ottalt@rulesection@prefix{#1-}\fi
  \drulesectionhead{#2}{#3}$\!\!\!\array{ll}}
  {\endarray$}
\makeatother
\drules{$[[C [ t ] ⟶ C' [ t' ] ]]$}{Small-step evaluation of commands}{%
Red-App,
Red-PatU,
Red-PatL,
Red-PatR,
Red-PatP,
Red-PatE,
Red-ToA,
Red-FromA,
Red-FillU,
Red-FillF,
Red-FillL,
Red-FillR,
Red-FillE,
Red-FillP,
Red-FillComp,
Open-Ampar,
Close-Ampar}
\egroup
\vspace*{-0.5cm}
\[
\text{\textit{where}}\quad\left\{\begin{array}{rcl}
[[h']] &=& [[max(hnames(C) ∪ {h}) + 1]]\\
[[h'']] &=& [[max(hnames(C)) + 1]]
\end{array}\right.\]
\end{ottfig}

Reduction rules for function application, pattern-matching, $\ottkw{to}_{\ottkw{\ltimes} }$ and $\ottkw{from}_{\ottkw{\ltimes} }$ are straightforward.

All reduction rules for destination-filling primitives trigger a substitution $[[ C[h ≔ H v] ]]$ on the evaluation context $[[C]]$ that corresponds to a memory update of a hole $[[+h]]$. \textsc{Red-FillU} and \textsc{Red-FillF} do not create any new hole; they only write a value to an existing one. On the other hand, rules \textsc{Red-FillL}, \textsc{Red-FillR}, \textsc{Red-FillE} and \textsc{Red-FillP} all write a hollow constructor to the hole $[[h]]$, that is to say a value containing holes itself. Thus, we need to generate fresh names for these new holes, and also return a destination for each new hole with a matching name.

Obtaining a fresh name is represented by the statement $[[h']] = [[max(hnames(C) ∪ {h}) + 1]]$ in the premises of these rules. One invariant of the system is that an ampar must have fresh names to be opened, so we always rename local hole names bound by an ampar to fresh names just when that ampar is $\ottkw{map}$ped on, as these local names --- represented by the set of hole names $[[H]]$ that the ampar carries --- could otherwise shadow already existing names in evaluation context $[[C]]$. This invariant is materialized by premise $[[hnames(C)]] ~\mathtt{\#\#}~ [[hnames(D3)]]$ in rule \textsc{Ty-ectxs-OpenAmpar} for the open ampar focus component that is created during reduction of a $\ottkw{map}$.

We use hole name shifting as a strategy to obtain fresh names. Shifting all hole names in a set $[[H]]$ by a given offset $[[h']]$ is denoted $[[H ⩲ h']]$. We extend this notation to define a conditional shift operation $\ottshname{[}[[H ⩲ h']]\ottshname{]}$ which shifts each hole name appearing in the operand to the left of the brackets by $[[h']]$ if this hole name is also member of $[[H]]$. This conditional shift can be used on a single hole name, a value, or a typing context.

In rule \textsc{Red-FillComp}, we write the left-hand side $[[v2]]$ of a closed ampar $[[H ⟨ v2 ❟ v1 ⟩]]$ to a hole $[[+h]]$ that is part of some focus fragment $[[H' ᵒᵖ⟨ v2' ❟ ⬜⟩]]$ in the evaluation context $[[C]]$. That fragment is not mentioned explicitly in the rule, as the destination $[[-h]]$ is enough to target it. This results in the composition of two structures with holes $[[v2']]$ and $[[v2]]$ through filling of $[[-h]]$. Because we split open the ampar $[[H ⟨ v2 ❟ v1 ⟩]]$ (its left-hand side gets written to a hole, while its right hand side is returned), we need to rename any hole name that it contains to a fresh one, as we do when an ampar is opened in the $\ottkw{map}$ rule. The renaming is carried out by the conditional shift $[[v2[H ⩲ h'] ]]$ and $[[v1[H ⩲ h'] ]]$ (only hole names local to the ampar, represented by the set $[[H]]$, gets renamed).

Last but not least, rules \textsc{Open-Ampar} and \textsc{Close-Ampar} dictates how and when a closed ampar (a term) is converted to an open ampar (a focusing fragment) and vice-versa. With \textsc{Open-Ampar}, the local hole names of the ampar gets renamed to fresh ones, and the left-hand side gets attached to the focusing fragment $[[H⩲h' ᵒᵖ⟨ v2[H⩲h'] ❟ ⬜⟩]]$ while the right-hand side (containing destinations) is substituted in the body of the $\ottkw{map}$ statement (which becomes the new term under focus). This effectively allows the right-hand side of an ampar to be a term instead of a value for a limited time.

The rule \textsc{Close-Ampar} triggers when the body of a $\ottkw{map}$ statement has reduced to a value. In that case, we can close the ampar, by popping the focus fragment from the stack $[[C]]$ and merging back with $[[v2]]$ to reform a closed ampar.

\paragraph{Type safety} With the semantics now defined, we can state the usual type safety theorems:

\begin{theorem}[Type preservation]
  If $[[⊢ C [t] : T]]$ and $[[C[t] ⟶ C'[t'] ]]$ then $[[⊢ C' [t'] : T]]$.
\end{theorem}

\begin{theorem}[Progress]
  If $[[⊢ C [t] : T]]$ and $\forall [[v]], [[C [t] ]] \neq [[ ⬜[v] ]]$ then $\exists [[C']], [[t']].~[[ C [t] ⟶ C' [t'] ]]$.
\end{theorem}

A command of the form $[[ ⬜[v] ]]$ cannot be reduced further, as it only contains a fully determined value, and no pending computation. This it is the expected stopping point of the reduction, and any well-typed command is supposed to reach such a form at some point.

\section{Formal proof of type safety}\label{sec:formal-proof}

We've proved type preservation and progress theorems with the Coq proof assistant. At time of writing, we have assumed, rather than proved, the substitution lemmas. The choice of turning to a proof assistant was a pragmatic choice: the context handling in \destcalculus{} can be quite finicky, and it was hard, without computer assistance, to make sure that we hadn't made mistakes in our proofs. The version of \destcalculus{} that we've proved is written in Ott~\cite{sewell_ott_2007}, the same Ott file is used as a source for this article, making sure that we've proved the same system as we're presenting; some visual simplification is applied by a script to produce the version in the article.

Most of the proof was done by an author with little prior experience with Coq. This goes to show that Coq is reasonably approachable even for non-trivial development. The proof is about 6000 lines long, and contains nearly 350 lemmas. Many of the cases of the type preservation and progress lemmas are similar, to handle such repetitive cases using of a large-language-model based autocompletion system has been quite effective.

Binders are the biggest problem. We've largely manage to make the proof to be only about closed terms, to avoid any complication with binders. This worked up until the substitution lemmas, which is the reason why we haven't proved them in Coq yet (that and the fact that it's much easier to be confident in our pen-and-paper proofs for those). There are backends to generate locally nameless representations from Ott definitions~\cite{nardelli_nameless_2009}; we haven't tried them yet, but the unusual binding nature of ampars may be too much for them to handle.

The proofs aren't very elegant. For instance, we don't have any abstract formalization of semirings: since our semirings are finite it was more expedient to brute-force the properties we needed by hand. We've observed up to 232 simultaneous goals, but a computer makes short work of this: it was solved by a single call to the \verb|congruence| tactic. Nevertheless there are a few points of interest:
\begin{itemize}
\item we represent context as finite-domain functions, rather than as syntactic lists. This works much better when defining sums of context. There are a bunch of finite-function libraries in the ecosystem, but we needed finite dependent functions (because the type of binders depend on whether we're binding a variable name or a hole name). This didn't exist, but for our limited purpose, it ended up not being too costly rolling our own. About 1000 lines of proofs. The underlying data type is actual functions, this was simpler to develop, but equality is more complex than with a bespoke data type;
\item addition of context is partial since we can only add two binding of the same name if they also have the same type. Instead of representing addition as a binary function to an optional context, we represent addition as a total function to contexts, but we change contexts to have faulty bindings on some names. This simplifies reasoning about properties like commutativity and associativity, at the cost of having well-formedness preconditions in the premises of typing rules as well as some lemmas.
\end{itemize}

The inference rules produced by Ott aren't conducive to using setoid equality. This turned out to be a problem with our type for finite function:

\begin{verbatim}
Record T A B := {
    underlying :> forall x:A, option (B x);
    supported : exists l : list A, Support l underlying;
  }.
\end{verbatim}

where \verb+Support l f+ means that \verb+l+ contains the domain of \verb+f+. To make the equality of finite function be strict equality \verb+eq+, we assumed functional extensionality and proof irrelevance. In some circumstances, we've also needed to list the finite functions' domains. But in the definition, the domain is sealed behind a proposition, so we also assumed classical logic as well as indefinite description

\begin{verbatim}
Axiom constructive_indefinite_description :
    forall (A : Type) (P : A->Prop), (exists x, P x) -> { x : A | P x }.
\end{verbatim}

together, they let us extract the domain from the proposition. Again this isn't particularly elegant, we could have avoided some of these axioms at the price of more complex development. But for the sake of this article, we decided to favor expediency over elegance.

% I don't think we need to write this: methodology: assume a lot of lemmas, prove main theorem, prove assumptions, some wrong, fix. A number of wrong lemma initially assumed, but replacing them by correct variant was always easy to fix in proofs.

\section{Implementation of \destcalculus{} using in-place memory mutations}\label{sec:implementation}

The formal language presented in~\cref{sec:syntax-type-system,sec:ectxs-sem} is not meant to be implemented as-is.

First, \destcalculus{} misses a form of recursion, but we believe that adding equirecursive types and a fix-point operator wouldn't compromise the safety of the system.

Secondly, ampars are not managed linearly in \destcalculus{}; only destinations are. That is to say that an ampar can be wrapped in an exponential, e.g. $[[ᴇ ων {h} ⟨ Inr (Inl (), +h) ❟ -h ⟩]]$ (representing a non-linear difference list $\expcons{[[ων]]} (0 :: \holesq)$), and then used twice, each time in a different way:
\codehere{[[
ᴇ ων {h} ⟨ Inr (Inl (), +h) ❟ -h ⟩ case ¹ν ᴇ ων x ⟼⮒
‥‥let x1 ≔ x append (succ zero) in⮒
‥‥let x2 ≔ x append (succ (succ zero)) in⮒
‥‥‥‥toList (x1 concat x2)
]]\\[\interdefskip][[⟶]]^*~0 :: 1 :: 0 :: 2 :: []}

It may seem counter-intuitive at first, but this program is valid and safe in \destcalculus{}. Thanks to the renaming discipline we detailed in~\cref{ssec:sem}, every time an ampar is $\ottkw{map}$ped over, its hole names are renamed to fresh ones. So when we call $\ottkw{append}$ to build $[[x1]]$ (which is implemented in terms of $\ottkw{map}$), we sort of allocate a new copy of the ampar before mutating it, effectively achieving a copy-on-write memory scheme. Thus it is safe to operate on $[[x]]$ again to build $[[x2]]$.

In the introduction of the article, we announced a safe framework for in-place memory mutation, so we will uphold this promise now. The key to go from a copy-on-write scheme to an in-place mutation scheme is to force ampars to be linearly managed too. For that we introduce a new type $[[Token]]$, together with primitives $\ottkw{dup}$ and $\ottkw{drop}$ (remember that unqualified arrows have mode $[[¹ν]]$, so are linear):

\codehere{\phantom{a}\!\!\!\!\!\!\begin{array}[t]{l}%
\ottkw{dup} ~\pmb{:}~ [[Token ¹ν → Token ⨂ Token]]\\[\interdefskip]
\ottkw{drop} ~\pmb{:}~ [[Token ¹ν → ①]]\\[\interdefskip]
[[allocCoW]] ~\pmb{:}~ [[T ⧔ ⌊ T ⌋ ¹ν]]\\[\interdefskip]
[[allocIP]] ~\pmb{:}~ [[Token ¹ν → T ⧔ ⌊ T ⌋ ¹ν]]
\end{array}}

We now have two possible versions of $\ottkw{alloc}$: the new one with an in-place mutation memory model ($\ottsctor{ip}$), that has to be managed linearly, and the old one that doesn't have to used linearly, and features a copy-on-write ($\ottsctor{cow}$) memory model.

We use the $[[Token]]$ type as an intrinsic source of linearity that infects the ampar returned by $[[allocIP]]$. Such a token can be duplicated using $\ottkw{dup}$, but as soon as it is used to create an ampar, that ampar cannot be duplicated itself. In the system featuring the $[[Token]]$ type and $[[allocIP]]$, ``closed'' programs now typecheck in the non-empty context $\{[[{tok0 : ¹∞ Token}]]\}$ containing a token variable that the user can $\ottkw{dup}$licate and $\ottkw{drop}$ freely to give birth to an arbitrary number of ampars, that will then have to be managed linearly.

Having closed programs to typecheck in non-empty context $\{[[{tok0 : ¹∞ Token}]]\}$ is very similar to having a primitive function $\ottkw{withToken} ~\pmb{:}~ [[(Token ¹∞ → !ω∞ T) ¹ν → !ω∞ T]]$ as it is done in~\cite{bagrel_destination-passing_2024}.

In such an extension, as ampars are managed linearly, we can change the allocation and renaming mechanisms:
\begin{itemize}
  \item the hole name for a new ampar can be chosen fresh right from the start (this corresponds to a new heap allocation);
  \item adding a new hollow constructor still require freshness for its hole names (this corresponds to a new heap allocation too);
  \item $\ottkw{map}$ping over an ampar and filling destinations or composing two ampars using \textopname{fillComp} ($\triangleleft\mybullet$) no longer require any renaming: we have the guarantee that the names are globally fresh, and thus we can do in-place memory updates.
\end{itemize}

We decided to omit the linearity aspect of ampars in \destcalculus{} as it clearly obfuscate the presentation of the system without adding much to the understanding of the latter. We believe that the system is still sound with this linearity aspect, and articles such as~\cite{spiwack_linearly_2022} gives a pretty clear view on how to implement the linearity requirement for ampars in practice without too much noise for the user.

\section{Related work}\label{sec:related-work}

\info{Subsection for each work seems a little heavyweight. On the other hand the italic paragraphs seem too lightweight. I'm longing for the days of paragraph heading in bold.}

\subsection{Destination-passing style for efficient memory management}\label{ssec:shaikhha-dps}

In~\cite{shaikhha_destination-passing_2017}, the authors present a destination-based intermediate language for a functional array programming language. They develop a system of destination-specific optimizations and boast near-C performance.

This is the most comprehensive evidence to date of the benefit of destination-passing style for performance in functional programming languages. Although their work is on array programming, while this article focuses on linked data structure. They can therefore benefit of optimizations that are perhaps less valuable for us, such as allocating one contiguous memory chunk for several arrays.

The main difference between their work and ours is that their language is solely an intermediate language: it would be unsound to program in it manually. We, on the other hand, are proposing a type system to make it sound for the programmer to program directly with destinations.

We consider that these two aspects complement each other: good compiler optimization are important to alleviate the burden from the programmer and allowing high-level abstraction; having the possibility to use destinations in code affords the programmer more control would they need it.

\subsection{Tail modulo constructor}\label{ssec:tmc}

Another example of destinations in a compiler's optimizer is~\cite{bour_tmc_2021}. It's meant to address the perennial problem that the map function on linked lists isn't tail-recursive, hence consumes stack space. The observation is that there's a systematic transformation of functions where the only recursive call is under a constructor to a destination-passing tail-recursive implementation.

Here again, there's no destination in user land, only in the intermediate representation. However, there is a programmatic interface: the programmer annotates a function like
\begin{verbatim}
let[@tail_mod_cons] rec map =
\end{verbatim}
to ask the compiler to perform the translation. The compiler will then throw an error if it can't. This way, contrary to the optimizations in~\cite{shaikhha_destination-passing_2017}, this optimization is entirely predictable.

This has been available in OCaml since version 4.14. This is the one example we know of of destinations built in a production-grade compiler. Our \destcalculus{} makes it possible to express the result tail-modulo-constructor in a typed language. It can be used to write programs directly in that style,  or it could serve as a typed target language for and automatic transformation. On the flip-side, tail modulo constructor is too weak to handle our difference lists or breadth-first traversal examples.

\TODO{Mention Tail modulo context}

\subsection{A functional representation of data structures with a hole}\label{ssec:minamide}

The idea of using linear types to let the user manipulate structures with holes safely dates back to~\cite{minamide_functional_1998}. Our system is strongly inspired by theirs. In their system, we can only compose functions that represent data structures with holes, we can't pattern-match on the result; just like in our system we cannot act on the left-hand side of $[[S ⧔ T]]$, only the right hand part.

In~\cite{minamide_functional_1998}, it's only ever possible to represent structures with a single hole. But this is a rather superficial restriction. The author doesn't comment on this, but we believe that this restriction only exists for convenience of the exposition: the language is lowered to a language without function abstraction and where composition is performed by combinators. While it's easy to write a combinator for single-argument-function composition, it's cumbersome to write combinators for functions with multiple arguments. But having multiple-hole data structures wouldn't have changed their system in any profound way.

The more important difference is that while their system is based on a type of linear functions, our is based on the linear logic's par combinator. This, in turns, lets us define a type of destinations which are representations of holes in values, which~\cite{minamide_functional_1998} doesn't have. This means that~\cite{minamide_functional_1998} can implement our examples with difference lists and queues from~\cref{ssec:efficient-queue}, but it can't do our breadth-first traversal example from~\cref{sec:bft}, since storing destinations in a data structure is the essential ingredient of this example.

This ability to store destination does come at a cost though: the system needs this additional notion of ages to ensure that destinations are use soundly. On the other hand, our system is strictly more general, in that the system from~\cite{minamide_functional_1998} can be embedded in \destcalculus{}, and if one stays in this fragment, we're never confronted with ages. Ages only show up when writing programs that go beyond Minamide's system.

\subsection{Destination-passing style programming: a Haskell implementation}\label{ssec:dps-haskell}

In~\cite{bagrel_destination-passing_2024}, the author proposes a system much like ours: it has a destination type, and a par-like construct (that they call $\ottstype{Incomplete}$), where only the right-hand side can be modified; together these two elements give extra expressiveness to the language compared to~\cite{minamide_functional_1998}.

In their system, $[[d ˢ⨞ t]]$ requires $[[t]]$ to be unrestricted, while in \destcalculus{}, $[[t]]$ can be linear. The consequence is that in~\cite{bagrel_destination-passing_2024}, destinations can be stored in data structures but not in data structures with holes. In order to do a breadth-first search algorithm like in~\cref{sec:bft}, they can't use improved queues like we do, they have to use regular functional queues.

However, \cite{bagrel_destination-passing_2024} is implemented in Haskell, which just features linear types. Our system subsumes theirs; but it requires the age system that is more than what Haskell provides. Encoding their system in ours will unfortunately make ages appear in the typing rules.

\subsection{Semi-axiomatic sequent calculus}\label{ssec:sax}

In~\cite{deyoung_sax_2020}, the author develop a system where constructors return to a destination rather than allocating memory. It is very unlike the other systems described in this section in that it's completely founded in the Curry-Howard isomorphism. Specifically it gives an interpretation of a sequent calculus which mixes Gentzen-style deduction rules and Hilbert-style axioms. As a consequence, the par connective is completely symmetric, and, unlike our $[[⌊ T ⌋ ¹ν]]$ type, their dualization connective is involutive.

The cost of this elegance is that computations may try to pattern-match on a hole, in which case they must wait for the hole to be filled. So the semantic of holes is that of a future or a promise. In turns this requires the semantic of their calculus to be fully concurrent. Which is a very different point in the design space.

\section{Conclusion and future work}\label{sec:conclusion}

Using a system of ages in addition to linearity, \destcalculus{} is a purely functional calculus which supports destinations in a very flexible way. It subsumes existing calculi from the literature for destination passing, allowing both composition of data structures with holes and storing destinations in data structures. Data structures are allowed to have multiple holes, and destinations can be stored in data structures that, themselves, have holes. The latter is the main reason to introduce ages and is key to \destcalculus{}'s flexibility.

We don't anticipate that a system of ages like \destcalculus{} will actually be used in a programming language: it's unlikely that destination are so central to the design of a programming language that it's worth baking them so deeply in the type system. Perhaps a compiler that makes heavy use of destinations in its optimizer could use \destcalculus{} as a typed intermediate representation. But, more realistically, our expectation is that \destcalculus{} can be used as a theoretical framework to analyze destination-passing systems: if an API can be defined in \destcalculus{} then it's sound.

In fact, we plan to use this very strategy to design an API for destination passing in Haskell, leveraging only the existing linear types, but retaining the possibility of storing destinations in data structures with holes.

\clearpage{}
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}{}

\clearpage{}
\appendix

\section{Full reduction rules for \destcalculus{}}\label{apx:full-reduction-rules}

\newlength{\widthaugment}
\NewEnviron{augmentwidth}[1]
  {\setlength{\widthaugment}{#1}
   \pgfmathsetmacro{\myratio}{\linewidth / (\widthaugment + \linewidth)}
   \scalebox{\myratio}{\begin{minipage}{\linewidth+\widthaugment}\BODY
   \end{minipage}}}

\renewenvironment{rulesection}[3][\relax]
  {\trivlist\item
   \ifx#1\relax\else\def\ottalt@rulesection@prefix{#1-}\fi
   %\drulesectionhead{#2}{#3}%
   \nopagebreak[4]%
   \noindent}
  {\endtrivlist}
\renewcommand\ottaltinferrule[4]{
  \inferrule*[narrower=0.3,right=#1,#2]
    {#3}
    {#4}
}

\begin{ottfig}[h]{\caption{Full reduction rules for \destcalculus{} (part 1)}\label{fig:sem-full1}}\begin{augmentwidth}{1.7cm}
\drules{$[[C [ t ] ⟶ C' [ t' ] ]]$}{Small-step evaluation of commands}{%
Focus-AppOne,
Unfocus-AppOne,
Focus-AppTwo,
Unfocus-AppTwo,
Red-App,
%
Focus-PatU,
Unfocus-PatU,
Red-PatU,
%
Focus-PatS,
Unfocus-PatS,
Red-PatL,
Red-PatR,
%
Focus-PatP,
Unfocus-PatP,
Red-PatP,
%
Focus-PatE,
Unfocus-PatE,
Red-PatE,
%
Focus-Map,
Unfocus-Map,
Open-Ampar,
Close-Ampar
}
\end{augmentwidth}\end{ottfig}

\begin{ottfig}[h]{\caption{Full reduction rules for \destcalculus{} (part 2)}\label{fig:sem-full2}}\begin{augmentwidth}{1.7cm}
\drules{}{}{%
Focus-ToA,
Unfocus-ToA,
Red-ToA,
%
Focus-FromA,
Unfocus-FromA,
Red-FromA,
%
Focus-FillU,
Unfocus-FillU,
Red-FillU,
%
Focus-FillL,
Unfocus-FillL,
Red-FillL,
%
Focus-FillR,
Unfocus-FillR,
Red-FillR,
%
Focus-FillE,
Unfocus-FillE,
Red-FillE,
%
Focus-FillP,
Unfocus-FillP,
Red-FillP,
%
Focus-FillF,
Unfocus-FillF,
Red-FillF,
%
Focus-FillCompOne,
Unfocus-FillCompOne,
Focus-FillCompTwo,
Unfocus-FillCompTwo,
Red-FillComp
}
\end{augmentwidth}\end{ottfig}

\end{document}

% Local Variables:
% eval: (auto-fill-mode 0)
% eval: (visual-line-mode 1)
% ispell-local-dictionary: "en_US"
% End:
% LocalWords:  ampar combinators combinator Gentzen sequent semirings
% LocalWords:  involutive dualization autocompletion Ott backends Coq
% LocalWords:  extensionality semiring setoid allocator
