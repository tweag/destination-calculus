% -*- latex -*-
\documentclass[acmsmall, screen, review]{acmart}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\let\Bbbk\relax % needed because of a conflict between amssymb and newtx
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{environ}
\usepackage[capitalize]{cleveref}
\citestyle{acmauthoryear}

% For faulty code
\usepackage[normalem]{ulem}
\makeatletter
\def\uwave{\bgroup \markoverwith{\lower3.5\p@\hbox{\sixly \textcolor{red}{\char58}}}\ULon}
\font\sixly=lasy6 % does not re-load if already loaded, so no memory problem.
\makeatother

% For OTT rendering
\usepackage[supertabular]{ottalt}
\inputott{destination_calculus_ott.tex}
\usepackage{ottstyling}
% Hide "Index for ranges" from the metavars displayed tabular
\patchcmd{\ottmetavars}{$ \ottmv{k} $ & \ottcom{Index for ranges} \\}{}{}{}

% \setlength\textfloatsep{\baselineskip}
% \setlength{\intextsep}{\baselineskip}

%%%%%%%%%%%%%%%%% Editing marks %%%%%%%%%%%%%%%%%

  % TOGGLE ME to turn off all the commentary:
  \InputIfFileExists{no-editing-marks}{
    \def\noeditingmarks{}
  }

  \usepackage{xargs}
  \usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
  % ^^ Need for pgfsyspdfmark apparently?
  \ifx\noeditingmarks\undefined
      % Adapting to acmart's small margins
      \setlength{\marginparsep}{0.3em}
      \setlength{\marginparwidth}{1.4cm}

      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{{\color{blue}{#1}}}
      \newcommand{\note}[1]{{\color{blue}{\begin{itemize} \item {#1} \end{itemize}}}}
      \newenvironment{alt}{\color{red}}{}

      \newcommandx{\unsure}[2][1=]{\todo[linecolor=orange,backgroundcolor=orange!25,bordercolor=orange,#1]{#2}}
      \newcommandx{\info}[2][1=]{\todo[linecolor=green,backgroundcolor=green!25,bordercolor=green,#1]{#2}}
      \newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
      \newcommandx{\inconsistent}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
      \newcommandx{\critical}[2][1=]{\todo[linecolor=purple,backgroundcolor=purple!25,bordercolor=purple,#1]{#2}}
      \newcommand{\improvement}[1]{\todo[linecolor=pink,backgroundcolor=pink!25,bordercolor=pink]{#1}}
      \newcommandx{\resolved}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}} % use this to mark a resolved question
  \else
  %    \newcommand{\Red}[1]{#1}
      \newcommand{\Red}[1]{{\color{red}{#1}}}
      \newcommand{\newaudit}[1]{#1}
      \newcommand{\note}[1]{}
      \newenvironment{alt}{}{}
  %    \renewcommand\todo[2]{}
      \newcommand{\unsure}[2][1=]{}
      \newcommand{\info}[2][1=]{}
      \newcommand{\change}[2]{}
      \newcommand{\inconsistent}[2]{}
      \newcommand{\critical}[2]{}
      \newcommand{\improvement}[1]{}
      \newcommand{\resolved}[2]{}

  \fi

%%%%%%%%%%%%%%%%% /Editing marks %%%%%%%%%%%%%%%%%

\newcommand{\TODO}[1]{~\textnormal{\textcolor{red}{TODO: #1} } }
\newcommand\sepimp{\mathrel{-\mkern-6mu*}}
\newcommand{\textopname}[1]{``#1''}
\newcommand{\parr}{\rotatebox[origin=c]{180}{\&}}
\makeatletter
\newcommand{\smallbullet}{} % for safety
\DeclareRobustCommand\smallbullet{%
\mathord{\mathpalette\smallbullet@{0.5}}%
}
\newcommand{\smallbullet@}[2]{%
\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}%
}
\makeatother

\makeatletter
\newcommand{\oset}[3][0ex]{%
\mathrel{\mathop{#3}\limits^{
  \vbox to#1{\kern-2\ex@
  \hbox{$\scriptstyle#2$}\vss}}}}
\makeatother

\def\mycasem#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{}{#1}}
\def\myfunvm#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{\,\,}{#1}}
\def\myfuntm#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{\,}{#1}}
\def\mydestm#1{\ifthenelse{\equal{#1}{[[¹ν]]}}{}{#1}}
\def\mymul#1{\ifthenelse{\equal{#1}{[[¹]]}}{}{#1}}

\newcommand{\destcalculus}{\ensuremath{\lambda_d} }
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newcommand\btriangleq{\pmb{\triangleq}}
\newcommand\btriangleqrec{\oset{\mathsf{rec}}{\pmb{\triangleq}}}
\newlength{\interdefskip}
\setlength{\interdefskip}{0.15cm}
\newcommand{\newtype}[3][]{#2~\ifthenelse{\equal{#1}{}}{\btriangleq}{\btriangleqrec}~#3\\[\interdefskip]}
\newcommand{\newoperator}[5][]{\phantom{a}\!\!\!\!\!\!\begin{array}[t]{l}%
#2 ~\pmb{:}~ #3 \\
#4 ~\ifthenelse{\equal{#1}{}}{\btriangleq}{\btriangleqrec}~ #5
\end{array}\\[\interdefskip]}

\newcommand{\newoperatorb}[5][]{\phantom{a}\!\!\!\!\!\!\begin{array}[t]{l}%
#2 ~\pmb{:}~ #3 \\
#4 ~\ifthenelse{\equal{#1}{}}{\btriangleq}{\btriangleqrec}~ #5
\end{array}\\[\interdefskip]}
\newcommand{\figureratio}{0.9}
\newcommand{\codehere}[1]{\begin{center}\begin{minipage}{\figureratio\linewidth}{\small$#1$}\end{minipage}\end{center}}
\NewEnviron{codefig}[2][h]{\begin{figure}[#1]
\codehere{\BODY}#2
\end{figure}}
\NewEnviron{ottfig}[2][h]{\begin{figure}[#1]\visiblespaces
\small\BODY\activespaces #2
\end{figure}}

\newenvironment{stretchedarray}[2][1]
  {\bgroup\renewcommand*{\arraystretch}{#1}\begin{array}{#2}}
  {\end{array}\egroup}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\acmJournal{PACMPL}
\acmConference[POPL'25]{Principles of Programming Languages}{January 19 -- 25, 2025}{Denver, Colorado}
\title{Destination calculus}
\subtitle{A linear $\lambda-$calculus for pure, functional memory updates}

\author{Arnaud Spiwack}
\orcid{0TBD-0TBD-0TBD-0TBD}
\affiliation{
\institution{Tweag}
\department{OSPO}
\position{Director, Research}
\city{Paris}
\country{France}
}
\email{arnaud.spiwack@tweag.io}

\author{Thomas Bagrel}
\orcid{0009-0008-8700-2741}
\affiliation{
\institution{LORIA/Inria}
\department{MOSEL VERIDIS}
\city{Nancy}
\country{France}
}
\affiliation{
\institution{Tweag}
\department{OSPO}
\city{Paris}
\country{France}
}
\email{thomas.bagrel@loria.fr}
\email{thomas.bagrel@tweag.io}

% On introduit de la mutation controlée dans les FP languages sans endommager la pureté (comme la lazyness peut être vu aussi)

\begin{abstract}
We present the destination calculus, a linear $\lambda-$calculus for
pure, functional memory updates. We introduce the
syntax, type system, and operational semantics of the destination
calculus, and prove type safety formally in the Coq proof assistant.

We show how the principles of the destination calculus can form a theoretical ground
for destination-passing style programming in functional languages. In particular,
we detail how the present work can be applied to Linear Haskell to lift the main 
restriction of DPS programming in Haskell as developed in \cite{bagrel_destination-passing_2024}.
We illustrate this with a range of pseudo-Haskell examples.
\end{abstract}

\maketitle

%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Destination-passing style programming takes its root in the early days of imperative programming. In such language, the programmer is responsible for managing memory allocation and deallocation, and thus is it often unpractical for function calls to allocate memory for their results themselves. Instead, the caller allocates memory for the result of the callee, and passes the address of this output memory cell to the callee as an argument. This is called an \emph{out parameter}, \emph{mutable reference}, or even \emph{destination}.

But destination-passing style is not limited to imperative settings; it can be used in functional programming as well. One example is the linear destination-based API for arrays in Haskell\cite{bernardy_linear_2018}, which enables the user to build an array efficiently in a write-once fashion, without sacrificing the language identity and main guarantees. In this context, a destination points to a yet-unfilled memory slot of the array, and is said to be \emph{consumed} as soon as the associated hole is written to. In this paper, we continue on the same line: we present a linear $\lambda-$calculus embedding the concept of \emph{destinations} as first-class values, in order to provide a write-once memory scheme for pure, functional programming languages.

Why is it important to have destinations as first-class values? Because it allows the user to store them in arbitrary control or data structures, and thus to build complex data structures in arbitrary order/direction. This is a key feature of first-class DPS APIs, compared to ones in which destinations are inseparable from the structure they point to. In the latter case, the user is still forced to build the structure in its canonical order (e.g. from the leaves up to the root of the structure when using data constructors).

\section{Working with destinations}

\phantom{a}\TODO{Some introductory words}

\subsection{Building up a vocabulary}
\label{sec:build-up-vocab}

\activespaces

In its simplest form, destination passing, much like continuation passing, is the idea of explicitly receiving a location where to return a value to (we say that the destination is filled when the supplied location is written to). Instead of a function with signature $[[T ¹ν → U]]$, in \destcalculus you would have $[[T ¹ν → ⌊ U ⌋ ¹ν ¹ν → ①]]$, where $[[⌊ U ⌋ ¹ν]]$ is read “destination of type $[[U]]$”. For instance, here is a destination-passing version of the identity function:

\codehere{\newoperator
{\ottkw{dId}}{[[T ¹ν → ⌊ T ⌋ ¹ν ¹ν → ①]]}
{\ottkw{dId}~[[x]]~[[d]]}{[[d ˢ⨞ x]]}}

We think of a destination as a reference to an uninitialized memory location, and $[[d ˢ⨞ x]]$ (read “fill $[[d]]$ with $[[x]]$”) as writing $[[x]]$ to the memory location.

The form $[[d ˢ⨞ x]]$ is the simplest way to use a destination. But we don't have to fill a destination with a whole value in a single step. Destinations can be filled piecemeal.
\unsure{Maybe it would be wise to explain the notations for sums and tuples, since the linear-logic notations are less standard than the ccc ones}\unsure{We probably want to give a reading for the fill-with-a-constructor construction.}

\codehere{\newoperator
{\ottkw{fillWithInl}}{[[⌊ T ⨁ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν]]}
{\ottkw{fillWithInl}~[[d]]}{[[d ⨞ Inl]]}}

In this example, we're building a value of type $[[T ⨁ U]]$ by setting the outermost constructor to $[[Inl]]$. We think of $[[d ⨞ Inl]]$ as allocating memory to store a block of the form $[[Inl]]~\holesq$\unsure{Introduce the terminology “hollow constructor” here?}, write the address of that block to the location that $[[d]]$ points to, and return a destination pointing to the uninitialized argument of $[[Inl]]$.

Notice that we are constructing the term from the outermost constructor inward: we've built a value of the form $[[Inl]]~\holesq$ but we have yet to describe the constructor's payload (we call such incomplete values ``hollow constructors''). This is opposite to how functional programming usually works, where values are built from the innermost constructors outward: first we make a value $[[v]]$ and only then can we use $[[Inl]]$ to make an $[[Inl v]]$\inconsistent{For some reason this $[[v]]$ is grey. $\to$ Thomas : that's the case for non-terminals in the grammar}. This will turn out to be a key ingredient in the expressiveness of destination passing.

Yet, everything we've shown so far could have been done with continuations. So it's worth asking: how are destination different from continuations? Part of the answer lies in our intention to represent destinations as pointers to uninitialized memory (see \cref{sec:implementation}). But where destinations really differ from continuations is when there are several destinations. Then you can (indeed you must!) fill all the destinations; whereas when you have multiple continuations, you can only return to one of them. Multiple destination arises from filling destination of tuples:

\codehere{\newoperator
{\ottkw{fillWithAPair}}{[[⌊ T ⨂ U ⌋ ¹ν ¹ν → ⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν]]}
{\ottkw{fillWithAPair}~[[d]]}{[[d ⨞ (,)]]}}

To fill a destination for a pair, we must fill both the first field and the second field. In plain English, it sounds obvious, but the key remark is that $\ottkw{fillWithAPair}$ doesn't exist on continuations.

\paragraph{Values with holes}
Let's now turn to how we can use the result made by filling destinations. Observe, as a preliminary remark, that while a destination is used to build a structure, the type of the structure being built might be different from the type of the destination. For instance, $\ottkw{fillWithInl}$, above, returns a destination $[[⌊ T ⌋ ¹ν]]$ while it is used to build a structure of type $[[T ⨁ U]]$. To represents this, \destcalculus uses a type $[[S ⧔ ⌊ T ⌋ ¹ν]]$ for a structure of type $[[S]]$ missing a value of type $[[T]]$ to be complete (we say it has a hole of type $[[T]]$). There can be several holes in $[[S]]$, in which case the right-hand side is a tuple of destinations: $[[S ⧔ (⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν)]]$.\improvement{Somewhere around this point it would be good to explain the difference between holes and destinations. $\to$ Thomas: see DIFF-HOLE-DEST}

The form $[[S ⧔ ⌊ T ⌋ ¹ν]]$ is read “$[[S]]$ ampar destination of $[[T]]$”. The name “ampar” stands for “asymmetric memory par”; the reasons for this name will become apparent as we get into more details of \destcalculus in Section~\ref{ssec:ampar-motivation}. For now, it's sufficient to observe that $[[S ⧔ ⌊ T ⌋ ¹ν]]$ is akin to a $\ottstype{S \parr T^\perp}$ in linear logic, indeed you can think of $[[S ⧔ ⌊ T ⌋ ¹ν]]$ as a (linear) function from $[[T]]$ to $[[S]]$. That values with holes could be seen a linear functions was first observed in~\cite{minamide_functional_1998}, we elaborate on the value of having a par type rather than a function type in \cref{sec:bft}. A similar connective is called $\ottstype{Incomplete}$ in~\cite{bagrel_destination-passing_2024}.

Destinations always exist within the context of a structure with holes. A hole of type $[[T]]$ inside $[[S]]$ represents the fact that $[[S]]$ contains uninitialized memory that will have to hold a $[[T]]$ for $[[S]]$ to be readable without errors; it only denotes the absence of a value and thus cannot be manipulated directly. A destination $[[⌊ T ⌋ ¹ν]]$, on the other hand, is a first-class value that witnesses the presence of a hole of type $[[T]]$ inside $[[S]]$ and can be used to complete the structure\improvement{DIFF-HOLE-DEST}. To access the destinations, \destcalculus provides a $\ottkw{map}$ construction, which lets us apply a function to the right-hand side of an ampar:

\unsure{It may be more readable to have fillWithInl' and fillWithAPair' side by side. Maybe they're too wide though}

\medskip

\noindent\begin{minipage}{0.5\textwidth}
\codehere{\newoperator
{\ottkw{fillWithInl'}}{[[S ⧔ ⌊ T ⨁ U ⌋ ¹ν ¹ν → S ⧔ ⌊ T ⌋ ¹ν]]}
{\ottkw{fillWithInl'}~[[x]]}{[[x map d ⟼ d ⨞ Inl]]}}
\end{minipage}\begin{minipage}{0.5\textwidth}
\codehere{\newoperator
{\ottkw{fillWithAPair'}}{[[S ⧔ ⌊ T ⨂ U ⌋ ¹ν ¹ν → S ⧔ (⌊ T ⌋ ¹ν ⨂ ⌊ U ⌋ ¹ν)]]}
{\ottkw{fillWithAPair'}~[[x]]}{[[x map d ⟼ d ⨞ Inl]]}}
\end{minipage}

To tie this up, we need a way to introduce and to eliminate values with holes. Values with holes are introduced with $[[alloc]]$ which creates a value $[[T ⧔ ⌊ T ⌋ ¹ν]]$, which is simply a hole expecting a value of type $[[T]]$. Values with holes are eliminated with\footnote{As the name suggest, there is a more general elimination $\ottkw{from}_{\ottstype{\ltimes} }$. It will be discussed in \cref{sec:type-system}.} $\ottkw{from}_{\ottstype{\ltimes} }' : [[S⧔① ¹ν → S]]$: if all the destinations have been filled, then a value with holes is really just a normal, complete value.

Equipped with these, we can, for instance, derive traditional constructors from piecemeal filling. In fact, \destcalculus doesn't have primitive constructor forms, they are syntactic sugar. We show here the definition of $[[Inl]]$ and $[[(,)]]$, but the other constructors are derived similarly.\unsure{Wording slightly awkward. Revisit at some point. Maybe point out that it makes destinations strictly more expressive than traditional constructor. Maybe discuss as Thomas did at some point that this means that the order in which a structure is built is actually arbitrary, not just outside-in.}

\codehere{\newoperator
{[[Inl]]}{[[T ¹ν → T ⨁ U]]}
{[[ˢInl x]]}{[[from⧔' (alloc map d ⟼ d ⨞ Inl ˢ⨞ x)]]}}

\codehere{\newoperator
{[[(,)]]}{[[T ¹ν → U ¹ν → T ⨂ U]]}
{[[ˢ(x, y)]]}{[[from⧔' (alloc map d ⟼ (d ⨞ (,)) case ¹ν  (d1, d2) ⟼ d1 ˢ⨞ x; d2 ˢ⨞ y)]]}}


\paragraph{Purity}\unsure{Thomas: I do not really like this paragraph. I would like this paragraph to be the other way around: we want a pure model, in particular a model that doesn't break observable immutability. So write-(exactly)-once memory model, with no way to read before every hole has been written to. Thus we need a linear type system.}
At this point, the reader may be forgiven for feeling distressed at all the talk of filling by mutation\improvement{Make sure that we make it clear} and uninitialized memory. How is it consistent with our claim to be building a pure language\improvement{Ensure that we do claim that}?\unsure{Let's revisit the phrasing of this couple of sentences at some point, it's not utterly elegant.}

Indeed, unrestricted use of destination can lead to rather catastrophic results. The simplest such issue happens when we forget to fill a destination:

\codehere{\newoperator
{\ottkw{forget}}{[[T]]}
{\ottkw{forget}}{[[from⧔' (alloc map d ⟼ ())]]}}

Here, $\ottkw{forget}$ claims to be a value of type $[[T]]$, but it's really just a hole, just uninitialized memory. So any attempt to use $\ottkw{forget}$ will read uninitialized memory.

A similar situation can happen if we reuse a destination.\critical{This is a terrible example, because what's really happening here is that we're forgetting a destination. Let's find a true problem of writing several times. Maybe some order-dependence issue?}

\phantom{a}\TODO{Syntactic sugar: let $\to$ Thomas: I added $[[let x ≔ t in u]]$ and its linebreak-allowing versions}
\codehere{\newoperator
{\ottkw{overwrite}}{[[T¹ν → T ⨁ U]]}
{\ottkw{overwrite}~[[x]]}{[[from⧔' (alloc map d ⟼ (ˢλ d' ¹ν ⟼ d ⨞ Inr; d' ˢ⨞ x) (d ⨞ Inl))]]}}\unsure{It took me too much time to understand what was happening there and why it is wrong}

These programs are rejected by \destcalculus, using a linear type system, ensuring that \destcalculus is a pure and safe language.\unsure{we should probably discuss the definition of ``pure'' somewhere.} This justifies the claim that we can think of destinations as write-once memory locations\unsure{are we making this claim?}.

Linearity of destination isn't the only measure that we need to take in order to ensure that uninitialized memory is never read. Indeed consider a value of type $[[v]]:[[S⧔T]]$. In memory $[[v]]$ is a value of type $[[S]]$ except that it has some uninitialized memory\critical{This is quite incorrect, $[[v]]$ is rather a pair, and only its left side cannot be read. At least, if we stick to a general $[[T]]$ on the right hand side, it would be too much of a lie to say that there isn't anything else than a $[[S]]$ in memory}. So we have really careful when we read anything in $[[v]]$. In \destcalculus we take the bluntest restriction: we simply do not give any way to read $[[v]]$ at all. \change{The related work must mention that Minamide has this same restriction. Whereas as SNAX (semi-axiomatisable sequent calculus) doesn't, in exchange of having the ability to wait on unfilled destination and requiring a parallel execution semantics. $\to$ Thomas: in SNAX we thus get involutive dest, and symetric ampar, as everything is now a promise}\unsure{I wanted to have an example here, but I couldn't find a natural way to write something bad since we truly don't have a tool to write them.}

\subsection{Functional queues, with destinations}\label{ssec:efficient-queue}

If we suppose equirecursive types and a fixed-point operator, then \destcalculus becomes expressive enough to build any usual data structure.

\paragraph{Linked lists}

For starters, we can define lists as the fixpoint of the functor $[[X]] \mapsto [[① ⨁ (T ⨂ X)]]$ where $[[T]]$ is the type of list items. Following the recipe that we've outline so far, instead of defining the usual \textopname{nil} $\ottsctor{[]}$ and \textopname{cons} $\ottsctor{(::)}$ constructors, we define the more general \textopname{fillNil}\unsure{Arnaud hasn't used in the previous section small-caps name for constructors and operations. If we are to use them in the next few sections, it would be good to introduce this vocabulary early.} $\triangleleft\ottsctor{[]}$ and \textopname{fillCons} $\triangleleft\ottsctor{(::)}$ operators, as presented in Figure~\ref{fig:impl-list}.

\begin{codefig}{\caption{List implementation in equirecursive destination calculus}\label{fig:impl-list}}
\newtype[~\mathttbf{rec}]{[[List T]]}{[[① ⨁ (T ⨂ (List T))]]}
\newoperator
  {\triangleleft\ottsctor{[]}}{[[⌊ List T ⌋ ¹ν ¹ν → ①]]}
  {[[d ⨞ [] ]]}{[[d ⨞Inl ⨞()]]}
\newoperator
  {\triangleleft\ottsctor{(::)}}{[[⌊ List T ⌋ ¹ν ¹ν → ⌊ T⌋ ¹ν ⨂ ⌊ List T ⌋ ¹ν]]}
  {[[d ⨞ (::)]]}{[[d ⨞Inr ⨞(,)]]}
\end{codefig}

Just like we did in \cref{sec:build-up-vocab} for the primitive constructors, we can recover the \textopname{cons} constructor: $[[T ⨂ (List T) ¹ν → List T]]$:

\codehere{\newoperator
{\ottsctor{(::)}}{[[T ⨂ (List T) ¹ν → List T]]}
{\ottsctor{(::)}~[[x]]~[[xs]]}{[[from⧔' (alloc map d ⟼ (d ⨞ (::)) case ¹ν (dx, dxs) ⟼ dx ˢ⨞ x ; dxs ˢ⨞ xs)]]}}

Going from a \textopname{fill} operator to the associated constructor is completely generic, and with more metaprogramming tools, we could build this transformation into the language.

\paragraph{Difference lists}

While linked lists are optimized for the prepend operation (\textopname{cons}), they are not efficient for appending or concatenation, as it requires a full copy (or traversal at least) of the first list before the last cons cell can be changed to point to the head of the second list.

Difference lists are a data structure that allows for efficient concatenation. In functional languages, difference lists are often encoded using a function that take a tail, and returns the previously-unfinished list with the tail appended to it. For example, the difference list $[[x1]] \ottsctor{:} [[x2]] \ottsctor{:} \ldots \ottsctor{:} [[xk]] \ottsctor{:} \holesq$ is represented by the linear function $\lambda [[xs]] \mapsto [[x1]] \ottsctor{:} [[x2]] \ottsctor{:} \ldots \ottsctor{:} [[xk]] \ottsctor{:} [[xs]]$. This encoding shines when list concatenation calls are nested to the left, as the function encoding delays the actual concatenation so that it happens in a more optimal, right-nested fashion.

In destination calculus, we can go even further, and represent difference lists much like we would do in an imperative programming language (although in a safe setting here), as a pair of an incomplete list who is missing its tail, and a destination pointing to the missing tail's location. This is exactly what an ampar is designed to allow: thanks to an ampar, we can handle incomplete structures safely, with no need to complete them immediately. The incomplete list is represented by the left side of the ampar, and the destination is represented by its right side. Creating an empty difference list is exactly what the $[[alloc]]$ primitive already does when specialized to type $[[List T]]$ : it returns an incomplete list with no items in it, and a destination pointing to that cell so that the list can be built later through destination-filling primitives, together in an ampar: $[[List T ⧔ ⌊List T⌋ ¹ν]]$. Type definition and operators for difference lists in destination calculus are presented in Figure~\ref{fig:impl-dlist}.

\begin{codefig}{\caption{Difference list implementation in equirecursive destination calculus}\label{fig:impl-dlist}}
\newtype{[[DList T]]}{[[(List T) ⧔ ⌊ List T ⌋ ¹ν]]}
\newoperator
  {\ottkw{append}}{[[DList T ¹ν → T ¹ν → DList T]]}
  {[[ys append y]]}{\!\!\!\begin{array}[t]{l}[[
ys map dys ⟼ (dys ⨞ (::)) case ¹ν⮒
‥‥(dy, dys') ⟼ dy ˢ⨞ y ; dys'
]]\end{array}}
\newoperator
  {\ottkw{concat}}{[[DList T ¹ν → DList T ¹ν → DList T]]}
  {[[ys concat ys']]}{[[ys map d ⟼ d ⨞· ys']]}
\newoperator
  {\ottkw{to}_{\ottstype{List} }}{[[DList T ¹ν → List T]]}
  {[[toList ys]]}{[[from⧔' (ys map d ⟼ d ⨞ [])]]}
\end{codefig}

The $\ottkw{append}$ simply appends an element at the end of the list. It uses \textopname{fillCons} to link a new hollow \textopname{cons} cell at the end of the list, and then handles the two associated destinations $[[dy]]$ and $[[dt]]$. The former, representing the item slot, is fed with the item to append, while the latter, representing the slot for the tail of the resulting difference list, is returned and so stored back in the right side of the ampar. If that second destination was consumed, and not returned, we would end up with a regular linked list, instead of a difference list.

The $\ottkw{concat}$ operator concatenates two difference lists by writing the head of the second one to the hole left at the end of the first one. This is done using the \textopname{fillComp} primitive $\triangleleft\mybullet \pmb{:} [[⌊ U1 ⌋n ¹ν → U1 ⧔ U2 ¹↑ · n → U2]]$\footnote{In this particular context, $[[U1]] = [[List T]]$ and $[[U2]] = [[⌊ List T ⌋¹ν]]$, so \textopname{fillComp} has signature $\triangleleft\mybullet \pmb{:} [[⌊ List T ⌋¹ν ¹ν → DList T ¹↑ → ⌊ List T ⌋¹ν]]$}. It takes a destination on its left-hand side, and an ampar on its right-hand side. The left side of the ampar (type $[[U1]]$) is fed to the destination (so the incomplete structure is written to a larger incomplete structure from which the destination originated from), and the right side of the ampar (type $[[U2]]$) is returned.

Here the left side of the second ampar is the second incomplete list, which is pasted at the end of the first incomplete list, consuming the destination of the first difference list in the process. Then the right side of the second ampar, that is to say the destination to the yet-unspecified tail of the second difference list, is returned, and stored back in the resulting ampar (thus serves as the new destination to the tail of the the resulting difference list).

Finally, the $\ottkw{to}_{\ottstype{List}}$ operator converts a difference list to a regular list by writing the \textopname{nil} constructor to the hole left in the incomplete list using \textopname{fillNil}.

We can note that although this exemple is typical of destination-style programming, it doesn't use the first-class nature of destinations that our calculus allows, and thus can be implemented in other destination-passing style frameworks such as~\cite{bour_tmc_2021} and~\cite{leijen_trmc_2023}. We will see in the next sections what kind of programs can be benefit from first-class destinations.

\paragraph{Efficient queue using previously defined structures}

The usual functional encoding for a queue is two use a pair of lists, one representing the front of the queue, and keeping the element in order, while the second list represent the back of the queue, and is kept in reversed order (e.g the latest inserted element will be at the front of the second list).

With such a queue implementation, dequeueing the front element is efficient (just pattern-match on the first cons cell of the first list, $\mathcal{O}(1)$), and enqueuing a new element is efficient too (just add a new \textopname{cons} cell at the front of the second list, $\mathcal{O}(1)$ too). However, when the first list is depleted, one has to transfer elements from the second list to the first one, and as such, has to reverse the second list, which is a $\mathcal{O}(n)$ operation (although it is amortized).

With access to efficient difference lists, as shown in the previous paragraph, we can replace the second list by a difference list, to maintain a quick $\ottkw{enqueue}$ operation (still $\mathcal{O}(1)$), but remove the need for a $\ottkw{reverse}$ operation (as $\ottkw{to}_{\ottstype{List} }$ is $\mathcal{O}(1)$ for difference lists). Nothing needs to change for the first list. The corresponding implementation is presented in Figure~\ref{fig:impl-queue}.

\begin{codefig}{\caption{Queue implementation in equirecursive destination calculus}\label{fig:impl-queue}}
\newtype{[[Queue T]]}{[[(List T) ⨂ (DList T)]]}
\newoperator 
  {\ottkw{singleton}}{[[T ¹ν → Queue T]]}
  {[[singleton x]]}{[[ˢ(ˢInr ˢ(x, Inl ()), alloc)]]}
\newoperator
  {\ottkw{enqueue}}{[[Queue T ¹ν → T ¹ν → Queue T]]}
  {[[q enqueue y]]}{[[q case ¹ν (xs, ys) ⟼ ˢ(xs, ys append y)]]}
\newoperator
  {\ottkw{dequeue}}{[[Queue T ¹ν → ① ⨁ (T ⨂ (Queue T))]]}
  {[[dequeue q]]}{\!\!\!\begin{array}[t]{l}[[
q case ¹ν {⮒
‥‥ˢ(ˢInr ˢ(x, xs), ys) ⟼ ˢInr ˢ(x, ˢ(xs, ys)),⮒
‥‥ˢ(Inl (), ys) ⟼ (toList ys) case ¹ν {⮒
‥‥‥‥Inl () ⟼ Inl (),⮒
‥‥‥‥ˢInr ˢ(x, xs) ⟼ ˢInr ˢ(x, ˢ(xs, alloc))⮒
‥‥}⮒
}
]]\end{array}}
\end{codefig}

The $\ottkw{singleton}$ operator creates a pair of a list with a single element, and a fresh difference list (obtained via $[[alloc]]$).\change{Replace by the empty queue}

The $\ottkw{enqueue}$ operator appends an element to the difference list, while letting the front list unchanged.

The $\ottkw{dequeue}$ operator is more complex though. It first checks if there is at least one element available in the front list. If there is, it extracts the element $[[x]]$ by removing the first \textopname{cons} cell of the front list, and returns it alongside the rest of the queue $[[ˢ(xs, ys)]]$. If there isn't, it converts the difference list $[[ys]]$ to a normal list, and pattern-matches on it to look for an available element. If none is found again, it returns $[[Inl ()]]$ to signal that the queue is definitely empty. If an element $[[x]]$ is found, then it returns it alongside the updated queue, made of the tail $[[xs]]$ of the difference list turned into a list, and a fresh difference list given by $[[alloc]]$.

\section{Limitations of the previous approach}

Everything described above is in fact already possible in destination-passing style for Haskell as presented in~\cite{bagrel_destination-passing_2024}. However, there is one fundamental limitation in~\cite{bagrel_destination-passing_2024}: the inability to store destinations in destination-based data structures.

Indeed, that first approach of destination-passing style for Haskell can only be used to build non-linear data structures. More precisely, the \textopname{fillLeaf} operator ($\triangleleft$) can only take arguments with multiplicity $[[ω]]$. This is in fact a much stronger restriction than necessary ; the core idea is \emph{just} to prevent any destination (which is always a linear resource) to appear somewhere in the right-hand side of \textopname{fillLeaf}.\improvement{Don't start with the solution, start with the problem. Then we can explain how previous attempts addresses it. In JFLA: fill's right-hand operand wants omega, in Minamide/Tail-modulo-context, we can't store destinations to begin with.}

\subsection{The problem with stored destinations}\label{ssec:problem-stored-dests}

One core assumption of destination-passing style programming is that once a destination has been linearly consumed, the associated hole has been written to.

However, in a realm where destinations $\ottstype{\lfloor[[T]]\rfloor}$ can be of arbitrary inner type $[[T]]$, they can in particular be used to store a destination itself when $[[T]] = \ottstype{\lfloor[[T']]\rfloor}$!

We have to mark the value being fed in a destination as linearly consumed, so that it cannot be both stored away (to be used later) and pattern-matched on/used in the current context. But that means we have to mark the destination $[[d]] \pmb{:} \ottstype{\lfloor[[T']]\rfloor}$ as linearly consumed too when it is fed to $[[dd]] \pmb{:} \ottstype{\lfloor\lfloor[[T']]\rfloor\rfloor}$ in $[[dd ˢ⨞ d]]$.

As a result, there are in fact two ways to consume a destination: feed it now with a value, or store it away and feed it later. The latter is a much weaker form of consumption, as it doesn't guarantee that the hole associated to the destination has been written to \emph{now}, only that it will be written to later. So our assumption above doesn't hold in general case.

The issue is particularly visible when trying to give semantics to the $\ottkw{alloc'}$ operator with signature $\ottkw{alloc'} \pmb{:} \ottstype{(\lfloor[[T]]\rfloor\,_{\mymul{[[¹]]}}\!\ottstype{\to}\,[[①]])\,_{\mymul{[[¹]]}}\!\ottstype{\to}\,[[T]]}$. It reads: ``given a way of consuming a destination of type $[[T]]$, I'll return an object of type $[[T]]$''. This is an operator we very much want in our system!

The morally correct semantics (in destination calculus pseudo-syntax) would be:\unsure{I'm [Arnaud] really unsure about introducing this one-off notation that we're never using again. Even though I like the use of the underwave to highlight the problem.}

\codehere{\ottkw{alloc'}~(\ottsctor{\lambda}[[d]]\,_{\mymul{[[¹]]}}\!\!\mapsto[[t]]) ~[[⟶]]~ \ottkw{withTmpStore}~\{ [[h]] \assigneq \holesq \}~\ottkw{do}~\{ [[ t[d ≔ -h] ]]~\patu~\ottkw{deref}~[[-h]] \}}

It works as expected when the function supplied to $\ottkw{alloc'}$ will indeed use the destination to store a value:

\codehere{\!\!\!\begin{array}{cl}
      & \ottkw{alloc'}~(\ottsctor{\lambda}[[d]]\,_{\mymul{[[¹]]}}\!\!\mapsto[[d ⨞ Inl ⨞ ()]]) \\
[[⟶]] & \ottkw{withTmpStore}~\{ [[h]] \assigneq \holesq \}~\ottkw{do}~\{ [[-h ⨞ Inl ⨞ ()]]~\patu~\ottkw{deref}~[[-h]] \} \\
[[⟶]] & \ottkw{withTmpStore}~\{ [[h]] \assigneq [[Inl ()]] \}~\ottkw{do}~\{ \ottkw{deref}~[[-h]] \} \\
[[⟶]] & [[Inl ()]]
\end{array}}

However this falls short when calls to $\ottkw{alloc'}$ are nested in the following way (where $[[dd]]$ \pmb{:} $\ottstype{\lfloor\lfloor[[①]]\rfloor\rfloor}$ and $[[d]]$ \pmb{:} $\ottstype{\lfloor[[①]]\rfloor}$):

\codehere{\!\!\!\begin{array}{cl}
      & \ottkw{alloc'}~(\ottsctor{\lambda}[[dd]]\,_{\mymul{[[¹]]}}\!\!\mapsto\ottkw{alloc'}~(\ottsctor{\lambda}[[d]]\,_{\mymul{[[¹]]}}\!\!\mapsto[[dd ˢ⨞ d]])) \\
[[⟶]] & \ottkw{withTmpStore}~\{ [[hd]] \assigneq \holesq \}~\ottkw{do}~\{ \ottkw{alloc'}~(\ottsctor{\lambda}[[d]]\,_{\mymul{[[¹]]}}\!\!\mapsto[[-hd ˢ⨞ d]])~\patu~\ottkw{deref}~[[-hd]] \} \\
[[⟶]] & \ottkw{withTmpStore}~\{ [[hd]] \assigneq \holesq \}~\ottkw{do}~\{ \ottkw{withTmpStore}~\{ [[h]] \assigneq \holesq \}~\ottkw{do}~\{ [[-hd ˢ⨞ -h]]~\patu~\ottkw{deref}~[[-h]] \}~\patu~\ottkw{deref}~[[-hd]] \} \\
[[⟶]] & \ottkw{withTmpStore}~\{ [[hd]] \assigneq [[-h]] \}~\ottkw{do}~\{ \uwave{ \ottkw{withTmpStore}~\{ [[h]] \assigneq \holesq \}~\ottkw{do}~\{ \ottkw{deref}~[[-h]] \} }~\patu~\ottkw{deref}~[[-hd]] \}
\end{array}}

The original term $\ottkw{alloc'}~(\ottsctor{\lambda}[[dd]]\,_{\mymul{[[¹]]}}\!\!\mapsto\ottkw{alloc'}~(\ottsctor{\lambda}[[d]]\,_{\mymul{[[¹]]}}\!\!\mapsto[[dd ˢ⨞ d]]))$ is well typed, as the inner call to $\ottkw{alloc'}$ returns a value of type $[[①]]$ (as $[[d]]$ is of type $\ottstype{\lfloor[[①]]\rfloor}$) and consumes $[[d]]$ linearly. However, we see that because $[[-h]]$ escaped to the parent scope by being stored in a destination of destination coming from the parent scope, the hole $[[h]]$ has not been written to, and thus the inner expression $\ottkw{withTmpStore}~\{ [[h]] \assigneq \holesq \}~\ottkw{do}~\{ \ottkw{deref}~[[-h]] \}$ cannot reduce in a meaningful way.

\improvement{I remember that we did wonder about that at some point, so it's worth preempting the question. Though I'd rather we found a slightly different example that doesn't exploit the return type of fill than a paragraph recounting our life story.

alloc \& dd -> alloc \& d -> dd <| d
alloc \& dd -> dd <| (,) \& case (dd1, dd2) -> (alloc \& d -> dd1 <| d) \& case { Inl () -> dd2 <| () , Inr () -> dd2 <| () }
dd : ||1+1| x 1|
dd1 : ||1+1||
dd2 : |1|
d : |1+1|
}
One could argue that the issue comes from the destination-filling primitive $\triangleleft$ returning unit instead of a special value of a distinct \emph{effect} type. However, the same issue arise if we introduce a distinct type $\ottstype{\lfloor\!\!\rfloor}$ for the effect of filling a destination ; there is always a way to cheat the system and make a destination escape to a parent scope. This distinct type for effects has in fact existed during the early prototypes of destination calculus, but we removed it as it doesn't solve the scope escape for destination and is indistinguishable in practice from the unit type.

\subsection{Age control to prevent scope escape of destinations}

\unsure{I don't think this is the right place for this section. It should appear in the formal system I think.}

The solution we chose is to instead track the age of destinations (as De-Brujin-like scope indices), and prevent a destination to escape into the parent scope when stored through age-control restriction on the typing rule of destination-filling primitives.

Age is represented by a commutative semiring, where $[[ν]]$ indicates that a destination originates from the current scope, and $[[↑]]$ indicates that it originates from the scope just before. We also extend ages to variables (a variable of age $[[a]]$ stands for a value of age $[[a]]$). Finally, age $[[∞]]$ is introduced for variables standing in place of a non-age-controlled value. In particular, destinations can never have age $[[∞]]$ in practice.

Semiring addition $\ottsmode{+}$ is used to find the age of a variable or destination that is used in two different branches of a program. Semiring multiplication $[[·]]$ corresponds to age composition, and is in fact an integer sum on scope indices.
$[[∞]]$ is absorbing for both addition and multiplication.

Tables for the operations $\ottsmode{+}$ and $\ottsmode{·}$ on ages are presented in Figure~\ref{fig:age-tables}.

\begin{figure}[h]\centering
We pose $[[↑]]^{0} = [[ν]]$ and $[[↑]]^{n} = [[↑]][[·]][[↑]]^{n-1}$

\medskip

\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{|c|c|c|}\hline
$\ottsmode{+}$ & $[[↑]]^{n}$               & $[[∞]]$ \\\hline
$[[↑]]^{m}$    & $\text{if }n = m\text{ then }[[↑]]^{n}\text{ else }[[∞]]$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$                   & $[[∞]]$ \\\hline
\end{tabular}
\end{minipage}\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{|c|c|c|}\hline
$[[·]]$        & $[[↑]]^{n}$               & $[[∞]]$ \\\hline
$[[↑]]^{m}$    & $[[↑]]^{n+m}$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$                   & $[[∞]]$ \\\hline
\end{tabular}
\end{minipage}

\caption{Tables for age operations}\label{fig:age-tables}
\end{figure}

Age commutative semiring is then combined with the multiplicity commutative semiring from~\cite{bernardy_linear_2018} to form a canonical product commutative semiring that is used to represent the mode of each typing context binding in our final type system.

The main restriction to prevent parent scope escape is materialized the simplified typing rules of Figure~\ref{fig:age-control-simpl}.

\begin{ottfig}{\caption{Simplified typing rules for age control of destinations}\label{fig:age-control-simpl}}
\ottusedrule{%
\ottdrule{%
}{
\ottshname{\destminus} \ottshname{h} :\!_{\!  \ottsmode{\nu}   }\ottstype{\lfloor}\ottstype{T} \ottstype{\rfloor}   \,\vdash\,   \ottshname{\destminus} \ottshname{h}   \ottsym{:}   \ottstype{\lfloor}\ottstype{T} \ottstype{\rfloor} }{%
{\ottdrulename{Ty\_val\_Dest$^{\star}$}}{}%
}}
\ottusedrule{%
\ottdrule{%
\ottpremise{\Theta_{{\mathrm{1}}}  \,\vdash\,  \ottnt{t}  \ottsym{:}   \ottstype{\lfloor} \ottstype{T} \ottstype{\rfloor} }%
\ottpremise{\Theta_{{\mathrm{2}}}  \,\vdash\,  \ottnt{t'}  \ottsym{:}  \ottstype{T}}%
}{
\Theta_{{\mathrm{1}}}  +  \ottsmode{\uparrow} \ottsmode{\hspace{-0.1ex}\cdot\hspace{-0.1ex} }  \Theta_{{\mathrm{2}}} \,\vdash\,   \ottnt{t} \triangleleft\!\, \ottnt{t'}   \ottsym{:}   \ottstype{1} }{%
{\ottdrulename{Ty\_term\_FillLeaf$^{\star}$}}{}%
}}
\end{ottfig}

Typing a destination $[[-h]]$ alone requires $[[-h]]$ to have age $[[ν]]$ in the context. And when storing a value through a destination, the ages of the value's dependencies in the context must be one higher than the corresponding ages required to type the value alone (this is the meaning of $\ottsmode{\uparrow} \ottsmode{\hspace{-0.1ex}\cdot\hspace{-0.1ex} }  \Theta_{{\mathrm{2}}}$).

Such a rule system prevents in particular the previous faulty expression $[[-hd ˢ⨞ -h]]$ where $[[-hd]]$ originates from the context parent to the one of $[[-h]]$.

\section{Breadth-first tree traversal}
\label{sec:bft}

The core example that showcases the power of destination-passing style programming with first-class destination is breadth-first tree traversal:

\begin{quote}
Given a tree, create a new one of the same shape, but with the values at the nodes replaced by the numbers $1\ldots|T|$ in breadth-first order.
\end{quote}

Indeed, breadth-first traversal implies that the order in which the structure must be populated (left-to-right, top-to-bottom) is not the same as the structural order of a functional binary tree i.e., building the leaves first and going up to the root.

In~\cite{bagrel_destination-passing_2024}, the author presents a breadth-first traversal implementation that relies on first-class destinations so as to build the final tree in a single pass over the input tree. Their implementation, much like ours, uses a queue to store pairs of an input subtree and a destination to the corresponding output subtree. This queue is what materialize the breadth-first processing order: the leading pair $(\langle\text{\itshape input subtree}\rangle, \langle\text{\itshape dest to output subtree}\rangle)$ of the queue is processed, and its children pairs are added back at the end of the queue to be processed later.

\improvement{Rework the next couple of paragraph to flow a little bit better.}
However, as evoked earlier in Section~\ref{ssec:problem-stored-dests}, The API presented in~\cite{bagrel_destination-passing_2024} is not able to store linear data, and in particular destinations, in destination-based data structures. It is thus reliant on regular constructor-based Haskell data structures for destination storage.

This is quite impractical as we would like to use the efficient, destination-based queue implementation from Section~\ref{ssec:efficient-queue} to power up the breadth-first tree traversal implementation\footnote{This efficient queue implementation can be, and is in fact, implemented in~\cite{bagrel_destination-passing_2024}: see \url{archive.softwareheritage.org/swh:1:cnt:29e9d1fd48d94fa8503023bee0d607d281f512f8}. But it cannot store linear data}. In our present work fortunately, thanks to the finer age-control mechanism, we can store linear resources in destination-based structures without any issue. Our system is in fact self-contained, as any structure, whatever the use for it be, can be built using a small core of destination-based primitives (and regular data constructors can be retrieved from destination-based primitives, see Section~\ref{ssec:sugar}).

Figure~\ref{fig:impl-boilerplate-bfs} introduces a few extra tools needed for implementation of breadth-first tree traversal, while Figure~\ref{fig:impl-bfs} presents the actual implementation of the traversal. This implementation is as similar as possible to the one from~\cite{bagrel_destination-passing_2024}, as to make it easier to spot the few differences between the two systems.\unsure{There's way too many mode annotations here. If we are to do that we have to dedicate some space above to explaining the modes and what all the constructions around them mean.}

\begin{codefig}{\caption{Boilerplate for breadth-first tree traversal}\label{fig:impl-boilerplate-bfs}}
\begin{minipage}[t]{0.68\textwidth}\vspace{-0.15cm}$
\newtype{[[Tree T]]}{[[① ⨁ (T ⨂ ((Tree T) ⨂ (Tree T)))]]}
\newoperator
  {\triangleleft\ottsctor{Nil}}{[[⌊ Tree T ⌋ n ¹ν → ①]]}
  {[[d ⨞ Nil]]}{[[d ⨞Inl ⨞()]]}
\newoperator
  {\triangleleft\ottsctor{Node}}{[[⌊ Tree T ⌋ n ¹ν → ⌊T⌋ n ⨂ (⌊ Tree T⌋ n ⨂ ⌊ Tree T⌋ n)]]}
  {[[d ⨞ Node]]}{[[(d ⨞Inr ⨞(,)) case ¹ν (dv, dtlr) ⟼ ˢ(dv, dtlr ⨞(,))]]}$\end{minipage}
\vrule height 0.02\textheight\hspace{0.5cm}
\begin{minipage}[t]{0.31\textwidth}$
\newtype[~\mathttbf{rec}]{[[Nat]]}{[[① ⨁ Nat]]}
\newoperator
  {\ottkw{zero}}{[[Nat]]}
  {[[zero]]}{[[Inl ()]]}
\newoperator
  {\ottkw{succ}}{[[Nat ¹ν → Nat]]}
  {[[succ x]]}{[[ˢInr x]]}$\end{minipage}
\end{codefig}

\begin{codefig}{\caption{Breadth-first tree traversal in destination-passing style}\label{fig:impl-bfs}}
\newoperatorb[~\mathttbf{rec}]
{\ottkw{go}}{[[((! ¹∞ S) ¹ν → T1 ¹ν → (! ¹∞ S) ⨂ T2) ων→ (! ¹∞ S) ¹ν → Queue (Tree T1 ⨂ ⌊ Tree T2 ⌋ ¹ν) ¹ν → (! ¹∞ S)]]}
{[[go f st q]]}{\!\!\!\begin{array}[t]{l}[[
(dequeue q) case ¹ν {⮒
‥‥Inl () ⟼ st,⮒
‥‥ˢInr ˢ(ˢ(tree, dtree), q') ⟼ tree case ¹ν {⮒
‥‥‥‥Inl () ⟼ dtree ⨞ Nil ; go f st q',⮒
‥‥‥‥ˢInr ˢ(x, ˢ(tl, tr)) ⟼ (dtree ⨞ Node) case ¹ν⮒
‥‥‥‥‥‥ˢ(dy, ˢ(dtl, dtr)) ⟼ (༼f st༽ x) case ¹ν⮒
‥‥‥‥‥‥‥‥(st', y) ⟼ dy ˢ⨞ y ; go f st' (⮒
‥‥‥‥‥‥‥‥‥‥q' enqueue ˢ(tl, dtl) enqueue ˢ(tr, dtr)⮒
‥‥‥‥‥‥‥‥)⮒
‥‥}⮒
}
]]\end{array}}
\newoperatorb
{\ottkw{mapAccumBFS}}{[[((! ¹∞ S) ¹ν → T1 ¹ν → (! ¹∞ S) ⨂ T2) ων→ (! ¹∞ S) ¹ν → Tree T1 ¹ν → Tree T2 ⨂ (! ¹∞ S)]]}
{[[mapAccumBFS f st tree]]}{\!\!\!\begin{array}[t]{l}[[
from⧔ (alloc map dtree ⟼ go f st (singleton ˢ(tree, dtree)))
]]\end{array}}
\newoperatorb
{\ottkw{relabelDPS}}{[[Tree ① ¹ν → (Tree Nat) ⨂ (! ¹∞ (! ων Nat))]]}
{[[relabelDPS tree]]}{\!\!\!\begin{array}[t]{l}[[
mapAccumBFS⮒
‥‥(ˢλ ex ¹ν ⟼ ˢλ un ¹ν ⟼ un ; ex case ¹ν⮒
‥‥‥‥ᴇ ¹∞ ex' ⟼ ex' case ¹∞⮒
‥‥‥‥‥‥ᴇ ων st ⟼ ˢ(ˢᴇ ¹∞ (ˢᴇ ων (succ st)), st))⮒
‥‥(ˢᴇ ¹∞ (ˢᴇ ων (succ zero)))⮒
‥‥tree
]]\end{array}}
\end{codefig}

The first important difference is that in the destination calculus implementation, the input tree of type $[[Tree T1]]$ is consumed linearly. The stateful transformer that is applied to each input node to get the output node value is also linear in its two arguments. The state has to be wrapped in an exponential $\ottstype{!_{[[¹∞]]}}$ so that it can be extracted from the right side of the ampar at the end of the processing (with $\ottkw{from}_{\ottstype{\ltimes}}$). We could imagine a more general version of the traversal, having no constraint on the state type, but necessitating a finalization function $[[S ¹ν → ! ¹∞ S']]$ so that the final state can be returned.

The $\ottkw{go}$ function is in charge of consuming the queue containing the pairs of input subtrees and destinations to the corresponding output subtrees. It dequeues the first pair, and processes it. If the input subtree is $\ottsctor{Nil}$, it feeds $\ottsctor{Nil}$ to the destination for the output tree and continues the processing of next elements with unchanged state. If the input subtree is a node, it writes a hollow $\ottsctor{Node}$ constructor to the hole pointed to by the destination, processes the value of the node with the stateful transformer $[[f]]$, and continues the processing of the updated queue where children subtrees and their accompanying destinations have been enqueued.

$\ottkw{mapAccumBFS}$ is in charge of spawning the initial memory slot for the output tree together with the associated destination, and preparing the initial queue containing a single pair, made of the whole input tree and the aforementioned destination.

$\ottkw{relabelDPS}$ is a special case of $\ottkw{mapAccumBFS}$ that takes the skeleton of a tree (where node values are all unit) and returns a tree of integers, with the same skeleton, but with node values replaced by naturals $1\ldots|T|$ in breadth-first order. The higher-order function passed to $\ottkw{mapAccumBFS}$ is quite verbose: it must consume the previous node value (unit), using $\fatsemi$, then extract the state (representing the next natural number to attribute to a node) from its two nested exponential constructors, and finally return a pair, whose left side is the incremented natural wrapped back into its two exponential layers (new state), and whose right side is the plain natural representing the node value for the output subtree.

Two exponentials are needed here. The first one $\ottstype{!}_{[[¹∞]]}$ is part of $\ottkw{from}_{\ottstype{\ltimes}}$ contract, and ensures that the state cannot capture destinations, so that it can be returned at the end of the processing (as $\ottkw{go}$ runs under a $\ottkw{map}$ over an ampar). The second one $\ottstype{!}_{[[ων]]}$ allows the natural number to be used in a non-linear fashion: it is used once in $[[succ st]]$ to serve as the new state, and another time as the value for the output node. With a more general\unsure{To Thomas: didn't you tell me you had plan of making something more general for this very case? I don't remember clearly. If so, don't forget to improve here} $\ottkw{from}_{\ottstype{\ltimes}}$ operator, we would be able to use just one exponential layer $\ottstype{!}_{[[ω∞]]}$ over the natural number to achieve the same result.\unsure{I'm [Arnaud] honestly uncomfortable at the quantity of mathematics written in this section without macros and completely uncorrelated from OTT. If we change any notation, we'll have to trawl through a lot of text to make sure they are properly propagated.}

\section{Language syntax}

\subsection{Introducing the \emph{ampar}}\label{ssec:ampar-motivation}

Minamide's work\cite{minamide_functional_1998} is the earliest record we could find of a functional calculus integrating the idea of incomplete data structures (structures with holes) that exist as first class values and can be interacted with by the user.

In that paper, a structure with a hole is named \emph{hole abstraction}. In the body of a hole abstraction, the bound \emph{hole variable} should be used linearly (exactly once), and must only be used as a parameter of a data constructor. In other terms, the bound \emph{hole variable} cannot be pattern-matched on or used as a parameter of a function call. A hole abstraction is thus a weak form of linear lambda abstraction, which just moves a piece of data into a bigger data structure.

In fact, the type of hole abstraction $\ottstype{([[T1]], [[T2]]) hfun}$ in Minamine's work shares a lot of similarity with the separating implication or \emph{magic wand} $\ottstype{[[T1]] \sepimp [[T2]]}$ from separation logic: given a piece of memory matching description $[[T1]]$, we obtain a (complete) piece of memory matching description $[[T2]]$.

Now, in classical linear logic, we know we can transform linear implication $\ottstype{[[T1]] \multimap [[T2]]}$ into $\ottstype{[[T1]]^{\perp}~\parr~[[T2]]}$. Doing so for the \emph{wand} type $\ottstype{([[T1]], [[T2]]) hfun}$ or $\ottstype{[[T1]] \sepimp [[T2]]}$ gives $\ottstype{\lfloor[[T1]]\rfloor~ \widehat{\parr}~[[T2]]}$, where $\ottstype{\lfloor\smallbullet\rfloor}$ is memory negation, and $\ottstype{\widehat{\parr}}$ is a memory \emph{par} (weaker than the CLL \emph{par} that allows more ``interaction'' of its two sides).

Transforming the hole abstraction from its original implication form to a \emph{par} form let us consider the type $\ottstype{\lfloor[[T1]]\rfloor}$ of \emph{sink} or \emph{destination} of $[[T1]]$ as a first class component of our calculus. We also get to see the hole abstraction aka memory par as a pair-like structure, where the two sides might be coupled together in a way that prevent using both of them simultaneously.

\paragraph{From memory par $\ottstype{\widehat{\parr}}$ to ampar $\ottstype{\ltimes}$}

In CLL, the cut rule states that given $\ottstype{[[T1]]~\parr~[[T2]]}$, we can free up $[[T1]]$ by providing an eliminator of $[[T2]]$, or free up $[[T2]]$ by providing an eliminator of $[[T1]]$. The eliminator of $[[T]]$ can be $\ottstype{[[T]]^\perp}$, or $\ottstype{[[T]]^{\perp^{-1}}} = [[T']]$ if $[[T]]$ is already of the form $\ottstype{[[T']]^\perp}$. In a classical setting, thanks to the involutive nature of negation $\ottstype{\smallbullet^\perp}$, the two potential forms of the eliminator of $[[T]]$ are equal.

In destination calculus though, we don't have an involutive memory negation $\ottstype{\lfloor\smallbullet\rfloor}$. If we are provided with a destination of destination $[[-h']] \pmb{:} \ottstype{\lfloor\lfloor[[T]]\rfloor\rfloor}$, we know that some structure is expecting to store a destination of type $\ottstype{\lfloor[[T]]\rfloor}$. If ever that structure is consumed, then the destination stored inside will have to be fed with a value (remember we are in a linear calculus). So if we allocate a new memory slot of type $[[+h]] \pmb{:} [[T]]$ and its linked destination $[[-h]] \pmb{:} \ottstype{\lfloor[[T]]\rfloor}$, and write $[[-h]]$ to the memory slot pointed to by $[[-h']]$, then we can get back a value of type $[[T]]$ at $[[+h]]$ if ever the structure pointed to by $[[-h']]$ is consumed. Thus, a destination of destination is only equivalent to the promise of an eventual value, not an immediate usable one.

As a result, in destination calculus, we cannot have the same kind of cut rule as in CLL. This is, in fact, the part of destination calculus that was the hardest to design, and the source of a lot of early errors. For a destination of type $\ottstype{\lfloor[[T]]\rfloor}$, both storing it through a destination of destination $\ottstype{\lfloor\lfloor[[T]]\rfloor\rfloor}$ or using it to store a value of type $[[T]]$ constitute a linear use of the destination. But only the latter is a genuine consumption in the sense that it guarantees that the hole associated to the destination has been written to! Storing away the destination of type $\ottstype{\lfloor[[T]]\rfloor}$ originating from $\ottstype{[[T]]~\widehat{\parr}~\lfloor[[T]]\rfloor}$ (through a destination of destination of type $\ottstype{\lfloor\lfloor[[T]]\rfloor\rfloor}$) should not allow to free up the $[[T]]$, as it would in a CLL-like setting.

However, we can recover a memory abstraction that is usable in practice if we know the nature of an memory par side:
\begin{itemize}
\item if the memory par side is a value made only of inert elements and destinations (negative polarity), then we can pattern-match/$\ottkw{map}$ on it, but we cannot store it away to free up the other side;
\item if the memory par side is a value made only of inert elements and holes (positive polarity), then we can store it away in a bigger struct and free up the associated destinations (this is not an issue as the bigger struct will be locked by an memory par too), but we cannot pattern-match/$\ottkw{map}$ on it as it (may) contains holes;
\item if one memory par side is only made of inert elements, we can in fact convert the memory par to a pair, as the memory par doesn't have any form of interaction between its sides.
\end{itemize}

It is important to note that the type of an memory par side is not really enough to determine the nature of the side, as a hole of type $[[T]]$ and and inert value of type $[[T]]$ are indistinguishable at the type level.

So we introduced a more restricted form of memory par, named \emph{ampar} ($\ottstype{\ltimes}$), for \emph{asymmetrical memory par}, in which:
\begin{itemize}
\item the left side is made of inert elements (normal values or destinations from previous scopes) and/or holes if and only if those holes are compensated by destinations on the right side;
\item the right side is made of inert elements and/or destinations.
\end{itemize}

As the right side cannot contain any holes, it is always safe to pattern-match or $\ottkw{map}$ on it. Because the left side cannot contain destinations from the current scope, it is always safe to store it away in a bigger struct and release the right side.

Finally, it is enough to check for the absence of destinations in the right side (which we can do easily just by looking at its type) to convert an \emph{ampar} to a pair, as any remaining hole on the left side would be compensated by a destination on the right side.

\paragraph{Destinations from previous scopes are inert}

In destination calculus, scopes are delimited by the $\ottkw{map}$ operation over ampars. Anytime a $\ottkw{map}$ happens, we enter a new scope, and any preexisting destination or variable see its age increased by one ($\ottsmode{[[↑]]}$). As soon as a destination or variable is no longer of age 0 ($\ottsmode{[[ν]]}$), it cannot be used actively but only passively (e.g. it cannot be applied if it is a function, or used to store a value if it is a destination, but it can be stored away in a dest, or pattern-matched on).

This is a core feature of the language that ensures part of its safety.

\subsection{Names and variables}

The destination calculus uses two classes of names: regular variable names $[[x]], [[y]]$, and \emph{hnames} $[[h]], [[h1]], [[h2]]$ which are identifiers for a memory cell that hasn't been written to yet, as illustrated in Figure~\ref{fig:grammar-names}.

Hole names are represented by natural numbers under the hood, so they can act both as relative offsets or absolute positions in memory. Typically, when a structure is effectively allocated, its hole (and destination) names are shifted by the maximum hname encountered so far in the program ; this corresponds to finding the next unused memory cell in which to write new data.

We sometimes need to keep track of hnames bound by a particular runtime value or evaluation context, hence we also define sets of hnames $[[H]], [[H1]], [[H2]]\ldots$.

Shifting all hnames in a set by a given offset $[[h']]$ is denoted $[[H ⩲ h']]$. We also define a conditional shift operation $\ottshname{[}[[H ⩲ h']]\ottshname{]}$ which shifts each hname appearing in the operand to the left of the brackets by $[[h']]$ if this hname is also member of $[[H]]$. This conditional shift can be used on a single hname, a value, or a typing context.

\begin{ottfig}{\caption{Grammar for variable, hole and destination names}\label{fig:grammar-names}}
\ottmetavars
\ottgrammartabular{
\otthname\ottinterrule
\otthnames\ottafterlastrule
}
\end{ottfig}

\subsection{Term and value core syntax}\label{ssec:core-syntax}

Destination calculus is based on linear simply-typed $\lambda$-calculus, with built-in support for sums, pairs, and exponentials. The syntax of terms, which is presented in Figure~\ref{fig:grammar-term-val} is quite unusual, as we need to introduce all the tooling required to manipulate destinations, which constitute the primitive way of building a data structures for the user.

\begin{ottfig}{\caption{Grammar for terms and values}\label{fig:grammar-term-val}}
\ottgrammartabular{
\ottterm\ottinterrule
\ottval\ottafterlastrule
}
\end{ottfig}

In fact, the grammatical class of values $[[v]]$, presented as a subset of terms $[[t]]$, could almost be removed completely from the user syntax, and just used as a denotation for runtime data structures. We only need to keep the \emph{ampar} value $[[{ h } ⟨ +h ❟ -h ⟩]]$ as part of the user syntax as a way to spawn a fresh memory cell to be later filled using destination-filling primitives (see $[[alloc]]$ in Section~\ref{ssec:sugar}).

Pattern-matching on every type of structure (except unit) is parametrized by a mode $[[m]]$ to which the scrutinee is consumed. The variables which bind the subcomponents of the scrutinee then inherit this mode. In particular, this choice crystalize the equivalence $[[! ωa (T1 ⨂ T2)]] \simeq [[(! ωa T1) ⨂ (! ωa T2)]]$, which is not part of intuitionistic linear logic, but valid in Linear Haskell\cite{bernardy_linear_2018}. We omit the mode annotation on $\ottkw{case}$ statements and lambda abstractions when the mode in question is the multiplicative neutral element $[[¹ν]]$ of the mode semiring.

$\ottkw{map}$ is the main primitive to operate on an \emph{ampar}, which represents an incomplete data structure whose building is in progress. $\ottkw{map}$ binds the right-hand side of the \emph{ampar} --- the one containing destinations of that \emph{ampar} --- to a variable, allowing those destinations to be operated on by destination-filling primitives. The left-hand side of the \emph{ampar} is inaccessible as it is being mutated behind the scenes by the destination-filling primitives.

$\ottkw{to}_{\ottstype{\ltimes}}$ embeds an already completed structure in an \emph{ampar} whose left side is the structure, and right side is unit. We have an operator \textopname{fillComp} ($\triangleleft\mybullet$) allowing to compose two \emph{ampar}s by writing the root of the second one to a destination of the first one, so by throwing $\ottkw{to}_{\ottstype{\ltimes}}$ to the mix, we can compose an \emph{ampar} with a normal (completed) structure (see the sugar operator \textopname{fillLeaf} ($\triangleleft$) in Section~\ref{ssec:sugar}).

$\ottkw{from}_{\ottstype{\ltimes}}$ is used to convert an \emph{ampar} to a pair, when the right side of the \emph{ampar} is an exponential of the form $[[ᴇ ¹∞ v]]$. Indeed, when the right side has such form, it cannot contains destinations (as destinations always have a finite age), thus it cannot contain holes in its left side either (as holes on the left side are always compensated 1:1 by a destination on the right side). As a result, it is valid to convert an \emph{ampar} to a pair in these circumstances. $\ottkw{from}_{\ottstype{\ltimes}}$ is in particular used to extract a structure from its \emph{ampar} building shell when it is complete (see the sugar operator $\ottkw{from'}_{\ottstype{\ltimes}}$ in Section~\ref{ssec:sugar}).

The remaining term operators $[[⨞]][[()]], [[⨞]][[Inl]], [[⨞]][[Inr]], [[⨞]]\,\expcons{[[m]]}, [[⨞]][[(,)]], [[⨞]](\lamnt{[[x]]}{[[m]]}{[[u]]})$ are all destination-filling primitives. They write a layer of value/constructor to the hole pointed by the destination operand, and return the potential new destinations that are created in the process (or unit if there is none).

\paragraph{Values} There are two important things to note on the value class.

First, a variable cannot contains any free variable. This will be better visible in the typing rule for function value form \textsc{Ty-val-Fun}, but a value only admits hole and destination type bindings in its typing context, no variable binding. This is quite useful for substitution lemmas, as no undesired capture can happen.

Secondly, values are allowed to have holes inside (represented by $[[+h]]$, $[[+h1]]$, $[[+h2]]$\ldots), but a value used as a term isn't allowed to have any free hole (i.e. a hole that is not compensated by an associated destination inside an ampar). This is enforced by the typing context $[[D]]$ meaning ``destination-only'' in the rule \textsc{Ty-term-Val}.

\subsection{Syntactic sugar for constructors and commonly used operations}\label{ssec:sugar}

As we said in section~\ref{ssec:core-syntax}, the grammatical class of values is mostly used for runtime only; in particular, data constructors in the value class can only take other values as arguments, not terms (this help us ensure that no free variable can appear in a value). Thus we introduce syntactic for data constructors taking arbitrary terms as parameters (as we often find in functional programming languages) using destination-filling primitives in Figure~\ref{fig:grammar-sterm}.

\begin{ottfig}{\caption{Syntactic sugar forms for terms}\label{fig:grammar-sterm}}
\ottgrammartabular{
\ottsterm\ottafterlastrule
}
\end{ottfig}

$\ottkw{from}_{\ottstype{\ltimes}'}$ is a simpler variant of $\ottkw{from}_{\ottstype{\ltimes}}$ that allows to extract the right side of an ampar when the right side has been fully consumed. We implement it in terms of $\ottkw{from}_{\ottstype{\ltimes}}$ to keep the core calculus tidier (and limit the number of typing rules, evaluation contexts, etc), but it can be implemented much more efficiently in a real-world implementation.

All the desugarings are presented in Figure~\ref{fig:desugaring}.

\begin{codefig}{\caption{Desugaring of syntactic sugar forms for terms}\label{fig:desugaring}}
\begin{minipage}[t]{0.62\textwidth}$
[[alloc]] \btriangleq \!\!\!\begin{array}[t]{l}[[
{ 1 } ⟨ +1 ❟ -1 ⟩
]]\end{array}\\[\interdefskip]
[[from⧔' t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
(from⧔ (t map un ⟼ un ; ᴇ ¹∞ () )) case ¹ν⮒
‥‥( st , ex ) ⟼ ex case ¹ν⮒
‥‥‥‥ᴇ ¹∞ un ⟼ un ; st
]]\end{array}\\[\interdefskip]
[[ˢλ x m ⟼ u]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ ( λ x m ⟼ u )⮒
)
]]\end{array}\\[\interdefskip]
[[ˢ( t1 , t2 )]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥(d ⨞ (,)) case ¹ν⮒
‥‥‥‥‥‥( d1 , d2 ) ⟼ d1 ˢ⨞ t1 ; d2 ˢ⨞ t2⮒
)
]]\end{array}$\end{minipage}
\vrule height 0.01\textheight\hspace{0.5cm}
\begin{minipage}[t]{0.38\textwidth}$
[[t ˢ⨞ t']] \btriangleq \!\!\!\begin{array}[t]{l}[[
t ⨞· (to⧔ t')
]]\end{array}\\[\interdefskip]
[[ˢInl t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ Inl ˢ⨞ t⮒
)
]]\end{array}\\[\interdefskip]
[[ˢInr t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ Inr ˢ⨞ t⮒
)
]]\end{array}\\[\interdefskip]
[[ˢᴇ m t]] \btriangleq \!\!\!\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ ᴇ m ˢ⨞ t⮒
)
]]\end{array}$\end{minipage}
\end{codefig}

\section{Type system}
\label{sec:type-system}

\subsection{Types, modes, and typing contexts}

\begin{ottfig}{\caption{Types, modes, and typing contexts}\label{fig:grammar-type-mode-ctx}}
\ottgrammartabular{
\otttype\ottinterrule
\ottmode\ottinterrule
\ottmul\ottinterrule
\ottage\ottinterrule
\ottctx\ottafterlastrule
}
\end{ottfig}

\TODO{2 paragraphes pour décrire le système : typage linéaire comonadique pour les modalités blabla}

The type system of \destcalculus is highly inspired from Linear Haskell~\cite{bernardy_linear_2018}. In particular, it uses the same additive/multiplicative approach on mode as Linear Haskell for linearity enforcement, except here we use a product semiring for mode that keeps track of both age and multiplicity of variables (whereas only multiplicity is tracked in the former).

The grammar of types, modes, and typing context for \destcalculus is presented in Figure~\ref{fig:grammar-type-mode-ctx}. It is based on intuitionistic linear logic, and thus provides multiplicative conjunction $\ottstype{\otimes}$ (product type) and additive disjunction (sum type) $\ottstype{\oplus}$. It also provides a function arrow $_{\myfuntm{[[m]]} }\!\ottstype{\to}$ and exponential connective $\ottstype{!}_{[[m]]}$ that are both parametrized by a mode $[[m]]$. We omit the mode annotation on the function arrow, as well as on the destination type, when the mode in question is the multiplicative neutral element $[[¹ν]]$ of the semiring (in particular, a function arrow without annotation is linear by default). A function arrow with multiplicity $[[¹]]$ is equivalent to the linear arrow $\ottstype{\multimap}$ from~\cite{girard_linear_1995}.

A mode is either a pair of a multiplicity and an age, or the special symbol $[[☠]]$\critical{remove mention of skull ?} representing an unsatisfiable requirement for a variable in a typing context (e.g. a variable that is used with different types in two branches of a $\ottkw{case}$ statement on a sum type). This special mode ``invalid'' $[[☠]]$ is only used in the formal proof of type safety of \destcalculus, when summing typing contexts. However, every mode $[[m]]$ present in the typing rules of this paper is assumed to be valid (i.e. made of a pair of a multiplicity and an age), and every typing context in a rule premises is assumed to contain only valid modes too.

\subsection{Typing of terms and values}

\begin{ottfig}{\caption{Typing rules for values and terms}\label{fig:ty-val-term}}
\ottdefnTyXXval{}
\ottdefnTyXXterm{}
\end{ottfig}

\TODO{Replacer turnstile s par une double barre ou équivalent}
\begin{ottfig}{\caption{Derived typing rules for syntactic sugar forms}\label{fig:ty-sterm}}
  \ottdefnTyXXsterm{}
\end{ottfig}

Figure~\ref{fig:ty-val-term} presents the typing rules for values and terms. Figure~\ref{fig:ty-sterm} presents the typing rules for the syntactic sugar forms, that have been derived from primitive rules of Figure~\ref{fig:ty-val-term} and proven formally too.

In every figure,
\begin{itemize}
  \item $[[O]]$ denotes an arbitrary typing context, with no particular constraint;
  \item $[[G]]$ denotes a typing context made only of hole and destination bindings;
  \item $[[P]]$ denotes a typing context made only of destination and variable bindings;
  \item $[[D]]$ denotes a typing context made only of destination bindings.
\end{itemize}

Destinations and holes are two faces of the same coin, as seen in Section~\ref{sec:build-up-vocab}, and must always be in 1:1 correspondance. Thus, the core idea of the type system is to add hole bindings $[[{ + h : T n }]]$ (hname in positive polarity) and destination bindings $[[{ - h : m ⌊ T ⌋ n }]]$ (hname in negative polarity), in addition to the variable bindings $[[{ x : m T}]]$ that usually populates typing contexts. Hole bindings and destination bindings of the same hname are meant to compensate each other in the typing context, a bit like how matter (positive polarity, hole) and antimatter (negative polarity, destination) annihilate each other. That way, the typing context of a term can stay constant during reduction, even when destination-filling primitives are evaluated to build up data structures, as those linearly consume a destination and write to a hole at the same time which makes both disappear.

\TODO{Evacuer tout de suite le double mode des dests}

However, that annihilation between a destination and a hole binding having the same hname in the typing tree is only allowed to happen around an ampar, as it is the ampar connective that bind the two polarities of a name together (the names bound are actually stored in a set $[[H]]$ on the ampar value $[[H ⟨ v2 ❟ v1 ⟩]]$). In fact, an ampar can be seen as a sort of lambda-abstraction, whose body (containing holes instead of variables) and sink/input site are split on two sides, and magically interconnected through the ampar connective.

Thus, the sum of typing contexts\unsure{Here, just speak how Ty-val-Prod and Ty-val-Ampar are different. In Coq, a context where both hole/dest for same name is not representable}, used in almost all rules, results in an erroneous context when the input operands contains the same hname but in different polarities: $\{[[{ + h : T n }]]\} + \{[[{ - h : ¹ν ⌊ T ⌋ n }]]\} = \{[[{ + h : ① ☠ }]]\}$ (the fact that the result is a hole binding instead of a destination binding is purely arbitrary ; the only important bit here is to have an ``invalid'' mode on the result binding). An individual context is also not allowed to contain a same hname in two different bindings. Instead, in the only position where the annihilation is allowed to happen, that is to say the \textsc{Ty-val-Ampar} rule, we explicitly identify and remove the parts of input contexts that can interact and annihilate each other, in the form of $[[D]]$ in the right side context and $[[-⁻¹ D]]$ for the left one.

$\ottshname{\destminus^{\scriptscriptstyle\text{-}1} }$ is a point-wise operation on typing bindings of a context where:

\[\left\{\begin{array}{rcl}
  [[-⁻¹ ({ - h : ¹ν ⌊ T ⌋ n })]] &=& [[{ + h : T n }]] \\
  \ottshname{\destminus^{\scriptscriptstyle\text{-}1} }(\ottnt{n} :\!_{\![[m]]}[[T]]) &=& \ottnt{n} :\!_{\![[☠]]}[[①]] \quad\text{otherwise}
\end{array}\right.\]

Only an input context $[[D]]$ made only of destination bindings, with leftmost mode being $[[¹ν]]$, results in a valid output context, which is then only composed of hole bindings.

It might be time to discuss what modes mean for hole and destination bindings.

A hole binding $[[{ + h : T n }]]$ has only one mode, $[[n]]$, that indicates the mode a value must have to be written to it (that is to say, the mode of bindings that the value depends on to type correctly). To this day, the only way for a value to have a constraining mode is to capture a destination (otherwise the value has mode $[[ω∞]]$, meaning it can be used in any possible way), as destinations are the only intrinsically linear values in the calculus, but we will see in Section~\ref{sec:implementation} that other forms of intrinsic linearity can be added to the langage for practical reasons. We see the mode of a hole coming into play when a hole is located behind an exponential constructor: we should only write a non-linear value to the hole $[[+h]]$ in $[[ᴇ ων +h]]$. In particular, we should not store a destination into this hole, otherwise it could later be extracted and used in a non-linear fashion.

On the other hand, a destination binding $[[{ - h : m ⌊ T ⌋ n }]]$ has two modes. The left-most\critical{change to inner/outer one} one, $[[m]]$, tells how the destination can be used as a passive value, e.g. if the destination get stored in a destination of destination or passed as a function argument. The right-most one, $[[n]]$, corresponds to the mode of its associated hole, and thus how it behaves as an active entity, when filled with a value. We'll see in an instant how a binding can \emph{grow old}, but let's just remember for now that a destination can only be filled with a value when $\ottsmode{\mathsfbf{age}(}[[m]]\ottsmode{)} = [[ν]]$. As soon at it gets older (only the mode $[[m]]$ is affected, the mode $[[n]]$ of its associated hole never changes), it behaves like a passive normal value and can only be stored, passed around, or returned.

A closed term can never have a destination binding with left-most multiplicity $[[ω]]$ or age $[[∞]]$ somewhere in the typing tree. They can only be linear and have a finite age. Indeed, only \textsc{Ty-val-Ampar} (or more precisely, \textsc{Ty-ectxs-OpenAmpar-Foc}) adds new destination bindings to a typing context, and those initially have mode $[[¹ν]]$, and can only grow old later by increments of $[[↑]]$ (but can never reach age $[[∞]]$). We have formally proved that this property holds during execution. So any typing context of the form $[[{ - h : ωa ⌊ T ⌋ n },O]]$ or $[[{ - h : p∞ ⌊ T ⌋ n },O]]$ is never satisfiable.

\TODO{Parler des termes en premier !}
\TODO{Remplacer hnames(flèche-1) par hnames()}

\TODO{Dire ensuite : The system is much simpler for the user: no apparent dest or hole, just variables, normal constructors, alloc, etc}
\TODO{Donc pas grave si le système de typage pour les valeurs est plus difficile /moche}

\paragraph{Typing of values $[[⫦]]$}

Values type in a typing context $[[G]]$ made only of hole and destination bindings. The absence of free variables in values make it easier to prove substitution properties (as we will see in Section~\ref{ssec:sem}, we perform substitutions not only in terms, but also in evaluation contexts sometimes).

Rules \textsc{Ty-val-Hole} and \textsc{Ty-val-Dest} indicates that a hole or destination must have left-most mode $[[¹ν]]$ in the typing context to be used (except when a destination is stored away, as we will see later)\critical{revisit this if we allow weakening for dests}. Rules for unit, left and right variants, and product are straightforward.

Rule \textsc{Ty-val-Exp} is rather classic too: we multiply the dependencies $[[G]]$ of the value by the mode $[[n]]$ of the exponential. The intuition is that if $[[v]]$ uses a resource $[[v']]$ twice, then $\expcons{\ottsmode{2}}[[v]]$, that corresponds to two uses of $[[v]]$ (in a system with such a mode), will use $[[v']]$ four times. The mode product on context $[[n·G]]$ in the exponential rule has a somewhat different effect on hole and destination bindings:

\[\left\{\begin{array}{rcl}
  [[n·({ x : m T })]] &=& [[{ x : n · m T }]] \\
  [[n·({ + h : T n })]] &=& [[{ + h : T n · n' }]] \\
  [[n·({ - h : m ⌊ T ⌋ n' })]] &=& [[{ - h : n · m ⌊ T ⌋ n' }]]
\end{array}\right.\]

On hole bindings $[[{ + h : T n' }]]$, it affects the mode $[[n']]$ of the value the hole can receive, as we said earlier. On destination bindings $[[{ - h : m ⌊ T ⌋ n' }]]$, on the other hand, it affects the mode $[[m]]$ of the destination viewed as a value (the left-most one), but not the mode of the value it can be filled with.\unsure{Do we need more details on what are the consequences of that difference?}

Rule \textsc{Ty-val-Fun} indicates that (value level) lambda abstractions can only contain (captured) destinations, they cannot have holes inside. In other terms, a function value cannot be built piecemeal like other data structures, its whole body must be a complete term right from the beginning. It cannot contain free variables either, as the body of the function must type in context $[[D,{x:m T}]]$ where $[[D]]$ is made only of destination bindings. One might wonder, how can we represent a curryfied function $[[ˢλ x ¹ν ⟼ ˢλ y ¹ν ⟼ x concat y]]$ as the value level, as the inner abstraction captures the free variable $[[x]]$ ? The answer is that such a function, at value level, is encoded as $[[ᵛλ x ¹ν ⟼ from⧔' (alloc map d ⟼ d ⨞ ( λ y ¹ν ⟼ x concat y))]]$, where the inner closure is not yet in value form, but pending to be built into a value. As the form $[[d ⨞ ( λ y ¹ν ⟼ t)]]$ is part of term syntax, and not value syntax, we allow free variable captures in it.

The last important rule, \textsc{Ty-val-Ampar}, is probably the most complex one.
The left side $[[v2]]$ is the data structure under construction, that may contain holes (in $[[-⁻¹ D3]]$), but also stored destinations from other scopes (in $[[D2]]$). The right side $[[v1]]$ is an arbitrary value, that we can manipulate using $\ottkw{map}$, and that must contain all destinations matching the holes of $[[v2]]$ (these destinations are represented by $[[D3]]$). The right side $[[v1]]$ may also contain stored destinations from other scopes, in $[[¹↑·D1]]$ (in which case those cannot be filled in that scope, because they are at least of age $[[↑]]$, and thus can only be stored or moved around)\unsure{Not sure whether we should explain why $[[D1]]$ is offset by $[[¹↑]]$ in the premise but not in conclusion. The reason is "because it is needed" more than having a good intuition narrative behind it.}. The resulting typing context for the ampar is $[[D1,D2]]$; it doesn't mention $[[-⁻¹ D3]]$ and $[[D3]]$ anymore as they annihilate each other, as we explained previously. $[[D1]]$ and $[[D2]]$, by convention, are made only of destination bindings, and thus we know the annihilation (and thus binding) between holes and destinations for this ampar is indeed maximal. 

The properties $\mathtt{LinOnly}~[[D3]]$ and $\mathtt{FinAgeOnly}~[[D3]]$ are true given that $[[-⁻¹ D3]]$ is a valid typing context, so are not really a new restriction on $[[D3]]$. They are mostly used to ease the mechanical proof of type safety for the system.

\paragraph{Typing of terms $[[⊢]]$}

Terms type in a context that can only contain hole and destination bindings. Holes are only allowed in a value shelled by an ampar, so that the hole binding disappear from the typing context (using rule \textsc{Ty-val-Ampar} as we saw before).

The property $\mathtt{DisposableOnly}~[[P]]$ says that $[[P]]$ can only contain variable bindings with multiplicity $[[ω]]$, for which weakening is allowed in linear logic. Only the leaves of the typing tree need to be able to absorb unused disposable variable bindings, that is to say rules \textsc{Ty-term-Val} (it is a leaf for judgments $[[⊢]]$, that holds a subtree of judgments $[[⫦]]$) and \textsc{Ty-term-Var}.

Rule \textsc{Ty-term-Var}, in addition to weakening, allows for dereliction of the mode for the variable used, with subtyping constraint $[[¹ν <: m]]$ defined as such:

\newcommand{\pleq}{\stackrel{\pmb{\mathrm{p}}}{\mathtt{<:}}}
\newcommand{\aleq}{\stackrel{\pmb{\mathrm{a}}}{\mathtt{<:}}}

{\setlength{\arraycolsep}{\widthof{\,}}
\[\begin{stretchedarray}[3]{l}\left\{\begin{stretchedarray}[1]{rcl}
  [[p a]] &\mathtt{<:}& [[p' a']] \Longleftrightarrow [[p]] \pleq [[p']] \land~[[a]] \aleq [[a']] \\
  [[m]] &\mathtt{<:}& [[☠]]
\end{stretchedarray}\right.\\\left\{\begin{stretchedarray}[1]{rcl}
  [[¹]] &\pleq& [[¹]] \\
  [[p]] &\pleq& [[ω]]
\end{stretchedarray}\right.\\\left\{\begin{stretchedarray}[1]{rcl}
  [[↑]]^{m} &\aleq& [[↑]]^{n} \Longleftrightarrow m = n \quad \text{(no finite age dereliction ; recall that $[[↑]]^{0} = [[ν]]$)} \\
  [[a]] &\aleq& [[∞]]
\end{stretchedarray}\right.\end{stretchedarray}\]
}

Rule \textsc{Ty-term-PatU} is elimination (or pattern-matching) for unit, and is also used to chain destination-filling operations.

Rules \textsc{Ty-term-App}, \textsc{Ty-term-PatS}, \textsc{Ty-term-PatP} and \textsc{Ty-term-PatE} are all parametrized by a mode $[[m]]$ by which the typing context $[[P1]]$ of the argument is multiplied. These rules otherwise follows closely from~\cite{bernardy_linear_2018}.

The Rule \textsc{Ty-term-Map} is where most of the safety of the system lies. It opens an ampar, and binds its right side (containing destinations for holes on the other side, among other things) to variable $[[x]]$ and then execute body $[[t']]$. The core idea is that $\ottkw{map}$ creates a new scope for $[[x]]$ and $[[t']]$, so anything coming from the current ambiant scope (represented by $[[P2]]$ in the conclusion) appears older (by $[[↑]]$) when we see it from $[[t']]$ point of view. That way we can distinguish $[[x]]$ from anything else that was already bound using age of bindings alone. That's why $[[t']]$ types in $[[¹↑·P2,{ x : ¹ν T }]]$  while the global term $[[t map x ⟼ t']]$ types in $[[P1,P2]]$ (notice the absence of shift on $[[P2]]$). A schematic explanation of the scope rules is given in Figure~\ref{fig:scope-rules}.

\begin{figure}[h]
  \includegraphics[width=8cm]{schema_scopes.png}
  \caption{Scope rules for $\ottkw{map}$ in \destcalculus}
  \label{fig:scope-rules}
\end{figure}

We see that the left of an ampar (the structure being built) ``takes place'' in the ambiant scope. This is important because when using $\ottkw{from}_{\ottstype{\ltimes} }'$, the left of the ampar is extracted to the ambiant scope (as seen at the bottom of the figure with $[[x22]]$).

The right side however, where destinations are, has its own new, inner scope that is opened when mapped over\unsure{There is a subtle thing here; although we say that the right side of the ampar is in its own inner scope, it doesn't prevent the compensation with holes in the left side in rule \textsc{Ty-val-Ampar}}. When feeding a destination (e.g. $[[x1 ˢ⨞ x0]]$ in the figure), the right operand must be from a scope one $[[↑]]$ older than the destination on the left of the operator, as this value will end up on the left of the ampar (which is thus in a scope $[[↑]]$ older than the destination originating from the right side).

The rule \textsc{Ty-term-FillComp} from Figure~\ref{fig:ty-val-term}, or its simpler variant, \textsc{Ty-sterm-FillLeaf} from Figure~\ref{fig:ty-sterm} confirm this intuition. The left operand of these operators must be a destination that types in the ambiant context (both $[[P1]]$ unchanged in the premise and conclusion of the rules). The right operand, however, is a value that types in a context $[[P2]]$ in the premise, but requires $[[¹↑·P2]]$ in the conclusion. This is the opposite of the shift that $\ottkw{map}$ does: while $\ottkw{map}$ opens a new scope for its body, \textopname{fillComp} ($\triangleleft\!\mybullet$)/\textopname{fillLeaf}($\blacktriangleleft$) opens a portal to the parent scope for their right operand, as seen in the schema.

When we enter a new scope, the age of every remaining binding from the previous scopes is incremented by $[[↑]]$, as seen in column \emph{Context}.
When an ampar is complete and disposed of with the more general $\ottkw{from}_{\ottstype{\ltimes} }$, the right side has to be of shape $[[! ¹∞ T]]$, which means it is scope-insensitive, and thus it is safe to extract the right side from its shelled inner scope to the ambiant scope too.\unsure{I don't know where to put these sentences}

\section{Evaluation contexts and semantics}\label{sec:ectxs-sem}

\subsection{Evaluation contexts forms}

\begin{ottfig}{\caption{Grammar for evaluation contexts}\label{fig:grammar-ectxs}}
\ottgrammartabular{
\ottectx\ottinterrule
\ottectxs\ottafterlastrule
}
\end{ottfig}

\subsection{Typing of evaluation contexts and commands}

\begin{ottfig}{\caption{Typing rules for evaluation contexts and commands}\label{fig:ty-ectxs-cmd}}
\ottdefnTyXXectxs{}
\ottdefnTy{}
\end{ottfig}

\subsection{Small-step semantics}\label{ssec:sem}

\begin{ottfig}{\caption{Small-step semantics}\label{fig:sem}}
\ottdefnSem{}
\end{ottfig}

\section{Proof of type safety using Coq proof assistant}

\begin{itemize}
\item Not particularly elegant. Max number of goals observed 232
(solved by a single call to the \verb|congruence| tactic). When you
have a computer, brute force is a viable strategy. (in particular,
no semiring formalisation, it was quicker to do directly)
\item Rules generated by ott, same as in the article (up to some
notational difference). Contexts are not generated purely by syntax,
and are interpreted in a semantic domain (finite functions).
\item Reasoning on closed terms avoids almost all complications on
binder manipulation. Makes proofs tractable.
\item Finite functions: making a custom library was less headache than
using existing libraries (including \verb|MMap|). Existing libraries
don't provide some of the tools that we needed, but the most important
factor ended up being the need for a modicum of dependency between
key and value. There wasn't really that out there. Backed by actual
functions for simplicity; cost: equality is complicated.
\item Most of the proofs done by author with very little prior
experience to Coq.
\item Did proofs in Coq because context manipulations are tricky.
\item Context sum made total by adding an extra invalid \emph{mode}
(rather than an extra context). It seems to be much simpler this
way.
\item It might be a good idea to provide statistics on the number of
lemmas and size of Coq codebase.
\item (possibly) renaming as permutation, inspired by nominal sets,
make more lemmas don't require a condition (but some lemmas that
wouldn't in a straight renaming do in exchange).
\item (possibly) methodology: assume a lot of lemmas, prove main
theorem, prove assumptions, some wrong, fix. A number of wrong lemma
initially assumed, but replacing them by correct variant was always
easy to fix in proofs.
\item Axioms that we use and why (in particular setoid equality not
very natural with ott-generated typing rules).
\item Talk about the use and benefits of Copilot.
\end{itemize}

\section{Implementation of destination calculus using in-place memory mutations}
\label{sec:implementation}

What needs to be changed (e.g. linear alloc)

\section{Related work}

\unsure{Subsection for each work seems a little heavyweight. On the other hand the italic paragraphs seem too lightweight. I'm longing for the days of paragraph heading in bold.}
\subsection{Destination-passing style for efficient memory management}
In~\cite{shaikhha_destination-passing_2017}, the authors present a destination-based intermediate language for a functional array programming language. They develop a system of destination-specific optimizations and boast near-C performance.

This is the most comprehensive evidence to date of the benefit of destination-passing style for performance in functional programming languages. Although their work is on array programming, while this article focuses on linked data structure. They can therefore benefit of optimizations that are perhaps less valuable for us, such as allocating one contiguous memory chunk for several arrays.

The main difference between their work and ours is that their language is solely an intermediate language: it would be unsound to program in it manually. We, on the other hand, are proposing a type system to make it sound for the programmer to program directly with destinations.

We consider that these two aspects complement each other: good compiler optimization are important to alleviate the burden from the programmer and allowing high-level abstraction; having the possibility to use destinations in code affords the programmer more control would they need it.

\subsection{Tail modulo constructor}

Another example of destinations in a compiler's optimizer is~\cite{bour_tmc_2021}. It's meant to address the perennial problem that the map function on linked lists isn't tail-recursive, hence consumes stack space. The observation is that there's a systematic transformation of functions where the only recursive call is under a constructor to a destination-passing tail-recursive implementation.

Here again, there's no destination in user land, only in the intermediate representation. However, there is a programmatic interface: the programmer annotates a function like
\begin{verbatim}
let[@tail_mod_cons] rec map =
\end{verbatim}
to ask the compiler to perform the translation. The compiler will then throw an error if it can't. This way, contrary to the optimizations in~\cite{shaikhha_destination-passing_2017}, this optimization is entirely predictable.

This has been available in OCaml since version 4.14. This is the one example we know of of destinations built in a production-grade compiler. Our \destcalculus makes it possible to express the result tail-modulo-constructor in a typed language. It can be used to write programs directly in that style,  or it could serve as a typed target language for and automatic transformation. On the flip-side, tail modulo constructor is too weak to handle

\subsection{A functional representation of data structures with a hole}

The idea of using linear types to safely represent structures with holes dates back to~\cite{minamide_functional_1998}. Our system is strongly inspired by theirs. In their system, we can only compose functions that represent data structures with holes, we can't pattern-match on the result; just like in our system we cannot act on the left-hand side of $[[S ⧔ T]]$, only the right hand part.

In~\cite{minamide_functional_1998}, it's only ever possible to represent structures with a single hole. But this is a rather superficial restriction. The author don't comment on this, but we believe that this restriction only exists for convenience of the exposition: the language is lowered to a language without function abstraction and where composition is performed by combinators. While it's easy to write a combinator for single-argument-function composition, it's cumbersome to write combinators for functions with multiple arguments. But having multiple-hole data structures wouldn't have changed their system in any profound way.

The more important difference is that while their system is based on a type of linear functions, our is based on the linear logic's par combinator. This, in turns, lets us define a type of destinations which are representations of holes in values, which~\cite{minamide_functional_1998} doesn't have. This means that~\cite{minamide_functional_1998} can implement our examples with difference lists and queues from \cref{ssec:efficient-queue}, but it can't do our breadth-first traversal example from \cref{sec:bft}, since storing destinations in a data structure is the essential ingredient of this example.

This ability to store destination does come at a cost though: the system needs this additional notion of ages to ensure that destinations are use soundly. On the other hand, our system is strictly more general, in that the system from~\cite{minamide_functional_1998} can be embedded in \destcalculus, and if one stays in this fragment, we're never confronted with ages. Ages only show up when writing programs that go beyond Minamide's system.

\subsection{Tail modulo context}
\critical{You'll [Thomas] have to do this section, because I'm not familiar enough with that work}
\subsection{Destination-passing style programming: a Haskell implementation}
In~\cite{bagrel_destination-passing_2024}, the author proposes a system much like ours: it has a par-like construct (that they call $\ottstype{Incomplete}$), where only the right-hand side can be modified, and a destination type. The main difference is that in their system, $[[d ˢ⨞ t]]$ requires $[[t]]$ to be unrestricted, while in \destcalculus, $[[t]]$ can be linear.\unsure{More of these grey ts I don't know why that is.}

The consequence is that in~\cite{bagrel_destination-passing_2024}, destinations can be stored in data structures but not in data structures with holes. In order to do a breadth-first search algorithm like in \cref{sec:bft}, they can't use improved queues like we do, they have to use regular functional queues.

However, unlike \destcalculus, \cite{bagrel_destination-passing_2024} is implemented in Haskell, which features linear types. Our \destcalculus, with the age modes, needs more than what Haskell provides. Our system subsumes theirs, however, ages will appear in the typing rules for that fragment.

\subsection{Semi-axiomatic sequent calculus}
In\critical{Add citation}, the author develop a system where constructors return to a destination rather than allocating memories. It is very unlike the other systems described in this section in that it's completely founded in the Curry-Howard isomorphism. Specifically it gives an interpretation of a sequent calculus which mixes Gentzen-style deduction rules and Hilbert-style axioms. As a consequence, the par connective is completely symmetric, and, unlike our $[[⌊ T ⌋ ¹ν]]$ type, their dualisation connective is involutive.

The cost of this elegance is that computations may try to pattern-match on a hole, in which case they must wait for the hole to be filled. So the semantic of holes is that of a future or a promise. In turns this requires the semantic of their calculus to be fully concurrent. Which is a very different point in the design space.
\section{Conclusion and future work}

\clearpage{}
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}{}

\end{document}

% Local Variables:
% eval: (auto-fill-mode 0)
% eval: (visual-line-mode 1)
% ispell-local-dictionary: "en_US"
% End:
% LocalWords:  ampar combinators combinator Gentzen sequent
% LocalWords:  involutive dualisation
