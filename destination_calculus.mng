% -*- latex -*-
\documentclass[acmsmall, screen]{acmart}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsthm}
\let\Bbbk\relax % needed because of a conflict between amssymb and newtx
\usepackage{amssymb}
\usepackage{subcaption}

% For faulty code
\usepackage[normalem]{ulem}
\makeatletter
\def\uwave{\bgroup \markoverwith{\lower3.5\p@\hbox{\sixly \textcolor{red}{\char58}}}\ULon}
\font\sixly=lasy6 % does not re-load if already loaded, so no memory problem.
\makeatother

% For OTT rendering
\usepackage[supertabular]{ottalt}
\inputott{destination_calculus_ott.tex}
\usepackage{ottstyling}
% Hide "Index for ranges" from the metavars displayed tabular
\patchcmd{\ottmetavars}{$ \ottmv{k} $ & \ottcom{Index for ranges} \\}{}{}{}

% \setlength\textfloatsep{\baselineskip}
% \setlength{\intextsep}{\baselineskip}

\newcommand{\TODO}[1]{~\textnormal{\textcolor{red}{TODO: #1} } }
\newcommand\sepimp{\mathrel{-\mkern-6mu*}}
\newcommand{\parr}{\rotatebox[origin=c]{180}{\&}}
\makeatletter
\newcommand{\smallbullet}{} % for safety
\DeclareRobustCommand\smallbullet{%
  \mathord{\mathpalette\smallbullet@{0.5}}%
}
\newcommand{\smallbullet@}[2]{%
  \vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}%
}
\makeatother

\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newcommand\btriangleq{\pmb{\triangleq}}
\newlength{\interdefskip}
\setlength{\interdefskip}{0.1cm}
\newcommand{\newtype}[3][\phantom{\mathttbf{ator}}]{\mathttbf{type}#1\,\phantom{a}#2~\btriangleq~#3\\[\interdefskip]}
\newcommand{\newoperator}[5][]{\mathttbf{operator}#1\,\phantom{a}\!\!\!\!\begin{array}[t]{l}%
  #2 ~\pmb{:}~ #3 \\
  #4 ~\btriangleq~ #5
\end{array}\\[\interdefskip]}

\newcommand{\newoperatorb}[5][]{\mathttbf{operator}#1\\\myspace{1}\!\!\!\!\begin{array}[t]{l}%
  #2 ~\pmb{:}~ #3 \\
  #4 ~\btriangleq~ #5
\end{array}\\[\interdefskip]}
\newcommand{\figureratio}{0.87}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE SECTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\acmJournal{PACMPL}
\acmConference[POPL'25]{Principles of Programming Languages}{January 19 -- 25, 2025}{Denver, Colorado}
\title{Destination calculus}
\subtitle{A linear $\lambda-$calculus for pure, functional memory updates}

\author{Arnaud Spiwack}
\orcid{0TBD-0TBD-0TBD-0TBD}
\affiliation{
  \institution{Tweag}
  \department{OSPO}
  \position{Director, Research}
  \city{Paris}
  \country{France}
}
\email{arnaud.spiwack@tweag.io}

\author{Thomas Bagrel}
\orcid{0009-0008-8700-2741}
\affiliation{
  \institution{LORIA/Inria}
  \department{MOSEL VERIDIS}
  \city{Nancy}
  \country{France}
}
\affiliation{
  \institution{Tweag}
  \department{OSPO}
  \city{Paris}
  \country{France}
}
\email{thomas.bagrel@loria.fr}
\email{thomas.bagrel@tweag.io}

% On introduit de la mutation controlée dans les FP languages sans endommager la pureté (comme la lazyness peut être vu aussi)

\begin{abstract}
  We present the destination calculus, a linear $\lambda-$calculus for
  pure, functional memory updates. We introduce the
  syntax, type system, and operational semantics of the destination
  calculus, and prove type safety formally in the Coq proof assistant.
  
  We show how the principles of the destination calculus can form a theoretical ground
  for destination-passing style programming in functional languages. In particular,
  we detail how the present work can be applied to Linear Haskell to lift the main 
  restriction of DPS programming in Haskell as developed in \cite{bagrel_destination-passing_2024}.
  We illustrate this with a range of pseudo-Haskell examples.
\end{abstract}

\maketitle

%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Destination-passing style programming takes its root in the early days of imperative programming. In such language, the programmer is responsible for managing memory allocation and deallocation, and thus is it often unpractical for function calls to allocate memory for their results themselves. Instead, the caller allocates memory for the result of the callee, and passes the address of this output memory cell to the callee as an argument. This is called an \emph{out parameter}, \emph{mutable reference}, or even \emph{destination}.

But destination-passing style is not limited to imperative settings; it can be used in functional programming as well. One example is the linear destination-based API for arrays in Haskell\cite{bernardy_linear_2018}, which enables the user to build an array efficiently in a write-once fashion, without sacrificing the language identity and main guarantees. In this context, a destination points to a yet-unfilled memory slot of the array, and is said to be \emph{consumed} as soon as the associated hole is filled. In this paper, we continue on the same line: we present a linear $\lambda-$calculus embedding the concept of \emph{destinations} as first-class values, in order to provide a write-once memory scheme for pure, functional programming languages.

Why is it important to have destinations as first-class values? Because it allows the user to store them in arbitrary control or data structures, and thus to build complex data structures in arbitrary order/direction. This is a key feature of first-class DPS APIs, compared to ones in which destinations are inseparable from the structure they point to. In the latter case, the user is still forced to build the structure in its canonical order (e.g. from the leaves up to the root of the structure when using data constructors).

\section{System in action on simple examples}

\activespaces

The idea of destination calculus is to provide a building canvas for data structures, represented as a special pair-like structure, called an \emph{ampar}, whose left side is the structure being built, and whose right side carries destinations pointing to holes present in the left side.

A \emph{hole}, denoted $[[+h]]$ in the language, is a memory cell that has not been written to yet. In a practical application, a \emph{hole} would contain garbage data left by the previous use of the memory location, and thus should not be read in any circumstances (or it could lead to a segmentation fault!).

A \emph{destination}, denoted $[[-h]]$, is the address of a hole. It is meant to be one-use only; unlike mutable references which can often be reused.

The simplest form of ampar is a single empty cell on the left with a destination pointing to it on the right: $[[{ h } ⟨ +h ❟ -h ⟩]]$. This ampar is highly analogous to the identity function: the final structure on the left side will correspond to what is fed in the right side.

The main operation to operate on an ampar is $\ottkw{map}$, which binds the right side of the ampar to a variable, while temporarily forgetting about the left side (the incomplete structure that is being mutated behind the scenes).

$[[ { h } ⟨ +h ❟ -h ⟩ map d ⟼ d ⨞ Inl ⨞ () ]]$ 
reduces to $[[ { h' } ⟨ Inl +h' ❟ -h' ⟩ ]]$ then to $[[ { } ⟨ Inl () ❟ () ⟩ ]]$

The destination-feeding primitive $[[⨞]][[Inl]]$ will in fact follow the destination on the left-hand side and write an $[[Inl]]$ data constructor to the hole pointed by the destination. A new destination $[[-h']]$ is returned to represented the (yet unspecified) payload for the $[[Inl]]$ constructor.

This new destination $[[-h']]$ is then passed to the destination-feeding primitive $[[⨞]][[()]]$, which writes a unit value to the hole pointed by $[[-h']]$. The unit data constructor $[[()]]$ doesn't hold any payload, so there is no new destination to return here (so $[[()]]$ is returned by the primitive instead).

\paragraph{Arbitrary building order}

As mentioned earlier, one key point of destination calculus is being able to build a structure in any desired order. For example, one can build a balanced binary tree of depth two by first building the skeleton, then giving values to the right leaves, then to the left leaves.

Let's build the skeleton first:

\noindent$[[t1]] \coloneq [[
{ h1 } ⟨ +h1 ❟ -h1 ⟩ map d1 ⟼⮒
‥‥d1 ⨞ (,) case ¹ν (d2, d3) ⟼⮒
‥‥‥‥d2 ⨞ (,) case ¹ν (d4, d5) ⟼⮒
‥‥‥‥‥‥d3 ⨞ (,) case ¹ν (d6, d7) ⟼ ˢ[d4, d5, d6, d7]
]]$

evaluates to:

$[[ { h4, h5, h6, h7 } ⟨ ((+h4, +h5), (+h6, +h7)) ❟ ˢ[-h4, -h5, -h6, -h7] ⟩ ]]$

We can see that the skeleton of the tree is here on the left-hand side, but leaves are unspecified yet (so are represented by holes). Let's now fill the right leaves:

\noindent$[[t2]] \coloneq [[
t1 map d ⟼⮒
‥‥d case ¹ν ˢ[d4, d5, d6, d7] ⟼⮒
‥‥‥‥d5 ⨞ Inr ⨞ () ; d7 ⨞ Inr ⨞ () ; ˢ[d4, d6]
]]$

reduces to

$[[ { h4, h6 } ⟨ ((+h4, Inr ()), (+h6, Inr ())) ❟ ˢ[-h4, -h6] ⟩ ]]$

Now only left leaves are left unspecified, that's why the associated destinations $[[-h4]]$ and $[[-h6]]$ are the only left on the right side. We can finally feed them:

\noindent$[[t3]] \coloneq [[
t2 map d ⟼⮒
‥‥d case ¹ν ˢ[d4, d6] ⟼⮒
‥‥‥‥d4 ⨞ Inl ⨞ () ; d6 ⨞ Inr ⨞ ()
]]$

$[[t3]]$ reduces to $[[ {} ⟨ ((Inl (), Inr ()), (Inl (), Inr ())) ❟ () ⟩ ]]$ which is now a complete structure. There is no destination remaining on the right-hand side.

We also see that structure can in fact be built in several stages with ease, with other operations taking place in-between.

\section{Limitations of the previous approach}

Everything described above is in fact already possible in destination-passing style for Haskell as presented in~\cite{bagrel_destination-passing_2024}. However, there is one fundamental limitation in~\cite{bagrel_destination-passing_2024}: the inability to store destinations in destination-based data structures.

Indeed, that first approach of destination-passing style for Haskell can only be used to build non-linear data structures. More precisely, the \textsc{FillLeaf} operator ($\triangleleft$) can only take arguments with multiplicity $[[ω]]$. This is in fact a much stronger restriction than necessary ; the core idea is \emph{just} to prevent any destination (which is always a linear resource) to appear somewhere in the right-hand side of \textsc{FillLeaf}.

\subsection{Why stored destinations are problematic}

One core assumption of destination-passing style programming is that once a destination has been linearly consumed, the associated hole has been filled.

However, in a realm where destinations $\ottstype{\lfloor[[T]]\rfloor}$ can be of arbitrary inner type $[[T]]$, they can in particular be used to store a destination itself when $[[T]] = \ottstype{\lfloor[[T']]\rfloor}$!

We have to mark the value being fed in a destination as linearly consumed, so that it cannot be both stored away (to be used later) and pattern-matched on/used in the current context. But that means we have to mark the destination $[[d]] : \ottstype{\lfloor[[T']]\rfloor}$ as linearly consumed too when it is fed to $[[dd]] : \ottstype{\lfloor\lfloor[[T']]\rfloor\rfloor}$ in $[[dd ˢ⨞ d]]$.

As a result, there are in fact two ways to consume a destination: feed it now with a value, or store it away and feed it later. The latter is a much weaker form of consumption, as it doesn't guarantee that the hole associated to the destination has been filled \emph{now}, only that it will be filled later. So our assumption above doesn't hold in general case.

The issue is particularly visible when trying to give semantics to the $\ottkw{alloc'}$ operator with signature $\ottkw{alloc'} : \ottstype{(\lfloor[[T]]\rfloor\,_{[[¹]]}\!\ottstype{\to}\,[[①]])\,_{[[¹]]}\!\ottstype{\to}\,[[T]]}$. It reads: ``given a way of consuming a destination of type $[[T]]$, I'll return an object of type $[[T]]$''. This is an operator we really much want in our system!

The morally correct semantics (in destination calculus pseudo-syntax) would be:

\noindent$\ottkw{alloc'}~(\lambda\,[[d]]\,_{[[¹]]}\!\!\mapsto\,[[t]]) ~[[⟶]]~ \ottkw{withTmpStore}~\{ [[h]] \coloneq \_ \}~\ottkw{do}~\{ [[ t[d ≔ -h] ]]~;~\ottkw{deref}~[[-h]] \}$

It works as expected when the function supplied to $\ottkw{alloc'}$ will indeed use the destination to store a value:

\noindent$\ottkw{alloc'}~(\lambda\,[[d]]\,_{[[¹]]}\!\!\mapsto\,[[d ⨞ Inl ⨞ ()]])$

$~[[⟶]]~ \ottkw{withTmpStore}~\{ [[h]] \coloneq \_ \}~\ottkw{do}~\{ [[-h ⨞ Inl ⨞ ()]]~;~\ottkw{deref}~[[-h]] \}$

$~[[⟶]]~ \ottkw{withTmpStore}~\{ [[h]] \coloneq [[Inl ()]] \}~\ottkw{do}~\{ \ottkw{deref}~[[-h]] \}$

$~[[⟶]]~ [[Inl ()]]$

However this falls short when calls to $\ottkw{alloc'}$ are nested in the following way (where $[[dd]]$ : $\ottstype{\lfloor\lfloor[[①]]\rfloor\rfloor}$ and $[[d]]$ : $\ottstype{\lfloor[[①]]\rfloor}$):

\noindent$\ottkw{alloc'}~(\lambda\,[[dd]]\,_{[[¹]]}\!\!\mapsto\,\ottkw{alloc'}~(\lambda\,[[d]]\,_{[[¹]]}\!\!\mapsto\,[[dd ˢ⨞ d]]))$

$~[[⟶]]~ \ottkw{withTmpStore}~\{ [[hd]] \coloneq \_ \}~\ottkw{do}~\{ \ottkw{alloc'}~(\lambda\,[[d]]\,_{[[¹]]}\!\!\mapsto\,[[-hd ˢ⨞ d]])~;~\ottkw{deref}~[[-hd]] \}$

$~[[⟶]]~ \ottkw{withTmpStore}~\{ [[hd]] \coloneq \_ \}~\ottkw{do}~\{ \ottkw{withTmpStore}~\{ [[h]] \coloneq \_ \}~\ottkw{do}~\{ [[-hd ˢ⨞ -h]]~;~\ottkw{deref}~[[-h]] \}~;~\ottkw{deref}~[[-hd]] \}$

$~[[⟶]]~ \ottkw{withTmpStore}~\{ [[hd]] \coloneq [[-h]] \}~\ottkw{do}~\{ \uwave{ \ottkw{withTmpStore}~\{ [[h]] \coloneq \_ \}~\ottkw{do}~\{ \ottkw{deref}~[[-h]] \} }~;~\ottkw{deref}~[[-hd]] \}$

The original term $\ottkw{alloc'}~(\lambda\,[[dd]]\,_{[[¹]]}\!\!\mapsto\,\ottkw{alloc'}~(\lambda\,[[d]]\,_{[[¹]]}\!\!\mapsto\,[[dd ˢ⨞ d]]))$ is well typed, as the inner call to $\ottkw{alloc'}$ returns a value of type $[[①]]$ (as $[[d]]$ is of type $\ottstype{\lfloor[[①]]\rfloor}$) and consumes $[[d]]$ linearly. However, we see that because $[[-h]]$ escaped to the parent scope by being stored in a destination of destination coming from the parent scope, the hole $[[h]]$ has not been filled, and thus the inner expression $\ottkw{withTmpStore}~\{ [[h]] \coloneq \_ \}~\ottkw{do}~\{ \ottkw{deref}~[[-h]] \}$ cannot reduce in a meaningful way.

One could argue that the issue comes from the destination-feeding primitive $\triangleleft$ returning unit instead of a special value of a distinct \emph{effect} type. However, the same issue arise if we introduce a distinct type $\ottstype{\lfloor\!\!\rfloor}$ for the effect of feeding a destination\footnote{This type has in fact existed during the early prototypes of destination calculus, but we removed it as it doesn't solve the scope escape for destination and is indistinguishable in practice from the unit type.}, as long as destinations of type $\ottstype{\lfloor\,\ottstype{\lfloor\!\!\rfloor}\,\rfloor}$ are allowed. And forbidding destinations of type $\ottstype{\lfloor\,\ottstype{\lfloor\!\!\rfloor}\,\rfloor}$ gets annoying very fast: typing rules are no longer as simple, parametric polymorphism is not really possible for most functions without a system of constraints to ensure the absence of $\ottstype{\lfloor\!\!\rfloor}$ in anything we want to store, etc.

\subsection{Age control to prevent scope escape of destinations}

The solution we chose is to instead track the age of destinations (as De-Brujin-like scope indices), and prevent a destination to escape into the parent scope when stored through age-control restriction on the typing rule of destination-feeding primitives.

Age is represented by a commutative semiring, where $[[ν]]$ indicates that a destination originates from the current scope, and $[[↑]]$ indicates that it originates from the scope just before. We also extend ages to variables (a variable of age $[[a]]$ stands for a value of age $[[a]]$). Finally, age $[[∞]]$ is introduced for variables standing in place of a non-age-controlled value. In particular, destinations can never have age $[[∞]]$ in practice.

Semiring addition $\ottsmode{+}$ is used to find the age of a variable or destination that is used in two different branches of a program. Semiring multiplication $[[·]]$ corresponds to age composition, and is in fact an integer sum on scope indices.
$[[∞]]$ is absorbing for both addition and multiplication.

Given $[[↑]]^{0} = [[ν]]$ and $[[↑]]^{n} = [[↑]][[·]][[↑]]^{n-1}$, we have the following age operation tables:

\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{|c|c|c|}\hline
$\ottsmode{+}$ & $[[↑]]^{n}$               & $[[∞]]$ \\\hline
$[[↑]]^{m}$    & $\text{if }n = m\text{ then }[[↑]]^{n}\text{ else }[[∞]]$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$                   & $[[∞]]$ \\\hline
\end{tabular}
\end{minipage}\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{|c|c|c|}\hline
$[[·]]$        & $[[↑]]^{n}$               & $[[∞]]$ \\\hline
$[[↑]]^{m}$    & $[[↑]]^{n+m}$ & $[[∞]]$ \\\hline
$[[∞]]$        & $[[∞]]$                   & $[[∞]]$ \\\hline
\end{tabular}
\end{minipage}

Age commutative semiring is then combined with the multiplicity commutative semiring from~\cite{bernardy_linear_2018} to form a canonical product commutative semiring that is used to represent the mode of each typing context binding in our final type system.

The main restriction to prevent parent scope escape is materialized in these simplified typing rules:\smallskip

\ottusedrule{%
\ottdrule{%
}{
 \ottshvar{\destminus} \ottshvar{h} :\!_{\!  \ottsmode{\nu}   }\ottstype{\lfloor}\ottstype{T} \ottstype{\rfloor}   \,\vdash\,   \ottshvar{\destminus} \ottshvar{h}   \ottsym{:}   \ottstype{\lfloor}\ottstype{T} \ottstype{\rfloor} }{%
{\ottdrulename{Ty\_val\_Dest$^{\star}$}}{}%
}}
\ottusedrule{%
\ottdrule{%
\ottpremise{\Theta_{{\mathrm{1}}}  \,\vdash\,  \ottnt{t}  \ottsym{:}   \ottstype{\lfloor} \ottstype{T} \ottstype{\rfloor} }%
\ottpremise{\Theta_{{\mathrm{2}}}  \,\vdash\,  \ottnt{t'}  \ottsym{:}  \ottstype{T}}%
}{
\Theta_{{\mathrm{1}}}  +  \ottsmode{\uparrow} \ottsmode{\hspace{-0.1ex}\cdot\hspace{-0.1ex} }  \Theta_{{\mathrm{2}}} \,\vdash\,   \ottnt{t} \triangleleft\!\, \ottnt{t'}   \ottsym{:}   \ottstype{1} }{%
{\ottdrulename{Ty\_term\_FillLeaf$^{\star}$}}{}%
}}
\smallskip

Typing a destination $[[-h]]$ requires $[[-h]]$ to have age $[[ν]]$ in the context. And when storing a value through a destination, the ages of the value's dependencies in the context must be one higher than the corresponding ages required to type the value alone (this is the meaning of $\ottsmode{\uparrow} \ottsmode{\hspace{-0.1ex}\cdot\hspace{-0.1ex} }  \Theta_{{\mathrm{2}}}$).

Such a rule system prevents in particular the previous faulty expression $[[-hd ˢ⨞ -h]]$ where $[[-hd]]$ originates from the context parent to the one of $[[-h]]$.

\section{(Updated) breadth-first tree traversal}

\begin{figure}[h]

  \scalebox{\figureratio}{\begin{minipage}{\linewidth}$
\newtype[~\mathttbf{rec}]{[[Int]]}{[[① ⨁ Int]]}
\newoperator
    {\ottkw{zero}}{[[Int]]}
    {[[zero]]}{[[Inl ()]]}
\newoperator
    {\ottkw{succ}}{[[Int ¹ν → Int]]}
    {[[succ x]]}{[[ˢInr x]]}
\newtype[~\mathttbf{rec}]{[[List T]]}{[[① ⨁ (T ⨂ (List T))]]}
\newoperator
    {\triangleleft\ottsctor{[]}}{[[⌊ List T ⌋ n ¹ν → ①]]}
    {[[d ⨞ [] ]]}{[[d ⨞Inl ⨞()]]}
\newoperator
    {\triangleleft\ottsctor{(:)}}{[[⌊ List T ⌋ n ¹ν → ⌊ T⌋ n ⨂ ⌊ List T ⌋ n]]}
    {[[d ⨞ (:)]]}{[[d ⨞Inr ⨞(,)]]}
\newtype{[[DList T]]}{[[(List T) ⧔ (⌊ List T ⌋ ¹ν)]]}
\newoperator
    {\ottkw{append}}{[[DList T ¹ν → T ¹ν → DList T]]}
    {[[x append y]]}{\!\!\!\begin{array}[t]{l}[[
x map d ⟼ d ⨞ (:) case ¹ν⮒
‥‥(dh, dt) ⟼ dh ˢ⨞ y ; dt
]]\end{array}}
\newoperator
    {\ottkw{concat}}{[[DList T ¹ν → DList T ¹ν → DList T]]}
    {[[x concat x']]}{[[x map d ⟼ d ⨞· x']]}
\newoperator
    {\ottkw{to}_{\ottstype{List} }}{[[DList T ¹ν → List T]]}
    {[[toList x]]}{[[from⧔' (x map d ⟼ d ⨞ [])]]}
\newtype{[[Queue T]]}{[[(List T) ⨂ (DList T)]]}
\newoperator
    {\ottkw{singleton}}{[[T ¹ν → Queue T]]}
    {[[singleton x]]}{[[ˢ(ˢInr ˢ(x, Inl ()), alloc)]]}
\newoperator
    {\ottkw{enqueue}}{[[Queue T ¹ν → T ¹ν → Queue T]]}
    {[[x enqueue y]]}{[[x case ¹ν (x1, x2) ⟼ ˢ(x1, x2 append y)]]}
\newoperator
    {\ottkw{dequeue}}{[[Queue T ¹ν → ① ⨁ (T ⨂ Queue T)]]}
    {[[dequeue x]]}{\!\!\!\begin{array}[t]{l}[[
x case ¹ν⮒
‥‥(x, y) ⟼ x case ¹ν {⮒
‥‥‥‥Inl un ⟼ un ; toList y case ¹ν {⮒
‥‥‥‥‥‥Inl un ⟼ ˢInl un,⮒
‥‥‥‥‥‥Inr y' ⟼ y' case ¹ν⮒
‥‥‥‥‥‥‥‥(y1', y2') ⟼ ˢInr ˢ(y1', ˢ(y2', alloc))⮒
‥‥‥‥},⮒
‥‥‥‥Inr x' ⟼ x' case ¹ν⮒
‥‥‥‥‥‥(x1', y1') ⟼ ˢInr ˢ(x1', ˢ(y1', y))⮒
‥‥}
]]\end{array}}
\newtype{[[Tree T]]}{[[① ⨁ (T ⨂ ((Tree T) ⨂ (Tree T)))]]}
\newoperator
    {\triangleleft\ottsctor{Nil}}{[[⌊ Tree T ⌋ n ¹ν → ①]]}
    {[[d ⨞ Nil]]}{[[d ⨞Inl ⨞()]]}
\newoperator
    {\triangleleft\ottsctor{Node}}{[[⌊ Tree T ⌋ n ¹ν → ⌊T⌋ n ⨂ (⌊ Tree T⌋ n ⨂ ⌊ Tree T⌋ n)]]}
    {[[d ⨞ Node]]}{[[d ⨞Inr ⨞(,) case ¹ν (dn, dlr) ⟼ ˢ(dn, dlr ⨞(,))]]}
$\end{minipage}}

\caption{Boilerplate for breadth-first tree traversal}
\label{fig:impl-boilerplate-bfs}
\end{figure}

\begin{figure}[h]

\scalebox{\figureratio}{\begin{minipage}{\linewidth}$
\newoperatorb[~\mathttbf{rec}]
  {\ottkw{go}}{[[((! ¹∞ S) ¹ν → T1 ¹ν → (! ¹∞ S) ⨂ T2) ων→ (! ¹∞ S) ¹ν → Queue (Tree T1 ⨂ ⌊ Tree T2 ⌋ ¹ν) ¹ν → (! ¹∞ S)]]}
  {[[go f st q]]}{\!\!\!\begin{array}[t]{l}[[
dequeue q case ¹ν {⮒
‥‥Inl un ⟼ un ; st,⮒
‥‥Inr x ⟼ x case ¹ν⮒
‥‥‥‥(x', q') ⟼ x' case ¹ν⮒
‥‥‥‥‥‥(tr, dtr) ⟼ tr case ¹ν {⮒
‥‥‥‥‥‥‥‥Inl un ⟼ un ; dtr ⨞ Nil ; go f st q',⮒
‥‥‥‥‥‥‥‥Inr y ⟼ y case ¹ν⮒
‥‥‥‥‥‥‥‥‥‥(y', y'') ⟼ y'' case ¹ν⮒
‥‥‥‥‥‥‥‥‥‥‥‥(trl, trr) ⟼ dtr ⨞ Node case ¹ν⮒
‥‥‥‥‥‥‥‥‥‥‥‥‥‥(dn, dlr) ⟼ dlr case ¹ν⮒
‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥(dl, dr) ⟼ ༼f st༽ y' case ¹ν⮒
‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥(st', y'') ⟼⮒
‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥‥dn ˢ⨞ y'' ; go f st' (q' enqueue ˢ(trl, dl) enqueue ˢ(trr, dr))⮒
‥‥‥‥‥‥}⮒
}
]]\end{array}}
\newoperatorb
  {\ottkw{mapAccumBFS}}{[[((! ¹∞ S) ¹ν → T1 ¹ν → (! ¹∞ S) ⨂ T2) ων→ (! ¹∞ S) ¹ν → Tree T1 ¹ν → Tree T2 ⨂ (! ¹∞ S)]]}
  {[[mapAccumBFS f st tr]]}{\!\!\!\begin{array}[t]{l}[[
from⧔' (alloc map dtr ⟼ go f st (singleton ˢ(tr, dtr)))
]]\end{array}}
\newoperatorb
  {\ottkw{relabelDPS}}{[[Tree ① ¹ν → (Tree Int) ⨂ Int]]}
  {[[relabelDPS tr]]}{\!\!\!\begin{array}[t]{l}[[
mapAccumBFS⮒
‥‥(ˢλ ex ¹ν ⟼ ˢλ un ¹ν ⟼ un ; ex case ¹ν⮒
‥‥‥‥ᴇ ¹∞ ex' ⟼ ex' case ¹∞⮒
‥‥‥‥‥‥ᴇ ων st ⟼ ˢ(ˢᴇ ¹∞ (ˢᴇ ων (succ st)), st))⮒
‥‥(ˢᴇ ¹∞ (ˢᴇ ων (succ zero)))⮒
‥‥tr
]]\end{array}}
$\end{minipage}}

\caption{Breadth-first tree traversal in destination-passing style}
\label{fig:impl-bfs}
\end{figure}

\section{Language syntax}

\subsection{Introducing the \emph{ampar}}

Minamide's work\cite{minamide_functional_1998} is the earliest record we could find of a functional calculus integrating the idea of incomplete data structures (structures with holes) that exist as first class values and can be interacted with by the user.

In that paper, a structure with a hole is named \emph{hole abstraction}. In the body of a hole abstraction, the bound \emph{hole variable} should be used linearly (exactly once), and must only be used as a parameter of a data constructor. In other terms, the bound \emph{hole variable} cannot be pattern-matched on or used as a parameter of a function call. A hole abstraction is thus a weak form of linear lambda abstraction, which just moves a piece of data into a bigger data structure.

In fact, the type of hole abstraction $\ottstype{([[T1]], [[T2]]) hfun}$ in Minamine's work shares a lot of similarity with the separating implication or \emph{magic wand} $\ottstype{[[T1]] \sepimp [[T2]]}$ from separation logic: given a piece of memory matching description $[[T1]]$, we obtain a (complete) piece of memory matching description $[[T2]]$.

Now, in classical linear logic, we know we can transform linear implication $\ottstype{[[T1]] \multimap [[T2]]}$ into $\ottstype{[[T1]]^{\bot}~\parr~[[T2]]}$. Doing so for the \emph{wand} type $\ottstype{([[T1]], [[T2]]) hfun}$ or $\ottstype{[[T1]] \sepimp [[T2]]}$ gives $\ottstype{\lfloor[[T1]]\rfloor~ \widehat{\parr}~[[T2]]}$, where $\ottstype{\lfloor\smallbullet\rfloor}$ is memory negation, and $\ottstype{\widehat{\parr}}$ is a memory \emph{par} (weaker than the CLL \emph{par} that allows more ``interaction'' of its two sides).

Transforming the hole abstraction from its original implication form to a \emph{par} form let us consider the type $\ottstype{\lfloor[[T1]]\rfloor}$ of \emph{sink} or \emph{destination} of $[[T1]]$ as a first class component of our calculus. We also get to see the hole abstraction aka memory par as a pair-like structure, where the two sides might be coupled together in a way that prevent using both of them simultaneously.

\paragraph{From memory par $\ottstype{\widehat{\parr}}$ to ampar $\ottstype{\ltimes}$}

In CLL, the cut rule states that given $\ottstype{[[T1]]~\parr~[[T2]]}$, we can free up $[[T1]]$ by providing an eliminator of $[[T2]]$, or free up $[[T2]]$ by providing an eliminator of $[[T1]]$. The eliminator of $[[T]]$ can be $\ottstype{[[T]]^\bot}$, or $\ottstype{[[T]]^{\bot^{-1}}} = [[T']]$ if $[[T]]$ is already of the form $\ottstype{[[T']]^\bot}$. In a classical setting, thanks to the involutive nature of negation $\ottstype{\smallbullet^\bot}$, the two potential forms of the eliminator of $[[T]]$ are equal.

In destination calculus though, we don't have an involutive memory negation $\ottstype{\lfloor\smallbullet\rfloor}$. If we are provided with a destination of destination $[[-h']] : \ottstype{\lfloor\lfloor[[T]]\rfloor\rfloor}$, we know that some structure is expecting to store a destination of type $\ottstype{\lfloor[[T]]\rfloor}$. If ever that structure is consumed, then the destination stored inside will have to be fed with a value (remember we are in a linear calculus). So if we allocate a new memory slot of type $[[+h]] : [[T]]$ and its linked destination $[[-h]] : \ottstype{\lfloor[[T]]\rfloor}$, and write $[[-h]]$ to the memory slot pointed to by $[[-h']]$, then we can get back a value of type $[[T]]$ at $[[+h]]$ if ever the structure pointed to by $[[-h']]$ is consumed. Thus, a destination of destination is only equivalent to the promise of an eventual value, not an immediate usable one.

As a result, in destination calculus, we cannot have the same kind of cut rule as in CLL. This is, in fact, the part of destination calculus that was the hardest to design, and the source of a lot of early errors. For a destination of type $\ottstype{\lfloor[[T]]\rfloor}$, both storing it through a destination of destination $\ottstype{\lfloor\lfloor[[T]]\rfloor\rfloor}$ or using it to store a value of type $[[T]]$ constitute a linear use of the destination. But only the latter is a genuine consumption in the sense that it guarantees that the hole associated to the destination has been filled! Storing away the destination of type $\ottstype{\lfloor[[T]]\rfloor}$ originating from $\ottstype{[[T]]~\widehat{\parr}~\lfloor[[T]]\rfloor}$ (through a destination of destination of type $\ottstype{\lfloor\lfloor[[T]]\rfloor\rfloor}$) should not allow to free up the $[[T]]$, as it would in a CLL-like setting.

However, we can recover a memory abstraction that is usable in practice if we know the nature of an memory par side:
\begin{itemize}
  \item if the memory par side is a value made only of inert elements and destinations (negative polarity), then we can pattern-match/$\ottkw{map}$ on it, but we cannot store it away to free up the other side;
  \item if the memory par side is a value made only of inert elements and holes (positive polarity), then we can store it away in a bigger struct and free up the associated destinations (this is not an issue as the bigger struct will be locked by an memory par too), but we cannot pattern-match/$\ottkw{map}$ on it as it (may) contains holes;
  \item if one memory par side is only made of inert elements, we can in fact convert the memory par to a pair, as the memory par doesn't have any form of interaction between its sides.
\end{itemize}

It is important to note that the type of an memory par side is not really enough to determine the nature of the side, as a hole of type $[[T]]$ and and inert value of type $[[T]]$ are indistinguishable at the type level.

So we introduced a more restricted form of memory par, named \emph{ampar} ($\ottstype{\ltimes}$), for \emph{asymmetrical memory par}, in which:
\begin{itemize}
  \item the left side is made of inert elements (normal values or destinations from previous scopes) and/or holes if and only if those holes are compensated by destinations on the right side;
  \item the right side is made of inert elements and/or destinations.
\end{itemize}

As the right side cannot contain any holes, it is always safe to pattern-match or $\ottkw{map}$ on it. Because the left side cannot contain destinations from the current scope, it is always safe to store it away in a bigger struct and release the right side.

Finally, it is enough to check for the absence of destinations in the right side (which we can do easily just by looking at its type) to convert an \emph{ampar} to a pair, as any remaining hole on the left side would be compensated by a destination on the right side.

\paragraph{Destinations from previous scopes are inert}

In destination calculus, scopes are delimited by the $\ottkw{map}$ operation over ampars. Anytime a $\ottkw{map}$ happens, we enter a new scope, and any preexisting destination or variable see its age increased by one ($\ottsmode{[[↑]]}$). As soon as a destination or variable is no longer of age 0 ($\ottsmode{[[ν]]}$), it cannot be used actively but only passively (e.g. it cannot be applied if it is a function, or used to store a value if it is a destination, but it can be stored away in a dest, or pattern-matched on).

This is a core feature of the language that ensures part of its safety.

\subsection{Names and variables}

The destination calculus uses two classes of names: regular (meta) variable names $[[x]], [[y]]$, and hole names, $[[h]], [[h1]], [[h2]]$ which represents the identifier or address of a memory cell that hasn't been written to yet.

\visiblespaces

\ottmetavars
\ottgrammartabular{
\otthvar\ottafterlastrule
}

Hole names are represented by natural numbers under the hood, so they can act both as relative offsets or absolute positions in memory. Typically, when a structure is effectively allocated, its hole names are shifted by the maximum hole name encountered so far in the program ; this corresponds to finding the next unused memory cell in which to write new data.

We sometimes need to keep track of hole names bound by a particular runtime value or evaluation context, hence we also define sets of hole names $[[H]], [[H1]], [[H2]]\ldots$.

\ottgrammartabular{
\otthvars\ottafterlastrule
}

Shifting all hole names in a set by a given offset $[[h']]$ is denoted $[[H ⩲ h']]$. We also define a conditional shift operation $\ottshvar{[}[[H ⩲ h']]\ottshvar{]}$ which shifts each hole name appearing in the operand to the left of the brackets by $[[h']]$ if this hole name is also member of $[[H]]$. This conditional shift can be used on a single hole name, a value, or a typing context.

\subsection{Term and value core syntax}\label{ssec:core-syntax}

Destination calculus is based on linear simply-typed $\lambda$-calculus, with built-in support for sums, pairs, and exponentials. The syntax of terms is quite unusual, as we need to introduce all the tooling required to manipulate destinations, which constitute the primitive way of building a data structures for the user.

In fact, the grammatical class of values $[[v]]$, presented as a subset of terms $[[t]]$, could almost be removed completely from the user syntax, and just used as a denotation for runtime data structures. We only need to keep the \emph{ampar} value $[[{ h } ⟨ +h ❟ -h ⟩]]$ as part of the user syntax as a way to spawn a fresh memory cell to be later filled using destination-feeding primitives (see $[[alloc]]$ in Section~\ref{ssec:sugar}).

\ottgrammartabular{
\ottterm\ottinterrule
\ottval\ottafterlastrule
}

Pattern-matching on every type of structure (except unit) is parametrized by a mode $[[m]]$ to which the scrutinee is consumed. The variables which bind the subcomponents of the scrutinee then inherit this mode. In particular, this choice crystalize the equivalence $[[! ωa (T1 ⨂ T2)]] \simeq [[(! ωa T1) ⨂ (! ωa T2)]]$, which is not part of intuitionistic linear logic, but valid in Linear Haskell\cite{bernardy_linear_2018}.

$\ottkw{map}$ is the main primitive to operate on an \emph{ampar}, which represents an incomplete data structure whose building is in progress. $\ottkw{map}$ binds the right-hand side of the \emph{ampar} --- the one containing destinations of that \emph{ampar} --- to a variable, allowing those destinations to be operated on by destination-filling primitives. The left-hand side of the \emph{ampar} is inaccessible as it is being mutated behind the scenes by the destination-feeding primitives.

$\ottkw{to}_{\ottstype{\ltimes}}$ embeds an already completed structure in an \emph{ampar} whose left side is the structure, and right side is unit. We have an operator \textsc{FillComp} ($\triangleleft\mybullet$) allowing to compose two \emph{ampar}s by writing the root of the second one to a destination of the first one, so by throwing $\ottkw{to}_{\ottstype{\ltimes}}$ to the mix, we can compose an \emph{ampar} with a normal (completed) structure (see the sugar operator \textsc{FillLeaf} ($\triangleleft$) in Section~\ref{ssec:sugar}).

$\ottkw{from}_{\ottstype{\ltimes}}$ is used to convert an \emph{ampar} to a pair, when the right side of the \emph{ampar} is an exponential of the form $[[ᴇ ¹∞ v]]$. Indeed, when the right side has such form, it cannot contains destinations (as destinations always have a finite age), thus it cannot contain holes in its left side either (as holes on the left side are always compensated 1:1 by a destination on the right side). As a result, it is valid to convert an \emph{ampar} to a pair in these circumstances. $\ottkw{from}_{\ottstype{\ltimes}}$ is in particular used to extract a structure from its \emph{ampar} building shell when it is complete (see the sugar operator $\ottkw{from'}_{\ottstype{\ltimes}}$ in Section~\ref{ssec:sugar}).

The remaining term operators $[[⨞]][[()]], [[⨞]][[Inl]], [[⨞]][[Inr]], [[⨞]]\,\expcons{[[m]]}, [[⨞]][[(,)]], [[⨞]](\lamnt{[[x]]}{[[m]]}{[[u]]})$ are all destination-feeding primitives. They write a layer of value/constructor to the hole pointed by the destination operand, and return the potential new destinations that are created in the process (or unit if there is none).

\subsection{Syntactic sugar for constructors and commonly used operations}\label{ssec:sugar}

As we said in section~\ref{ssec:core-syntax}, the grammatical class of values is mostly used for runtime only; in particular, data constructors can only take other values as arguments, not terms. Thus we introduce syntactic for data constructors taking arbitrary terms as parameters (as we often find in functional programming languages) using destination-feeding primitives.

$\ottkw{from}_{\ottstype{\ltimes}'}$ is a simpler variant of $\ottkw{from}_{\ottstype{\ltimes}}$ that allows to extract the right side of an ampar when the left side has been fully consumed. We implement it in terms of $\ottkw{from}_{\ottstype{\ltimes}}$ to keep the core calculus tidier (and limit the number of typing rules, evaluation contexts, etc), but it can be implemented much more efficiently in a real-world implementation.

\ottgrammartabular{
\ottsterm\ottafterlastrule
}

\activespaces

\begin{table}[h]
\centering\small
\setlength\tabcolsep{0.4ex}
\begin{tabular}{|lcl|lcl|}
\hline

$[[alloc]]$ & $\btriangleq$ & $\begin{array}[t]{l}[[
  { 1 } ⟨ +1 ❟ -1 ⟩
]]\end{array}$ &

$[[t ˢ⨞ t']]$ & $\btriangleq$ & $\begin{array}[t]{l}[[
t ⨞· (to⧔ t')
]]\end{array}$ \\\hline

$[[from⧔' t]]$ & $\btriangleq$ & $\begin{array}[t]{l}[[
(from⧔ (t map un ⟼ un ; ᴇ ¹∞ () )) case ¹ν⮒
‥‥( st , ex ) ⟼ ex case ¹ν⮒
‥‥‥‥ᴇ ¹∞ un ⟼ un ; st
]]\end{array}$ &

$[[ˢλ x m ⟼ u]]$ & $\btriangleq$ & $\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ ( λ x m ⟼ u )⮒
)
]]\end{array}$ \\\hline

$[[ˢInl t]]$ & $\btriangleq$ & $\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ Inl ˢ⨞ t⮒
)
]]\end{array}$ &

$[[ˢInr t]]$ & $\btriangleq$ & $\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ Inr ˢ⨞ t⮒
)
]]\end{array}$ \\\hline

$[[ˢᴇ m t]]$ & $\btriangleq$ & $\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥d ⨞ ᴇ m ˢ⨞ t⮒
)
]]\end{array}$ &

$[[ˢ( t1 , t2 )]]$ & $\btriangleq$ & $\begin{array}[t]{l}[[
from⧔' (⮒
‥‥alloc map d ⟼⮒
‥‥‥‥(d ⨞ (,)) case ¹ν⮒
‥‥‥‥‥‥( d1 , d2 ) ⟼ d1 ˢ⨞ t1 ; d2 ˢ⨞ t2⮒
)
]]\end{array}$ \\\hline

\end{tabular}
\caption{Desugaring of syntactic sugar forms for terms}
\label{tab:desugaring}
\end{table}

\clearpage

\section{Type system}

\subsection{Syntax for types, modes, and typing contexts}

\ottgrammartabular{
\otttype\ottinterrule
\ottmode\ottinterrule
\ottmul\ottinterrule
\ottage\ottinterrule
\ottctx\ottafterlastrule
}

\subsection{Typing of terms and values}

\ottdefnTyXXval{}
\ottdefnTyXXterm{}

\subsection{Derived typing rules for syntactic sugar forms}

\ottdefnTyXXsterm{}

\section{Evaluation contexts and semantics}

\subsection{Evaluation contexts forms}

\ottgrammartabular{
\ottectx\ottinterrule
\ottectxs\ottafterlastrule
}

\subsection{Typing of evaluation contexts and commands}

\ottdefnTyXXectxs{}
\ottdefnTy{}

\subsection{Small-step semantics}

\ottdefnSem{}

\section{Proof of type safety using Coq proof assistant}

\begin{itemize}
\item Not particularly elegant. Max number of goals observed 232
  (solved by a single call to the \verb|congruence| tactic). When you
  have a computer, brute force is a viable strategy. (in particular,
  no semiring formalisation, it was quicker to do directly)
\item Rules generated by ott, same as in the article (up to some
  notational difference). Contexts are not generated purely by syntax,
  and are interpreted in a semantic domain (finite functions).
\item Reasoning on closed terms avoids almost all complications on
  binder manipulation. Makes proofs tractable.
\item Finite functions: making a custom library was less headache than
  using existing libraries (including \verb|MMap|). Existing libraries
  don't provide some of the tools that we needed, but the most important
  factor ended up being the need for a modicum of dependency between
  key and value. There wasn't really that out there. Backed by actual
  functions for simplicity; cost: equality is complicated.
\item Most of the proofs done by author with very little prior
  experience to Coq.
\item Did proofs in Coq because context manipulations are tricky.
\item Context sum made total by adding an extra invalid \emph{mode}
  (rather than an extra context). It seems to be much simpler this
  way.
\item It might be a good idea to provide statistics on the number of
  lemmas and size of Coq codebase.
\item (possibly) renaming as permutation, inspired by nominal sets,
  make more lemmas don't require a condition (but some lemmas that
  wouldn't in a straight renaming do in exchange).
\item (possibly) methodology: assume a lot of lemmas, prove main
  theorem, prove assumptions, some wrong, fix. A number of wrong lemma
  initially assumed, but replacing them by correct variant was always
  easy to fix in proofs.
\item Axioms that we use and why (in particular setoid equality not
  very natural with ott-generated typing rules).
\item Talk about the use and benefits of Copilot.
\end{itemize}

\section{Implementation of destination calculus using in-place memory mutations}

What needs to be changed (e.g. linear alloc)

\section{Related work}

\section{Conclusion and future work}

\clearpage{}
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}{}

\end{document}
